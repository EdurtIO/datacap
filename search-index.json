[
  {
    "title": "自主机部署",
    "content": "DataCap 是支持用户将服务部署到自主机中。通过本文档用户可以了解如何在自主机中部署 DataCap。 --- !!! warning \"注意\" 该软件的二进制包基于以下系统进行编译和测试。它尚未在其他版本上进行测试，理论上受支持。 如果有不支持的系统，请使用源码编译方法主动编译二进制文件。 !!! | System | Version | |--------|------------| | JDK | \\>=11 | | MySQL | \\>=5.6.x | --- !!! info \"提示\" 从以下地址下载相应系统的二进制软件包进行安装。如果您需要使用源码安装请前往查看开发者文档模块。 !!! 1.下载最新发布版本 2.将二进制文件下载到本地后运行以下命令 bash tar -xvzf datacap-<VERSION>-bin.tar.gz 进入软件根目录 bash cd datacap-<VERSION> --- 对于软件的首次安装，您需要将 schema/datacap.sql 文件中的sql脚本导入MySQL服务器。注意需要导入的脚本根据下载的软件包进行匹配 !!! danger \"注意\" 如果您是通过其他版本升级，请执行 schema/<VERSION>/schema.sql !!! datacap 软件中的所有配置均在 configure/application.properties 文件中。 导入 SQL 脚本后，修改 configure/application.properties 配置文件以修改MySQL服务器的配置信息 properties server.port=9096 server.address=localhost spring.jackson.time-zone=GMT+8 spring.jackson.date-format=yyyy-MM-dd HH:mm:ss datacap.security.secret=DataCapSecretKey datacap.security.expiration=86400000 datacap.editor.sugs.maxSize=1000 server.port: 用于配置服务在服务器中启动监听的端口，默认为 9096 server.address: 用于配置服务在本地的监听地址，如果需要使用 IP+端口 方便外部机器访问，请不要设置为 localhost，建议设置为 0.0.0.0 spring.jackson.time-zone: 用于配置时区，默认为 GMT+8 spring.jackson.date-format: 用于配置日期格式，默认为 yyyy-MM-dd HH:mm:ss datacap.security.secret: 用于配置数据安全管理的密钥，默认为 DataCapSecretKey datacap.security.expiration: 用于配置数据安全管理的过期时间，单位为毫秒，默认为 86400000 datacap.editor.sugs.maxSize: 用于配置数据编辑器的最大行数，默认为 1000 已经失效不在使用 properties spring.mvc.throw-exception-if-no-handler-found=true spring.resources.add-mappings=false spring.web.resources.add-mappings=true spring.mvc.throw-exception-if-no-handler-found: 用于配置是否抛出异常 spring.resources.add-mappings: 用于配置是否启用静态资源映射 spring.web.resources.add-mappings: 用于配置是否启用静态资源映射 !!! danger \"注意\" 如果版本 >=8.x，请设置 allowPublicKeyRetrieval=true !!! properties spring.datasource.url=jdbc:mysql://localhost:3306/datacap?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull&allowMultiQueries=true&useSSL=false&useOldAliasMetadataBehavior=true&jdbcCompliantTruncation=false&allowPublicKeyRetrieval=true spring.datasource.username=root spring.datasource.password=12345678 spring.datasource.url: 用于配置数据库连接地址 spring.datasource.username: 用于配置数据库用户名 spring.datasource.password: 用于配置数据库密码 !!! info \"提示\" 支持所有 Spring Data 的配置参数 !!! properties datacap.executor.data= datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=SPARK datacap.executor.startScript=start-seatunnel-spark-connector-v2.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel datacap.executor.data: 用于配置执行器的数据缓冲路径 datacap.executor.way: 用于配置执行器的执行方式，不同的执行器拥有不同的执行方式 datacap.executor.mode: 用于配置执行器的执行模式，不同的执行器拥有不同的执行模式 datacap.executor.engine: 用于配置执行器的执行引擎 datacap.executor.startScript: 用于配置执行器的启动脚本 datacap.executor.seatunnel.home: 用于配置执行器的 Apache Seatunnel 主目录 ::: tabs === \"Spark 引擎配置\" properties datacap.executor.data= datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=SPARK datacap.executor.startScript=start-seatunnel-spark-connector-v2.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel === \"Flink 引擎配置\" properties datacap.executor.data= datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=FLINK datacap.executor.startScript=start-seatunnel-flink-13-connector-v2.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel === \"Seatunnel 引擎配置\" properties datacap.executor.data= # Only support LOCAL datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=SEATUNNEL datacap.executor.startScript=seatunnel.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel ::: properties datacap.config.data= datacap.cache.data= datacap.config.data: 用于配置上传配置文件的路径 datacap.cache.data: 用于配置上传缓存文件的路径 properties datacap.openai.backend=https://api.openai.com datacap.openai.token= datacap.openai.model=GPT35TURBO0613 datacap.openai.timeout=30 datacap.openai.backend: 用于配置 OpenAI 的后端地址 datacap.openai.token: 用于配置 OpenAI 的 token datacap.openai.model: 用于配置 OpenAI 的模型 datacap.openai.timeout: 用于配置 OpenAI 的超时时间，单位为秒 properties datacap.registration.enable= datacap.captcha.enable= datacap.cache.maximum=100000 datacap.cache.expiration=5 datacap.audit.sql.print=false datacap.registration.enable: 用于配置是否开启注册 datacap.captcha.enable: 用于配置是否开启验证码 datacap.cache.maximum: 用于配置缓存最大值 datacap.cache.expiration: 用于配置缓存过期时间，单位为分钟 datacap.audit.sql.print: 用于配置是否打印 SQL properties datacap.pipeline.maxRunning=100 datacap.pipeline.maxQueue=200 datacap.pipeline.reset=STOPPED datacap.pipeline.maxRunning: 用于配置最大运行数 datacap.pipeline.maxQueue: 用于配置最大队列 datacap.pipeline.reset: 用于配置重置策略 --- 支持的存储类型详见 https://github.com/devlive-community/datacap/tree/dev/fs properties datacap.fs.type=Local datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 用于配置文件系统类型 datacap.fs.access: 用于配置文件系统访问，该类型可以为空 datacap.fs.secret: 用于配置文件系统密钥，该类型可以为空 datacap.fs.endpoint: 用于配置文件系统端点，如果填写后将追加为目录 该类型可以为空 datacap.fs.bucket: 用于配置文件系统存储桶，该类型可以为空 properties datacap.fs.type=AliOss datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: AliOss properties datacap.fs.type=Qiniu datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: Qiniu properties datacap.experimental.autoLimit=true datacap.experimental.data={user.dir}/data datacap.experimental.avatarPath={username}/avatar/ datacap.experimental.autoLimit: 用于配置是否自动增加 LIMIT datacap.experimental.data: 用于配置实验性功能的数据路径 datacap.experimental.avatarPath: 用于配置实验性功能的头像路径 !!! warning \"警告\" 如果需要修改日志配置，只需修改 configure/logback.xml 配置文件即可 !!! !!! warning \"警告\" 如果您需要定制化 JVM 配置，只需修改 configure/jvm.conf 配置文件即可 !!! properties plugin.manager.extend.packages=com.fasterxml.jackson plugin.manager.extend.packages: 用于配置插件扩展包，配置后将优先加载父类加载器中的依赖 --- > 启动服务前请安装系统需要的各种插件，执行命令 ./bin/install-plugin.sh，也可以到服务商店中进行安装。 DataCap服务启动非常简单，执行以下脚本 bash ./bin/startup.sh 停止服务并执行以下脚本 bash ./bin/shutdown.sh !!! info \"提示\" 如果要调试系统，可以使用 ./bin/debug.sh 启动服务，但关闭窗口时它将停止 !!!",
    "url": "https://datacap.devlive.org/en/reference/get-started/install.html",
    "lang": "en"
  },
  {
    "title": "Docker 部署",
    "content": "DataCap 项目提供 devliveorg/datacap 包含 DataCap 服务器和默认配置的 Docker 映像。Docker 映像发布到 Docker Hub，可以与 Docker 运行时等一起使用。 要在 Docker 中运行 DataCap，您必须在计算机上安装 Docker 引擎。您可以从 Docker website, 或使用操作系统的打包系统。 使用 docker 命令从 devliveorg/datacap 图像。为其分配数据帽名称，以便以后更容易引用它。在后台运行它，并将默认 DataCap 端口（即 9096）从容器内部映射到工作站上的端口 9096。 bash docker run -d -p 9909:9096 --name datacap devliveorg/datacap 如果不指定容器映像标记，则默认为 latest ，但可以使用许多已发布的 DataCap 版本，例如 devliveorg/datacap:2024.4。 !!! danger \"注意\" 需要挂在外接 MySQL 配置，服务启动的方式为 bash docker run -d -p 9096:9096 -v /root/application.properties:/opt/app/datacap/configure/application.properties --name datacap devliveorg/datacap 假设您的配置文件在 /root/application.properties，如需要其他路径请指定绝对路径即可。 !!! 运行 docker ps 以查看在后台运行的所有容器。 bash -> % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2096fba19e2a devliveorg/datacap:latest \"sh ./bin/debug.sh\" 5 days ago Up 14 seconds 0.0.0.0:9909->9096/tcp datacap 您可以使用 docker stop datacap 和 docker start datacap 命令停止和启动容器。要完全删除已停止的容器，请运行 docker rm datacap。",
    "url": "https://datacap.devlive.org/en/reference/get-started/installContainers.html",
    "lang": "en"
  },
  {
    "title": "Docker Compose 部署",
    "content": "DataCap 项目提供 Docker Compose 方式部署，通过下载 docker-compose.yml 文件，或者使用以下代码进行服务部署。 --- > 只有基础的一些功能 yaml version: '3.8' services: app-mysql: image: mysql:latest environment: MYSQLROOTPASSWORD: 12345678 MYSQLDATABASE: datacap ports: \"3306:3306\" volumes: ./configure/schema/datacap.sql:/docker-entrypoint-initdb.d/schema.sql app-datacap: image: qianmoq/datacap:latest restart: always ports: \"9099:9099\" dependson: app-mysql app-clickhouse volumes: ./configure/docker/application.properties:/opt/app/datacap/configure/application.properties --- > 该方式包含了 数据集 功能 yaml version: '3.8' services: app-mysql: image: mysql:latest environment: MYSQLROOTPASSWORD: 12345678 MYSQLDATABASE: datacap ports: \"3306:3306\" volumes: ./configure/schema/datacap.sql:/docker-entrypoint-initdb.d/schema.sql app-clickhouse: image: clickhouse/clickhouse-server:latest restart: always ports: \"8123:8123\" environment: CLICKHOUSEDB=datacap app-datacap: image: qianmoq/datacap:latest restart: always ports: \"9099:9099\" dependson: app-mysql app-clickhouse volumes: ./configure/docker/application.properties:/opt/app/datacap/configure/application.properties !!! warning \"注意\" 需要同时下载一下多个文件： datacap.sql application.properties 下载完成后将他们放置到指定目录，也就是 ./configure/docker/ 和 ./configure/schema/ 如果需要自定义目录，可以修改 docker-compose.yml 文件中挂载的 volumes 配置即可。 !!! --- 以上工作完成后，使用以下命令进行启动服务。必须在包含 docker-compose.yml 文件的目录下执行 bash docker-compose up 如果需要后台启动使用以下命令 bash docker-compose up -d 启动成功后，浏览器打开 http://localhost:9096/ 即可看到网站。 --- 停止服务需要使用以下命令 bash docker-compose down",
    "url": "https://datacap.devlive.org/en/reference/get-started/installFromDockerCompose.html",
    "lang": "en"
  },
  {
    "title": "Rainbond 部署",
    "content": "如果您不熟悉 Kubernetes，想在 Kubernetes 中安装 DataCap，可以使用 Rainbond 来部署。Rainbond 是一个基于 Kubernetes 构建的云原生应用管理平台，可以很简单的将你的应用部署到 Kubernetes 中。 安装 Rainbond, 请参阅 Rainbond 快速安装. DataCap 已发布到 Rainbond 开源应用商店，可通过 Rainbond 开源应用商店一键部署 DataCap。 进入 Rainbond 控制台的 平台管理 -> 应用市场 -> 开源应用商店 中搜索 DataCap 并安装。 ![](https://static.goodrain.com/wechat/datacap/1.png) 填写以下信息，然后点击确认按钮进行安装。 团队：选择现有团队或创建新的团队 集群：选择对应的集群 应用：选择现有应用或创建新的应用 版本：选择要安装的版本 安装完成后，可通过 Rainbond 提供的默认域名访问 DataCap，默认用户密码 admin/12345678 ![](https://static.goodrain.com/wechat/datacap/topology.png)",
    "url": "https://datacap.devlive.org/en/reference/get-started/installRainbond.html",
    "lang": "en"
  },
  {
    "title": "即席查询",
    "content": "软件安装完成后点击顶部的 查询 菜单，进入即席查询页面。 !!! info \"提示\" 即席查询需要添加一个数据源，如何添加数据源请前往 数据源管理 !!! !img.png --- 进入查询页面后，窗口大致如下 !img1.png 查询页面分为左右两部分： 左侧为数据源 + 数据源元数据（需要选择数据源后才会展示） 右侧为 SQL 编辑器 + 结果展示（需要查询成功后才会展示） 当我们选择数据源后，左侧会展示类似下面的数据源元数据： !img2.png 此时右侧编辑器上方的工具栏也可以使用，我们在 SQL 编辑器中输入如下语句 sql SELECT FROM datacap.datacapsourcequery 点击编辑器顶部的 运行 按钮，即可查询。当查询成功后，编辑器下方会展示本次查询结果，类似如下 !img3.png --- 当我们在编辑器中选择执行的 SQL 后，顶部工具栏中的 运行 将会变为 运行选择内容 !img4.png!img4.png --- 当我们在编辑器中输入 SQL 后，点击顶部工具栏中的 格式化 按钮，即可格式化我们输入的 SQL !img5.png --- 当我们在编辑器中输入 SQL 并执行后，点击顶部工具栏中的 取消 按钮，即可取消本次查询。 !!! danger \"注意\" 取消功能并不意味着实际查询结束，查询将继续在后台运行。只是本次查询将不会在接受后续返回的结果。 !!! --- 这是一个片段模块的快捷功能，可以将编辑器中执行成功后的 SQL 快速添加到片段中。后续也可以在编辑器中实现片段的自动填充。 点击按钮后，会在右侧展示如下窗口，填写完成后，保存即可。 !img6.png --- 当查询完成后，会在 片段 按钮右侧出现一个展示耗时的按钮，点击后可以查看本次查询的消耗时间详情 !img7.png --- 在 datacap 中接入了 ai 模型，需要用户配置相应的信息方可使用，AI 模型支持 解析 优化 修复问题（只有查询出现错误后，才会出现该功能） 这里我们不多展示，可以自己体验。 --- 在 AI 按钮右侧有一个用于输入数字的输入框，他主要用于输入自动添加 LIMIT 的总数量（需要启动该功能，目前为实验性功能） --- 在编辑器上方的右侧有个 !btn[创建编辑器](){bg-white border} 按钮，用于添加编辑器，点击后可以增加一个编辑窗口 !img8.png --- 当我们添加新的编辑器后，在编辑器名称后有个 :circle-x: 按钮，点击后可以关闭该编辑器 !img9.png 默认查询后渲染为普通查询表格，可以在表格的头部做排序，筛选等操作。 !img10.png 当点击!btn[可视化]{bg-white border}按钮后的开关，会重新渲染下方表格，对表格中的数据进行分页的转换，当关闭分页后回会展示如下 !img11.png 会展示所有的数据不在进行分页。 目前只支持导出 CSV ，该操作并不会访问后端服务，点击后会导出当前查询的所有返回结果数据。",
    "url": "https://datacap.devlive.org/en/reference/get-started/query/home.html",
    "lang": "en"
  },
  {
    "title": "仪表盘",
    "content": "软件安装完成后点击顶部的 仪表盘 菜单，进入仪表盘页面。 !img.png --- 点击页面左侧的 :circle-plus: 按钮弹出如下窗口 !img1.png 点击!btn[添加图表]{bg-white border}按钮，弹出数据报表窗口 !img2.png 选择需要添加的图表后点击 !btn[保存]{bg-blue-500 text-white} 按钮，出现类似如下页面 !img3.png 可以调整报表的位置大小等，然后点击右侧的 !btn[保存]{bg-blue-500 text-white} 按钮，弹出保存配置窗口 !img4.png 配置相关信息后，点击 !btn[保存]{bg-blue-500 text-white} 按钮，保存成功后将跳转到仪表盘预览页面。 !img5.png 这里会显示当前仪表盘的报表数据。 --- 在仪表盘列表页面，点击仪表盘的名称，可跳转到当前选择的仪表盘。 !img6.png !!! info \"提示\" 该操作将会跳转到新页面进行仪表盘的显示。 !!! --- 在仪表盘列表页面，点击 :cog: 图标，可出现操作列表 !img7.png 点击 !btn[修改仪表盘] 菜单，即可跳转到编辑仪表盘页面，该操作和新建仪表盘操作一致。 --- 在仪表盘列表页面，点击 :cog: 图标，可出现操作列表 点击 !btn[删除仪表盘] 菜单，即可弹出如下窗口 !img8.png 在输入框中输入仪表盘名称，点击删除即可删除该仪表盘。",
    "url": "https://datacap.devlive.org/en/reference/get-started/dashboard/home.html",
    "lang": "en"
  },
  {
    "title": "新建 & 编辑",
    "content": "在查询页面进行查询后会在结果表格上方显示出如下窗口 !img.png 点击图中红框选择的按钮，跳转到新建数据集页面 !img1.png 该页面中我们可以修改要执行的 SQL 点击 !btn[执行] 按钮，会在下方出现预览数据的列表，点击顶部的 !btn[配置] 按钮，窗口如下 !img2.png 在页面中我们可以配置 数据列 和 基本信息，根据选择不同的 tab 进行相关配置。 !!! info \"提示\" 在数据集中列分为两种 虚拟列 和 真实列。 虚拟列 不会在实际的底层存储构建（只是用函数比较，只会在查询中实时展示），真实列 需要在底层存储中进行构建，这样在查询的时候会有更好的性能。 !!! 我们点击每行数据中的 操作 配置下的 :CirclePlus: 按钮，可以添加虚拟列 !img3.png 在虚拟列中部分配置无法使用，因为它不做具体的存储操作。 !!! danger \"警告\" 虚拟列 是 DataCap 中的一个新特性，该列只会在运行时生效，一般列名都是函数的表示，比如 SUM(id)，在查询的时候会将该 SQL 转换。 !!! 点击顶部的 数据配置 标签，配置项显示如下 !img4.png 我们完成基本的配置信息后点击顶部的 创建数据集 按钮，即可在后台构建数据集。届时会跳转到数据集列表。",
    "url": "https://datacap.devlive.org/en/reference/get-started/dataset/create/home.html",
    "lang": "en"
  },
  {
    "title": "即席查询",
    "content": "在数据集列表中在每行数据的末尾有 操作 按钮，大概如下 !img.png 我们点击 :ChartNoAxesColumn: 图标后会跳转到 即席查询 页面 !img1.png 页面分为左右两侧，左侧是当前数据集的 指标 & 维度 配置，右侧是 查询 配置 当拖拽左侧的 指标 & 维度 时会在右侧显示查询结果 !img2.png 当查询列包含指标时，点击指标后面的 :cog:，弹出如下窗口 !img3.png 可以配置当前指标的 表达式，别名，排序。 !!! warning \"注意\" 不同的类型指标包含不同的表达式 !!! 当查询列包含维度时，点击 :cog:，弹出如下窗口 !img4.png 可以配置当前维度的 别名，排序，自定义函数。 当查询成功后，可以配置多种图表类型。 !img5.png 可以根据自己的需求定制目前已经支持的图表。 图表配置完成后，点击 发布 按钮，弹出如下窗口 !img6.png 配置图表的名称后点击 发布 按钮，图表发布成功后，可以在图表列表中查看。",
    "url": "https://datacap.devlive.org/en/reference/get-started/dataset/adhoc/home.html",
    "lang": "en"
  },
  {
    "title": "同步数据",
    "content": "数据集提供了手动同步数据的操作，可以通过数据集列表中在每行数据的末尾有 同步数据 按钮，如下窗口 !img.png 点击 同步数据 按钮后会弹出 同步数据 窗口 !img1.png 确定同步数据后，点击 同步数据 按钮后，系统会在后台创建同步数据任务，进行数据同步。",
    "url": "https://datacap.devlive.org/en/reference/get-started/dataset/sync/home.html",
    "lang": "en"
  },
  {
    "title": "同步历史",
    "content": "数据集提供了查看同步历史的操作，可以通过数据集列表中在每行数据的末尾有 同步历史 按钮，如下窗口 !img.png 点击 同步历史 按钮后会弹出 同步历史 窗口 !img1.png 在该窗口中会展示当前数据集的所有同步历史，包含手动同步和定时任务同步的历史记录。 如果任务同步失败会展示任务的错误信息，点击任务的状态按钮即可看到错误信息。 !img2.png 该操作只会在同步失败的状态下生效。",
    "url": "https://datacap.devlive.org/en/reference/get-started/dataset/history/home.html",
    "lang": "en"
  },
  {
    "title": "清空数据",
    "content": "数据集提供了清空数据的操作，可以通过数据集列表中在每行数据的末尾有 清空数据 按钮，如下窗口 !img.png !!! info \"提示\" 该操作只会在当前数据集有数据的情况下可以操作。 !!! 当数据集无数据的情况下，该菜单将不可操作。 点击 清空数据 按钮后会弹出 清空数据 窗口 !img1.png 该窗口中会展示当前数据集的 总行数 和 总大小 确定清空数据后，点击 清空数据 按钮后，系统会在后台创建清空数据任务，进行清空数据。",
    "url": "https://datacap.devlive.org/en/reference/get-started/dataset/clear/home.html",
    "lang": "en"
  },
  {
    "title": "新建 & 修改",
    "content": "!img.png 鼠标打开管理员菜单，点击 工作流 子菜单，跳转到工作流列表页面 !img1.png 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加工作流窗口 !img2.png 我们通过左侧拖拽需要的节点，拖拽后系统会自动进行配置的验证，默认验证失败会出现以下页面 !img3.png 如果验证成功，页面如下 !img4.png 配置完成后，左上角 发布 按钮可以点击，点击后弹出配置窗口 !img5.png 输入流程名称点击 提交 按钮即可完成工作流的创建，创建完成后跳转到工作流列表页面。 在工作流列表中，我们找到需要修改的工作流对应的数据行，点击操作栏的 设置 图标 !img6.png 点击修改流程选项，即可进入修改流程页面，操作方式和新建一致。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/create/home.html",
    "lang": "en"
  },
  {
    "title": "查看工作流错误",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的叹号图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中会显示当前工作流的错误信息。 !!! warning 只有工作流在执行失败的情况下，才可以查看错误信息。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/error/home.html",
    "lang": "en"
  },
  {
    "title": "停止工作流",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，按照提示输入，点击 停止流程 按钮，即可停止当前工作流的执行。 !!! warning 只有工作流在执行中的情况下，才可以停止工作流。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/stop/home.html",
    "lang": "en"
  },
  {
    "title": "重启工作流",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，按照提示输入，点击 重新启动 按钮，即可重新启动当前工作流的执行。 !!! warning 只有工作流在非执行中的情况下，才可以重新启动工作流。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/restart/home.html",
    "lang": "en"
  },
  {
    "title": "工作流日志",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，显示的是当前工作流的日志。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/logger/home.html",
    "lang": "en"
  },
  {
    "title": "删除工作流",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，按照提示输入，点击 删除流程 按钮，即可删除流程。 !!! warning 只有工作流在非执行中的情况下，才可以删除工作流。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/delete/home.html",
    "lang": "en"
  },
  {
    "title": "工作流视图",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，显示的是当前工作流的视图。",
    "url": "https://datacap.devlive.org/en/reference/get-started/workflow/dag/home.html",
    "lang": "en"
  },
  {
    "title": "命令行界面",
    "content": "DataCap CLI 提供基于终端的交互式 shell 来运行查询。 CLI 是一个自动执行的 JAR 文件，这意味着它的行为就像普通的 UNIX 可执行文件一样。 --- CLI 需要路径上有可用的 Java 虚拟机。它可以与 Java 版本 8 及更高版本一起使用。 CLI 使用基于 HTTP/HTTPS 的 DataCap 客户端 REST API 与系统进行通信。 CLI 版本应与系统版本相同或更高。 --- 下载 datacap-client-cli-1.6.0.jar, 将其重命名为 datacap，使用 chmod +x 命令将其设置为可执行。 --- bash ./datacap connect -h 127.0.0.1 -p 9096 -u username -P password 如果成功，您将收到执行命令的提示。使用 help 命令查看支持的命令列表。 | 命令 | 描述 | |-------------------------------|---------------------| | source info | 获取数据源详细信息 | | source list | 获取远程服务器数据源列表 | | source use <SourceID> | 设置数据源标志，以便后续对数据源的操作 | | source execute \"<QuerySQL>\" | 执行远程SQL |",
    "url": "https://datacap.devlive.org/en/reference/clients/cli.html",
    "lang": "en"
  },
  {
    "title": "个人资料",
    "content": "!!! info \"提示\" 通过个人资料功能，可以对个人用户的一些查询历史和数据源的使用进行一些概览查看。 !!! 鼠标移向顶部菜单的最右侧的头像菜单，会弹出下拉框，点击下拉框中的第一个子菜单。弹出类似如下窗口： !img.png 首先我们看到的最上面的一个图表是近一年的查询日历图。它根据每天的查询总数汇总来计算。 第二个图表是近7日的数据源使用情况。它根据每天的数据源使用情况汇总来计算。 --- 当点击个人资料按钮后（也就是左侧菜单的第一个按钮），会弹出如下窗口： !img1.png 该页面主要展示了个人资料的基本信息。 --- 当点击登录日志按钮后（也就是左侧菜单的第二个按钮），会弹出如下窗口： !img2.png 该页面主要展示了当前用户的详细登录日志，包括了登录时间、登录地点、登录方式等。 --- 当点击账号设置按钮后（也就是左侧菜单的第三个按钮），会弹出如下窗口： !img3.png 该页面主要展示了一些用户可以进行的配置功能。 --- 当点击修改用户名按钮后，会弹出如下窗口： !img4.png 输入修改后的用户名和当前密码，点击确定按钮即可完成修改。 !!! danger \"警告\" 需要注意的是：修改用户名后，需要重新登录。系统会默认退出当前账号。 !!! --- 当点击修改密码按钮后，会弹出如下窗口： !img5.png 输入原密码和新密码，点击确定按钮即可完成修改。 !!! danger \"警告\" 需要注意的是：修改密码后，需要重新登录。系统会默认退出当前账号。 !!! --- 当点击 ChatGPT 按钮后，会弹出如下窗口： !img6.png 该页面主要展示了一些 ChatGPT 可以进行的配置功能。 --- 当点击编辑器按钮后，会弹出如下窗口： !img7.png 在修改编辑器时，修改的配置会同步到系统中的所有用到编辑器的位置。 编辑器的修改是所见即所得的状态，可以实时显示出来当前的配置方案。",
    "url": "https://datacap.devlive.org/en/reference/manager/profile/home.html",
    "lang": "en"
  },
  {
    "title": "数据源",
    "content": "!!! info \"提示\" 通过数据源功能，可以添加对各种自定义数据源的支持，并执行后续的数据源操作等。 !!! 鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的第一个子菜单。弹出类似如下窗口，默认列表为空，需要自行添加。 !img.png 如果您添加了数据源会显示类似如下页面 !img1.png --- 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加数据源窗口 !img2.png 当我们选择某种类型的数据源时，数据源配置信息将显示在顶部标签栏中，不同的数据源有不同的配置项，它的配置单在服务启动是的指定目录中。 当我们选择类型为 MySQL 的源时，弹出类似以下窗口 !img3.png 在配置页面中出现了4个选项卡，点击不同的选项卡填充相关信息，然后点击底部的 Test 按钮，会弹出如下页面： !img4.png 当数据源测试成功后顶部会展示当前服务测试后的版本号，此时点击地步的 Save 按钮即可保存数据。 !!! info \"提示\" 数据源保存后，数据源列表会自动刷新。 !!! --- 点击列表中某个数据源中 Action 中的第一个按钮即可修改数据源，操作类似于 添加数据源 操作 --- 点击列表中某个数据源的 Action 中的第二个按钮，删除该数据源，点击后会弹出以下内容 !img5.png 单击弹出的小窗口，然后单击 OK 以删除选中的数据源。 !!! danger \"注意\" 需要注意的是，删除数据源后，与数据源相关的查询历史记录将被删除。 !!! --- 单击列表中数据源的 Action 中的第三个或者第四个按钮，跳转到数据源管理页面。 !img6.png 页面分为左右两部分。左侧主要展示数据源的基本信息，包括： 选中数据源的相关元数据 当我们在左侧选择数据库和数据表时，右侧的内容显示如下 !img7.png 在右侧内容中出现两个选项卡: Info（默认选项） Data !!! info \"提示\" 默认当前选项卡下显示关于当前表的相关信息。 !!! 点击 Data 选项卡，会出现类似如下页面，它展示了当前选中表的相关数据。 !img8.png 在顶部的四个按钮分别是： First Page Previous Page Next Page Last Page 接下来后面的按钮是用于设置数据查询的配置： !img9.png Jump to Page Show Page Size 填充配置后，点击 Apply 按钮即可应用当前配置信息。 在右侧还有一个按钮，点击后会展示当前查询使用到的详细 SQL 内容 !img10.png !!! info \"提示\" 当前 SQL 生成是根据同步到元数据的顺序而定。 !!! !!! danger \"注意\" 目前并不是所有的数据源都支持管理，如果需要可自行添加模版。如果有兴趣可将源码贡献给我们。 !!!",
    "url": "https://datacap.devlive.org/en/reference/manager/datasource/home.html",
    "lang": "en"
  },
  {
    "title": "代码片段",
    "content": "!!! note 通过代码段功能，可以添加对各种自定义代码段的支持，并执行后续的代码段操作等。添加的代码片段后续会添加到编辑器中。 !!! 鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 Snippet 子菜单。弹出类似如下窗口，默认列表为空，需要自行添加。 !img.png 如果您添加了片段会显示类似如下页面 !img1.png --- 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加数据源窗口 !img2.png 在窗口中，我们需要输入以下内容 | 属性 | 描述 | |:-------------:|:----------------:| | Name | 标记当前代码段的名称 | | Description | 当前代码片段的说明 | | Snippet | 当前代码片段的特定 SQL 内容 | 填写完以上内容后，点击底部的 Submit 按钮保存代码片段。 !!! note 数据保存后，数据源列表会自动刷新。 !!! --- 点击列表中某个数据中 Action 中的第一个按钮，查看具体的代码片段内容，会弹出一个对话框，大致如下 !img3.png 单击 OK 或 Cancel 关闭对话框 --- 点击列表中某个数据中 Action 中的第二个按钮即可修改代码段，该操作类似于 添加片段 操作。 --- 点击列表中某个数据的 Action 中的第三个按钮，引用当前代码片段，会跳转到查询页面，片段内容会直接输入到编辑器中。 --- 点击列表中某个数据的 Action 中的第四个按钮，删除代码段，点击后会弹出以下内容 !img4.png 单击弹出的小窗口，然后单击 OK 以删除代码段。",
    "url": "https://datacap.devlive.org/en/reference/manager/snippet/home.html",
    "lang": "en"
  },
  {
    "title": "查询历史",
    "content": "鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 History 子菜单。弹出类似如下窗口，默认列表为空，通过查询页面进行查询即可自动添加记录。 !img.png --- 点击列表中某个数据中 Action 中的第一个按钮，查看具体的代码片段内容，会弹出一个对话框，大致如下 !img1.png 此查询中查询的特定 SQL 语句将显示在窗口中。 --- !!! danger 只有查询出现异常情况下，才可以查看错误信息。 !!! 点击列表中某个数据中 Action 中的第二个按钮即可查看错误信息 !img2.png",
    "url": "https://datacap.devlive.org/en/reference/manager/history/query/home.html",
    "lang": "en"
  },
  {
    "title": "流水线",
    "content": "在 DataCap 软件中流水线用于用户针对数据进行迁移等一些数据操作的工具。 --- 进入系统后，点击顶部 Admin 菜单下对应的 Pipeline 子菜单，默认进入流水线列表。类似下图： !img.png 点击列表右侧的 + Create 按钮，系统会弹出配置页面： !img1.png 配置页面分为三个配置模块，分别是： Description 主要配置用户执行的 SQL From 配置数据接入源 To 配置数据输出源 每个数据源具有不同的配置属性，当选择数据源后按照提示配置相关属性即可。 当任务发布成功后默认会刷新任务列表，如下图所示： !img2.png --- 任务发布后默认将会启动，在任务的右侧 Action 操作中包含以下功能： 当我们点击 Action 中的第一个按钮时，系统会弹出错误信息页面。 > 只有任务运行失败后该功能才会被启用 !img3.png 当我们点击 Action 中的第二个按钮时，系统会弹出日志页面。 !img4.png 当我们点击 Action 中的第三个按钮时，系统会弹出停止任务页面。 > 只有任务运行中该功能才会被启用 !img5.png 我们按照窗口提示的信息输入任务名字后点击 Stop 按钮即可。 当我们点击 Action 中的第四个按钮时，系统会弹出删除任务页面。 !img6.png 我们按照窗口提示的信息输入任务名字后点击 Delete 按钮即可。",
    "url": "https://datacap.devlive.org/en/reference/manager/pipeline/home.html",
    "lang": "en"
  },
  {
    "title": "报表",
    "content": "鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 Report 子菜单。弹出类似如下窗口，默认列表为空，通过查询页面进行查询即可添加报表。 !img.png --- 新建报表功能在查询页面，当我们查询成功后会在结果表格上方显示出如下窗口 !img1.png 点击图中红框选择的按钮，弹出如下窗口 !img2.png 顶部为报表名称和是否实时报表 !!! note 如果是实时报表，那么报表数据会在展示时进行后端服务查询从而渲染，不同的查询引擎这可能会造成报表渲染慢的问题。非实时报表只会展示当前查询的静态数据。 !!! 左侧是报表渲染区域，右侧为报表的配置，目前支持报表的类型： 折线图 柱状图 以下是一个我们配置好的报表： !img3.png 报表配置完成后，点击发布即可将报表保留下来。 --- 在报表列表页面查询记录后有操作按钮，操作按钮的第一个 !img4.png{ width=\"20\" } 点击后可以查看报表 !img5.png --- 在报表列表页面查询记录后有操作按钮，操作按钮的第二个 !img6.png{ width=\"20\" } 点击后可以弹出如下窗口 !img7.png 在输入框中输入报表名称，点击删除即可删除该报表。",
    "url": "https://datacap.devlive.org/en/reference/manager/report/home.html",
    "lang": "en"
  },
  {
    "title": "Functions",
    "content": "!!! note We can enhance the automatic prompt function of the code editor through the function module provided by the system. !!! --- The system has built-in the following data source functions by default (it does not mean that it is the most complete function of the new version, if there is something missing, please submit issues or pr to fix it): ClickHouse MySQL Hive Trino & Presto (fit a part) --- After entering the system, click the corresponding Function submenu under the top Settings menu to go to the function configuration function !img.png Click the Add button on the top right to add a new function, and the following window will pop up after clicking: !img1.png The following is a detailed parameter description: Name: The name used to mark the function prompt, the suggestion is English Plugin: The plugin this function applies to, multiple options can be selected Content: The specific content of the function, which will be entered into the editor Description: Description of the function Type: Type of function, can be: KeyWord, Operator, Function, default is KeyWord Example: For the use example of this function, it is convenient for users to understand how to use the function When the above content is written, click the Submit button at the bottom to save the operation, and you can use it in the editor later. --- The system provides a way to import functions in batches. Currently, it supports the import of content and URI addresses. Next, let's take a look at how to do it. We perform the batch import function by clicking the import button on the top right. !img2.png The content import method allows us to enter a list of functions, and they are divided according to each line. Adding the following keywords we need to import: sql SHOW USE In Plugin, we choose to use the ClickHouse plug-in, and in Type, we choose KeyWord. After the operation is completed, we click the Submit button at the bottom to use the import function of the current input function. The URI import method is relatively simple. We can import data in batches by specifying the remote server URI address, which can be your local server address or the address provided by the software. The URI address format provided by the software by default is sql (http|https)://datacap.edurt.io/resources/functions/plugin/keywords.txt (http|https)://datacap.edurt.io/resources/functions/plugin/operators.txt (http|https)://datacap.edurt.io/resources/functions/plugin/functions.txt We only need to replace the value of plugin in the address with the name of the plugin that needs to be imported. !!! warning It should be noted that due to local network problems, the URI import method may be slow. !!!",
    "url": "https://datacap.devlive.org/en/reference/system/functions/home.html",
    "lang": "en"
  },
  {
    "title": "Sql Template",
    "content": "!!! note The system supports the SQL template function, through which the realization of some monitoring and other functions can be supported. !!! --- The system supports some default templates, currently supports: getAllDatabaseAndTable getAllDatabase getAllTablesFromDatabase Of course, each template can support one or more plug-ins, and they will be used in subsequent operations of the system. After entering the system, click the corresponding Sql submenu under the top Settings menu to go to the function configuration function !img.png Click the Add button on the top right to add a new function, and the following window will pop up after clicking: !img1.png The following is a detailed parameter description: Name: The name used to mark the function prompt, the suggestion is English Plugin: The plugin this function applies to, multiple options can be selected Description: Description of the function Template: The SQL statement executed by the template When the above content is written, click the Submit button at the bottom to save the operation, and you can use it in the editor later. !!! warning The default template does not carry any parameters and we can execute it directly. !!! We can realize the template dynamic parameter passing function by defining variables. Let's take an example, we need to display all the data tables under the default database, the normal SQL is sql SHOW TABLES FROM default When we use the template, the SQL changes to sql SHOW TABLES FROM ${database:String} The system parses the parameter into database=String by collecting {database:String} expression, where database is the parameter name, and String is the type of parameter passing. When we use the expression time, we only need to pass the Map type parameter, where key=parameter name, value=data value passed according to the type.",
    "url": "https://datacap.devlive.org/en/reference/system/sql/home.html",
    "lang": "en"
  },
  {
    "title": "菜单",
    "content": "鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 Menu 子菜单。弹出类似如下窗口，默认列表为空，通过查询页面进行查询即可自动添加记录。 !img.png --- 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加数据源窗口 !img1.png 填写完以上内容后，点击底部的 Submit 按钮保存代码片段。 !!! note 数据保存后，数据源列表会自动刷新。 !!! --- 点击列表中某个数据中 Action 中的第一个按钮，会弹出修改窗口，它和新建菜单一致。",
    "url": "https://datacap.devlive.org/en/reference/system/menu/home.html",
    "lang": "en"
  },
  {
    "title": "InfluxDB",
    "content": "InfluxDB 是一个由 InfluxData 开发的开源时序型数据库。它由 Go 写成，着力于高性能地查询与存储时序型数据。InfluxDB 被广泛应用于存储系统的监控数据。 --- DataCap 整合 InfluxDB 模块，用于实现对 InfluxDB 数据源的数据操作。 在该模块中我们使用的是 influxdb-jdbc 依赖，版本为 0.2.6。 使用到的驱动器为 net.suteren.jdbc.influxdb.InfluxDbDriver。 驱动源码可参考：https://github.com/konikvranik/jdbc-influxdb/ --- | 操作 | 是否支持 | |:--------:|:----:| | SELECT | ✅ | > 所有驱动器支持的操作均支持。 --- !!! note 如果需要使用该数据源，需要将 DataCap 服务升级到 >= 2024.03.7 !!! 支持时间: 2024-06-26 --- !!! note 如果您的服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"基本配置\" | 属性 | 是否必要 | 默认值 | 备注 | |---|---|---|---| | 名称 | ✅ | - |-| | 主机地址 | ✅ | 127.0.0.1 | - | | 端口 | ✅ | 8086 | - | === \"授权配置\" | 属性 | 是否必要 | 默认值 | |---|---|---| | 用户名 | ❌ | - | | 密码 | ❌ | - | === \"自定义\" > 可以添加所有 InfluxDB 驱动支持的配置，方式为 key = value 默认: | 属性 | 默认值 | |---|---| | database | default | :::",
    "url": "https://datacap.devlive.org/en/reference/connectors/influxdb/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Solr",
    "content": "Apache Solr 是基于 Apache Lucene™ 构建的快速开源搜索平台，提供可扩展的索引和搜索，以及分面、命中突出显示和高级分析/标记化功能。Solr 由 Apache 软件基金会管理。 --- DataCap 整合 Apache Solr 模块，用于实现对 Apache Solr 数据源的数据操作。 在该模块中我们使用的是 solr-solrj 依赖，版本为 6.0.0。 使用到的驱动器为 org.apache.solr.client.solrj.io.sql.DriverImpl。 --- | 操作 | 是否支持 | |:--------:|:----:| | SELECT | ✅ | > 所有驱动器支持的操作均支持。 --- !!! note 如果需要使用该数据源，需要将 DataCap 服务升级到 >= 2024.03.7 !!! 支持时间: 2024-06-25 --- !!! note 如果您的服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"基本配置\" | 属性 | 是否必要 | 默认值 | 备注 | |---|---|---|---| | 名称 | ✅ | - |-| | 主机地址 | ✅ | 127.0.0.1 | 这里的地址需要填写 zookeeper 的相关地址 | | 端口 | ✅ | 8983 | - | === \"授权配置\" | 属性 | 是否必要 | 默认值 | |---|---|---| | 用户名 | ❌ | - | | 密码 | ❌ | - | === \"高级配置\" | 属性 | 是否必要 | 默认值 | 备注 | |---|---|---|---| | 数据库 | ✅ | local | 这里是指在 solr 服务器中创建的 collection | === \"自定义\" > 可以添加所有 Solr 驱动支持的配置，方式为 key = value 暂无默认配置 :::",
    "url": "https://datacap.devlive.org/en/reference/connectors/solr/home.html",
    "lang": "en"
  },
  {
    "title": "Timescale",
    "content": "Timescale 是一个用于时间序列、事件和分析的 PostgreSQL 数据平台。 它为您提供 PostgreSQL 的可靠性、TimescaleDB 的时间序列超能力以及完全托管服务的安心。 它提供自动备份和恢复、复制的高可用性、无缝扩展和调整大小等功能。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.02.1 !!! 支持时间: 2024-02-05 --- ::: tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |------|---------------------------------|-------------| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5432 | === \"高级\" | 属性 | 是否必要 | 默认值 | |----------|--------------------------------|-------------| | Database | :check{20,#3CA34F}: | | ::: --- | 功能 | 支持 | |:-----:|:----------------:| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :x: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] all !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/timescale/home.html",
    "lang": "en"
  },
  {
    "title": "ParadeDB",
    "content": "ParadeDB 是基于 Postgres 构建的 Elasticsearch 替代方案。我们正在对 Elasticsearch 的功能进行现代化改造， 从实时搜索和分析开始。 ParadeDB 不是 Postgres 的分支，而是安装了自定义扩展的常规 Postgres。ParadeDB 本身 随 Postgres 16 一起提供。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.02.1 !!! 支持时间: 2024-02-05 --- :::tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |------|---------------------------------|-------------| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5432 | === \"高级\" | 属性 | 是否必要 | 默认值 | |----------|--------------------------------|-------------| | Database | :check{20,#3CA34F}: | | ::: --- | 功能 | 支持 | |-----|----------------| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :x: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues !!! [x] all",
    "url": "https://datacap.devlive.org/en/reference/connectors/paradedb/home.html",
    "lang": "en"
  },
  {
    "title": "ScyllaDB",
    "content": "ScyllaDB 是一个分布式 NoSQL 宽列数据库，适用于需要高性能和低延迟的数据密集型应用程序。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.01.1 !!! 支持时间: 2024-01-12 --- ::: tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9042 | === \"高级\" | 属性 | 是否必要 | 默认值 | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | | ::: --- | 功能 | 支持 | |:-----:|:----------------:| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :x: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] all !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/scylladb/home.html",
    "lang": "en"
  },
  {
    "title": "MatrixOne",
    "content": "MatrixOne 是一款面向未来的超融合云和边缘原生 DBMS，通过简化的分布式数据库引擎，跨多个数据中心、云、边缘和其他异构基础设施支持事务、分析和流式工作负载。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.01.1 !!! 支持时间: 2024-01-12 --- :::tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 6001 | === \"授权\" | 属性 | 是否必要 | 默认值 | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | - | | Password | :check{20,#3CA34F}: | - | | SSL | :x{20,#ff0000}: | false | === \"高级\" | 属性 | 是否必要 | 默认值 | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | | === \"自定义\" 暂无默认配置，支持用户自定义添加 ::: --- | 功能 | 支持 | |:-----:|:----------------:| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :check: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] 1.1.0 !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/matrixone/home.html",
    "lang": "en"
  },
  {
    "title": "ClickHouse",
    "content": "ClickHouse® 是一个面向列的数据库管理系统（DBMS），用于在线分析查询处理（OLAP）。ClickHouse 的性能超过了所有其他面向列的数据库管理系统。它每台服务器每秒处理数十亿行和数十 GB 的数据。 !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 1.0.x !!! 支持时间: 2022-09-22 --- !!! note 如果您的 ClickHouse 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9000 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | === \"自定义\" !!! note 您可以通过添加 Key Value 来添加已经支持的 ClickHouse 参数，参数可以是 参考文档 !!! ::: --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] 19.x [x] 20.x [x] 21.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/clickhouse/home.html",
    "lang": "en"
  },
  {
    "title": "MySQL",
    "content": "MySQL is one of the most recognizable technologies in the modern big data ecosystem. Often called the most popular database and currently enjoying widespread, effective use regardless of industry, it’s clear that anyone involved with enterprise data or general IT should at least aim for a basic familiarity of MySQL. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.0.x !!! Support Time: 2022-09-19 --- DataCap uses configuration files by default mysql.json !!! note If your MySQL service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 3306 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :check{20,#3CA34F}: | \\- | | SSL | :x{20,#ff0000}: | false | === \"Advanced\" | Field | Required | Default Value | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | default | === \"Custom\" !!! note You can add the already supported MySQL parameters by adding Key Value, the parameters can be reference document Default: | Key | value | |:-----------------------------:|:------:| | useOldAliasMetadataBehavior | true | !!! ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 5.x [x] 6.x [x] 7.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/mysql/home.html",
    "lang": "en"
  },
  {
    "title": "Redis",
    "content": "The open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.3.x !!! Support Time: 2022-12-01 --- DataCap uses configuration files by default redis.json !!! note If your Redis service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 6379 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Password | :x{20,#ff0000}: | \\- | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 6.x [x] 7.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/redis/home.html",
    "lang": "en"
  },
  {
    "title": "H2 Database",
    "content": "H2 is an embedded database developed in Java that is itself just a class library. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.8.x !!! Support Time: 2023-04-05 --- !!! note If your h2 service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | === \"Authorization\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"Advanced\" | Field | Required | Default Value | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] all !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/h2/home.html",
    "lang": "en"
  },
  {
    "title": "Snowflake",
    "content": "Execute your most critical workloads on top of Snowflake's multi-cluster shared data architecture in a fully managed platform that capitalizes on the near-infinite resources of the cloud. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.4.x !!! Support Time: 2023-01-29 --- DataCap uses configuration files by default snowflake.json !!! note If your Snowflake service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 80 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :check{20,#3CA34F}: | \\- | === \"Advanced\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | === \"Custom\" !!! note You can add the already supported Snowflake parameters by adding Key Value, the parameters can be reference document !!! ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/snowflake/home.html",
    "lang": "en"
  },
  {
    "title": "Yandex Database",
    "content": "YDB is a fault-tolerant distributed SQL DBMS. YDB provides high availability, horizontal scalability, strict consistency, and ACID transaction support. Queries are made using an SQL dialect (YQL). !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.4.x !!! Support Time: 2023-01-30 --- DataCap uses configuration files by default ydb.json !!! note If your YDB service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 2136 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"Advanced\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | local | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 2.1.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/ydb/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Zookeeper",
    "content": "ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.5.x !!! Support Time: 2023-02-07 --- This chapter describes the SQL syntax used in Zookeeper on DataCap. !!! note \"Synopsis\" sql SELECT [ | <Columns> ] selectexpression [, ...] FROM fromitem [. ...] where fromitem is one of sql tablename [ a.b | a.b | a.b] !!! !!! danger When tablename is set to all the root directory is searched. !!! !!! note \"Select expressions\" Each selectexpression must be in one of the following forms: sql expression [ columnalias ] sql In the case of expression [ columnalias ], a single output column is defined. In the case of , all columns of the relation defined by the query are included in the result set. sql -------- data !!! !!! danger If it is a multi-level directory, such as /zookeeper/id/2, it will be written \\zookeeper\\.\\id\\.\\2\\, and use . to split between directories. !!! --- DataCap uses configuration files by default zookeeper.json !!! note If your Zookeeper service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1:2181 | | Port | :x{20,#ff0000}: | 1 | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 3.1.x - 3.7.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/zookeeper/home.html",
    "lang": "en"
  },
  {
    "title": "DuckDB",
    "content": "DuckDB is an in-process SQL OLAP database management system. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.6.x !!! Support Time: 2023-02-20 --- DataCap uses configuration files by default duckdb.json !!! note If your DuckDB service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | /root | | Port | :check{20,#3CA34F}: | 0 | === \"Advanced\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | local | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 0.7.0 !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/duckdb/home.html",
    "lang": "en"
  },
  {
    "title": "AliYun OSS",
    "content": "Fully managed object storage service to store and access any amount of data from anywhere --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.6.x !!! Support Time: 2023-02-23 --- This chapter describes the SQL syntax used in Aliyun OSS on DataCap. !!! note \"Synopsis\" sql SELECT [ | <Columns> ] selectexpression [, ...] FROM fromitem [. ...] where fromitem is one of sql tablename [ a.b | a.b | a.b] !!! !!! danger When tablename is set to all the root directory is searched. !!! !!! note \"Select expressions\" Each selectexpression must be in one of the following forms: sql expression [ columnalias ] sql In the case of expression [ columnalias ], a single output column is defined. In the case of , all columns of the relation defined by the query are included in the result set. sql -------- data !!! !!! danger If it is a multi-level directory, such as /oss/id/2, it will be written \\oss\\.\\id\\.\\2\\, and use . to split between directories. !!! --- DataCap uses configuration files by default alioss.json !!! note If your Aliyun OSS service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------------------------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | https://oss-cn-regison.aliyuncs.com | === \"Authorization\" | Field | Required | Description | Default Value | |:----------:|:---------------------------------:|:-------------:|:-------------:| | Username | :check{20,#3CA34F}: | access Id | \\- | | Password | :check{20,#3CA34F}: | access Secret | \\- | === \"Advanced\" | Field | Required | Description | Default Value | |:----------:|:--------------------------------:|:-----------:|:-------------:| | Database | :check{20,#3CA34F}: | bucket name | default | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] all version !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/alioss/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Kafka",
    "content": "Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.7.x !!! Support Time: 2023-03-06 --- This chapter describes the SQL syntax used in DataCap Kafka plugin. SHOW TOPICS SHOW CONSUMERS --- DataCap uses configuration files by default kafka.json !!! note If your Apache Kafka service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:----------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | localhost:9092 | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 1.0.0 [x] 1.1.0 [x] 1.2.0 !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/kafka/home.html",
    "lang": "en"
  },
  {
    "title": "SHOW TOPICS",
    "content": "--- sql SHOW TOPICS --- Returns a list of all defined topics in the current cluster. Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/en/reference/sql-syntax/connectors/kafka/show_topics.html",
    "lang": "en"
  },
  {
    "title": "SHOW CONSUMERS",
    "content": "--- sql SHOW CONSUMERS SHOW CONSUMERS FROM topic --- Returns a list of all defined consumers in the current cluster (if a topic is specified, a list of consumers for the specified topic will be returned). Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/en/reference/sql-syntax/connectors/kafka/show_consumers.html",
    "lang": "en"
  },
  {
    "title": "SHOW DATABASES",
    "content": "--- sql SHOW DATABASES --- Returns a list of all defined topics in the current cluster. Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/en/reference/sql-syntax/connectors/kafka/show_databases.html",
    "lang": "en"
  },
  {
    "title": "SHOW TABLES",
    "content": "--- sql SHOW TABLES SHOW TABLES FROM database > database kafka topic name --- Returns a list of all defined consumers in the current cluster (if a topic is specified, a list of consumers for the specified topic will be returned). Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/en/reference/sql-syntax/connectors/kafka/show_tables.html",
    "lang": "en"
  },
  {
    "title": "CeresDB",
    "content": "CeresDB 是一款高性能、分布式的云原生时序数据库。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-12 --- !!! note 如果您的CeresDB服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5440 | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 1.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/ceresdb/home.html",
    "lang": "en"
  },
  {
    "title": "GreptimeDB",
    "content": "一个开源、云原生、分布式时间序列数据库，支持PromQL/SQL/Python。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-14 --- !!! note 如果您的 GreptimeDB 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5440 | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 0.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/greptimedb/home.html",
    "lang": "en"
  },
  {
    "title": "QuestDB",
    "content": "QuestDB 是一个开源时间序列数据库，用于高吞吐量摄取和快速 SQL 查询，操作简单。它支持使用 InfluxDB 行协议、PostgreSQL 有线协议和用于批量导入和导出的 REST API 进行与模式无关的摄取。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-17 --- !!! note 如果您的 QuestDB 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9000 | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 7.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/questdb/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Doris",
    "content": "一个易于使用、高性能和统一的分析数据库 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-19 --- !!! note 如果您的 Doris 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9093 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/doris/home.html",
    "lang": "en"
  },
  {
    "title": "StarRocks",
    "content": "开源、高性能的分析数据库 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-20 --- !!! note 如果您的 StarRocks 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9030 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 2.2.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/starrocks/home.html",
    "lang": "en"
  },
  {
    "title": "Hologres",
    "content": "Hologres是兼容PostgreSQL的一站式实时数据仓库引擎，支持PB级数据多维分析（OLAP）与即席分析（Ad Hoc），支持高并发低延迟的在线数据服务（Serving）。与MaxCompute、Flink、DataWorks深度融合，提供离在线一体化全栈数仓解决方案。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-25 --- !!! note 如果您的 Hologres 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | hologres-cn-regison.aliyuncs.com | | Port | :check{20,#3CA34F}: | 80 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] all !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/hologres/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Hadoop HDFS",
    "content": "The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.9.x !!! Support Time: 2023-04-27 --- !!! note If your HDFS service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------------------------------:| | Name | :check{20,#3CA34F}: | \\- | === \"Advanced\" | Field | Required | Description | Default Value | |:----------:|:--------------------------------:|:-----------:|:-------------:| | file | :check{20,#3CA34F}: | core-site.xml <br /> hdfs-site.xml | [] | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 3.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/hdfs/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Pinot",
    "content": "Apache Pinot 是一个实时分布式 OLAP 数据存储，专为低延迟、高吞吐量分析而构建，非常适合面向用户的分析工作负载。 --- !!! note 如果需要使用该数据源，需要将DataCap服务升级到 >= 1.10.x !!! 支持时间: 2023-05-06 --- !!! note 如果您的服务版本需要其他特殊配置，请参考修改配置文件并重启DataCap服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9000 | === \"授权\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :check{20,#3CA34F}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 0.8.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/pinot/home.html",
    "lang": "en"
  },
  {
    "title": "Apache Cassandra",
    "content": "Apache Cassandra® powers mission-critical deployments with improved performance and unparalleled levels of scale in the cloud. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.11.x !!! Support Time: 2023-06-07 --- !!! note If your plugin service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9042 | === \"Advanced\" | Field | Required | Default Value | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | datacenter | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 0.4.x !!!",
    "url": "https://datacap.devlive.org/en/reference/connectors/cassandra/home.html",
    "lang": "en"
  },
  {
    "title": "Local (本地)",
    "content": "在 DataCap 中目前已经支持 Local，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=Local datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 Local datacap.fs.access: 不做任何设置 datacap.fs.secret: 不做任何设置 datacap.fs.endpoint: 不做任何设置 datacap.fs.bucket: 如果设置则为目录的名称，不设置则不做任何设置 --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-local</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： java package io.edurt.datacap.fs; import com.google.inject.Guice; import com.google.inject.Injector; import com.google.inject.Key; import com.google.inject.TypeLiteral; import lombok.extern.slf4j.Slf4j; import org.junit.Assert; import org.junit.Before; import org.junit.Test; import java.io.BufferedReader; import java.io.File; import java.io.IOException; import java.io.InputStreamReader; import java.nio.charset.StandardCharsets; import java.util.Optional; import java.util.Set; @Slf4j public class LocalFsTest { private Injector injector; private FsRequest request; @Before public void before() { injector = Guice.createInjector(new FsManager()); request = FsRequest.builder() .access(null) .secret(null) .endpoint(String.join(File.separator, System.getProperty(\"user.dir\"), \"data\")) .bucket(\"tmp\") .localPath(String.join(File.separator, System.getProperty(\"user.dir\"), \"src/main/java/io/edurt/datacap/fs/LocalFs.java\")) .fileName(\"LocalFs.java\") .build(); } @Test public void test() { Set<Fs> sets = injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})); Assert.assertEquals(\"Local\", sets.stream().findFirst().get().name()); } @Test public void writer() { Optional<Fs> optional = injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})).stream().findFirst(); if (optional.isPresent()) { Assert.assertEquals(\"Local\", optional.get().name()); } FsResponse response = optional.get().writer(request); Assert.assertEquals(true, response.isSuccessful()); } @Test public void reader() { Optional<Fs> optional = injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})).stream().findFirst(); if (optional.isPresent()) { Assert.assertEquals(\"Local\", optional.get().name()); } FsResponse response = optional.get().reader(request); Assert.assertEquals(true, response.isSuccessful()); log.info(\"====== [ {} ] ======\", response.getRemote()); try (BufferedReader reader = new BufferedReader(new InputStreamReader(response.getContext(), StandardCharsets.UTF8))) { String line; while ((line = reader.readLine()) != null) { log.info(line); } } catch (IOException e) { log.error(\"Reader error\", e); } } @Test public void testDelete() { injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})) .stream() .findFirst() .ifPresent(it -> { FsResponse response = it.delete(request); Assert.assertTrue(response.isSuccessful()); }); } }",
    "url": "https://datacap.devlive.org/en/reference/filesystem/local/home.html",
    "lang": "en"
  },
  {
    "title": "阿里云 OSS",
    "content": "在 DataCap 中目前已经支持阿里云 OSS，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=AliOss datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 AliOss datacap.fs.access: 阿里云 OSS 的 AccessKey datacap.fs.secret: 阿里云 OSS 的 SecretKey datacap.fs.endpoint: 阿里云 OSS 的 Endpoint，如 oss-cn-hangzhou.aliyuncs.com datacap.fs.bucket: 阿里云 OSS 的 Bucket，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-alioss</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.alioss import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class AliOssFsTest { private val log = LoggerFactory.getLogger(AliOssFsTest::class.java) private val name = \"AliOss\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"IOUtilsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/alioss/IOUtilsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun testDelete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/en/reference/filesystem/aliyun/home.html",
    "lang": "en"
  },
  {
    "title": "七牛云存储",
    "content": "在 DataCap 中目前已经支持七牛云，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=Qiniu datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 Qiniu datacap.fs.access: 七牛云的 AccessKey datacap.fs.secret: 七牛云的 SecretKey datacap.fs.endpoint: 七牛云的 Endpoint datacap.fs.bucket: 七牛云的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-qiniu</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.qiniu import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets import kotlin.test.assertTrue class QiniuFsTest { private val log = LoggerFactory.getLogger(QiniuFsTest::class.java) private val name = \"Qiniu\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"IOUtilsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/qiniu/IOUtilsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun testDelete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/en/reference/filesystem/qiniu/home.html",
    "lang": "en"
  },
  {
    "title": "腾讯云 COS",
    "content": "在 DataCap 中目前已经支持腾讯云 COS，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=TencentCos datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 TencentCos datacap.fs.access: 腾讯云的 AccessKey datacap.fs.secret: 腾讯云的 SecretKey datacap.fs.endpoint: 腾讯云的 Endpoint datacap.fs.bucket: 腾讯云的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-tencent-cos</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.cos import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class TencentCosFsTest { private val log = LoggerFactory.getLogger(TencentCosFsTest::class.java) private val name = \"TencentCos\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"TencentCosFsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/cos/TencentCosFsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun delete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/en/reference/filesystem/cos/home.html",
    "lang": "en"
  },
  {
    "title": "亚马逊云 S3",
    "content": "在 DataCap 中目前已经支持亚马逊云 S3，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=AmazonS3 datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 AmazonS3 datacap.fs.access: 亚马逊云的 AccessKey datacap.fs.secret: 亚马逊云的 SecretKey datacap.fs.endpoint: 亚马逊云的 Endpoint，如 cn-north-1，不需要输入整个的地址 datacap.fs.bucket: 亚马逊云的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-amazon-s3</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.s3 import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class AmazonS3FsTest { private val log = LoggerFactory.getLogger(AmazonS3FsTest::class.java) private val name = \"AmazonS3\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"TencentCosFsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/s3/AmazonS3FsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun delete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/en/reference/filesystem/s3/home.html",
    "lang": "en"
  },
  {
    "title": "MinIO",
    "content": "在 DataCap 中目前已经支持 MinIO，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=MinIO datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 MinIO datacap.fs.access: MinIO 的 AccessKey datacap.fs.secret: MinIO 的 SecretKey datacap.fs.endpoint: MinIO 的 Endpoint，如 http://localhost:9000 datacap.fs.bucket: MinIO 的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-minio</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.minio import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory.getLogger import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class MinIOFsTest { private val log = getLogger(this::class.java) private val name = \"MinIO\" private val fileName = \"MinIOUtilsTest.kt\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = fileName request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/minio/${fileName}\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun delete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/en/reference/filesystem/minio/home.html",
    "lang": "en"
  },
  {
    "title": "latest",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:----------:|:------------:| | 2024.4.1 | 2024-12-31 | 尊敬的 DataCap 用户： DataCap 2024.4.1 版本现已正式发布。本次更新包含多项重要功能升级和性能优化，现将主要更新内容公布如下： 数据库功能增强 (实现功能) 新增数据库管理功能：支持创建、删除和切换数据库 完善表管理功能：支持创建表、删除表、插入数据等操作 MongoDB 驱动全面升级 优化查询性能：将 find 替换为 aggregate 新增元数据支持 支持 com.dbschema.MongoJdbcDriver 适配 完善版本控制和索引获取功能 工作流引擎优化 新增工作流任务提交功能 支持工作流重启操作 优化 SeatTunnel 执行器，支持自定义节点类型 SQL 解析器增强 优化 G4 表达式结构 新增 SHOW 语句支持 完善 SELECT 语句格式化功能 支持 CREATE DATABASE 语法 修复查询历史记录创建问题 修复 LocalDateTime 类型 JSON 转换问题 修复历史数据获取失败问题 优化达梦数据库插件版本获取 完善 Windows 平台支持 优化发布脚本 优化 CI/CD 流程 GitHub：https://github.com/devlive-community/datacap 官方网站：https://datacap.devlive.org/ Docker：已更新最新镜像 感谢社区用户一直以来的支持与反馈。如有问题或建议，欢迎通过 GitHub Issues 与我们交流。",
    "url": "https://datacap.devlive.org/en/release/latest.html",
    "lang": "en"
  },
  {
    "title": "2024.04.0",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:----------:|:------------:| | 2024.4.0 | 2024-12-02 | !!! note 本版本是一个全新的版本，完全使用了新的插件管理系统，新的 API 等各种新特性。本次更新为全新的版本，不兼容之前的版本。升级前要做好数据备份，以免数据丢失。数据库是兼容的，只要执行升级的 SQL 就可以了。<br /> 还需要注意的是升级版本后，要修改 datacapuser 和 datacaprole 表的 code 字段的值每条数据唯一即可，否则会导致无法登录。(如果是全新安装可忽略这个步骤)<br /> 执行以下 SQL 语句升级数据库:<br /> sql INSERT INTO datacapmenu VALUES (18,'全局 - 商店','STORE','','/store','',3,'VIEW',0,1,'common.store','Store',NULL,'2024-11-05 21:18:28',0,0,NULL); INSERT INTO datacaprolemenurelation VALUES ('1','18') --- 修复未登录导致国际化获取失败的问题 拆分插件系统为全新的模块，支持更好的插件管理 支持插件的在线安装，卸载等操作 增加通用测试模块 增加全新的插件商店 修复数据集保存时出现异常的问题 使用全新的 JsonView 注解，支持更好的数据安全及显示 --- 增加 Open API 文档",
    "url": "https://datacap.devlive.org/en/release/2024.4.0.html",
    "lang": "en"
  },
  {
    "title": "2024.03.11",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.3.11 | 2024-11-17 | --- 修复部分 API 返回多余字段 添加清除 UI 依赖脚本 替换 yarn 为 pnpm 修复了仅分配 Submenu 时缺少 Parent Menu 的问题 --- 替换整体 UI 为 view-shadcn-ui",
    "url": "https://datacap.devlive.org/en/release/2024.3.11.html",
    "lang": "en"
  },
  {
    "title": "2024.03.10",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.3.10 | 2024-10-09 | --- 移除示例地址 修复登录页面上的验证码问题 修复注册页面上的验证码问题 修复验证码验证规则 修复文档指定 Java 版本 --- 支持 MinIO --- Bump dep.antlr4.version from 4.13.1 to 4.13.2 Bump ag-grid-community and ag-grid-vue3 Bump commons-io:commons-io from 2.11.0 to 2.14.0 Bump com.clickhouse:clickhouse-jdbc from 0.4.6 to 0.6.5 Bump vite from 5.2.8 to 5.4.8 Bump rollup from 4.14.2 to 4.22.4 Bump micromatch from 4.0.5 to 4.0.8 Bump axios from 1.6.8 to 1.7.4 Bump org.yaml:snakeyaml from 2.2 to 2.3 Bump com.github.vertical-blank:sql-formatter from 2.0.3 to 2.0.5 Bump org.apache.maven.plugins:maven-javadoc-plugin from 3.8.0 to 3.10.0 Bump com.amazonaws:aws-java-sdk-s3 from 1.12.770 to 1.12.771 Bump org.testcontainers:oracle-xe from 1.18.1 to 1.20.1 Bump com.github.eirslett:frontend-maven-plugin from 1.13.4 to 1.15.0 Bump kotlin.version from 1.9.10 to 2.0.20 Bump org.duckdb:duckdbjdbc from 0.9.2 to 1.0.0 Bump com.taosdata.jdbc:taos-jdbcdriver from 3.2.7 to 3.3.0 Bump com.microsoft.sqlserver:mssql-jdbc from 12.6.0.jre8 to 12.8.1.jre8 Bump avatica.version from 1.22.0 to 1.25.0 Bump org.apache.iotdb:iotdb-jdbc from 1.1.0 to 1.3.2",
    "url": "https://datacap.devlive.org/en/release/2024.3.10.html",
    "lang": "en"
  },
  {
    "title": "2024.03.9",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.9 | 2024-08-27 | --- 修复 maven 发布 CI 修复包缺少文件导致启动失败的问题 修复构建数据集不携带查询 SQL 的问题 修复数据集未携带 injector 导致构建失败的问题 --- 支持腾讯 COS 支持 Amazon S3 --- 升级 UI braces 3.0.2 到 3.0.3 升级 org.apache.kyuubi:kyuubi-hive-jdbc-shaded 1.7.1 到 1.9.2 升级 org.apache.maven.plugins:maven-compiler-plugin 3.12.1 到 3.13.0 升级 testcontainers.version 1.19.5 到 1.20.1 升级 org.apache.maven.plugins:maven-javadoc-plugin 3.6.3 到 3.8.0 升级 jackson.version 2.17.0 到 2.17.2 升级 org.elasticsearch.plugin:x-pack-sql-jdbc 8.12.0 到 8.15.0 升级 org.apache.commons:commons-csv 1.10.0 到 1.11.0 升级 monetdb:monetdb-jdbc 3.2 到 11.19.15",
    "url": "https://datacap.devlive.org/en/release/2024.03.9.html",
    "lang": "en"
  },
  {
    "title": "2024.03.8",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.8 | 2024-07-29 | --- 优化核心打包方式 --- 优化打包方式为按需加载 --- 优化打包方式为按需加载 --- 优化打包方式为按需加载 --- 升级 net.snowflake:snowflake-jdbc 3.14.4 到 3.17.0 升级 com.dameng:DmJdbcDriver18 8.1.3.62 到 8.1.3.140 升级 hadoop.version 3.3.4 到 3.4.0 升级 org.apache.pinot:pinot-jdbc-client 0.8.0 到 1.1.0 升级 org.apache.phoenix:phoenix-core 4.16.0 到 5.2.0 升级 jackson.version 2.16.1 到 2.17.0",
    "url": "https://datacap.devlive.org/en/release/2024.03.8.html",
    "lang": "en"
  },
  {
    "title": "2024.03.7",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.7 | 2024-07-02 | --- 重新布局 README.md 连接器列表 支持 Banner 显示应用信息 --- 支持 Apache Solr 支持 InfluxDB 重命名 ScyllaDB 包名 修复 SPI 异常连接未被拦截的问题 --- 重命名 file 为 convert 支持 xml 转换器 --- 升级 io.trino:trino-jdbc 414 至 450",
    "url": "https://datacap.devlive.org/en/release/2024.03.7.html",
    "lang": "en"
  },
  {
    "title": "2024.03.6",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.6 | 2024-06-19 | --- 替换 findbugs 为 spotbugs 修复 JwtResponse 返回问题 适配数据源元数据 适配元数据转换器 --- 添加通知器文档 添加转换器文档 --- 支持 Json 支持 Txt 支持 Csv 支持 None",
    "url": "https://datacap.devlive.org/en/release/2024.03.6.html",
    "lang": "en"
  },
  {
    "title": "2024.03.5",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.5 | 2024-06-05 | --- [Docker] 修复 JDK 版本 [User] 修复用户头像重复上传问题 [Role] 修复设置多个权限会导致菜单显示重复的问题 [Query] 修复并发访问导致无法记录查询历史记录的问题 [Dashboard] 支持上传缩略图 [Dashboard] 添加本地存储路径 [User] 修复用户登录注册加载状态 [User] 支持后台添加用户 [User] 优化用户操作函数 --- 替换 npm 为 yarn --- 添加 SPI 支持钉钉功能 移除 antlr4 代码",
    "url": "https://datacap.devlive.org/en/release/2024.03.5.html",
    "lang": "en"
  },
  {
    "title": "2024.03.4",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.4 | 2024-05-19 | --- 移除 stale ci 修复聊天对话框文本过多导致样式混乱的问题 添加数据库、表、列补全符 修复了单词未设置为编辑器的问题 更新 openai-java-sdk 版本 支持显示查询错误信息 支持即席查询可视化 --- 修复查看SQL时使用ESC导致页面异常的问题 --- 修复虚拟列无法构建 修复临时查询不传递列类型的问题 支持根据表达式生成别名 更换头像 添加玫瑰图 --- 支持多数据源 --- 支持 aliyun oss 将 fs 存储实验功能标记为正式功能",
    "url": "https://datacap.devlive.org/en/release/2024.03.4.html",
    "lang": "en"
  },
  {
    "title": "2024.03.3",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.3 | 2024-04-19 | --- [用户] 支持上传头像 [数据源] 删除 v1 saveAndUpdate、getInfo fc [数据源] 替换 getSource 函数 [数据源] 添加代码和名称栏 [数据源] 重构元数据布局 [数据源] 将数据库信息添加到元数据中 [函数] 修复info中未定义属性转换时的错误 [查询] 删除旧的 api [查询] 修复历史数据为空的问题 [片段] 适配器基础模块 [流水线] 适配器基础模块 [仪表板] 优化仪表板获取方式 [仪表板] 添加描述 [仪表板] 添加默认图像 删除配置目录下的 schema 启用统一的 BaseEntity、BaseRepository、BaseController、BaseService 支持 saveOrUpdate 方法根据属性设置相关值 添加代码权限信息 --- [布局] 优化菜单选择和高亮显示 [布局] 支持父菜单选择高亮显示 [布局] 添加404页面回调 [布局] 添加403页面重定向 [布局] 添加重定向到网络页面 [用户] 修复加载状态 添加页面加载进度 删除 echarts 合并 i18n --- 修复数据集构建中缺失列的问题 固定信息路径 优化查询列传输数据过大的问题 添加获取列的权限 突出显示指标维度容器 将 id 替换为代码 [折线图] 添加标题 删除未使用的组件 将配置合并到编辑器中 修复配置不回显的问题 删除未使用的配置组件 禁用未查询可视化功能 添加散点图 添加雷达图 添加漏斗图 添加仪表图表 --- 替换 schema 路径 修复 docker compose 镜像并更改示例地址",
    "url": "https://datacap.devlive.org/en/release/2024.03.3.html",
    "lang": "en"
  },
  {
    "title": "2024.03.2",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.2 | 2024-04-12 | > 紧急发布版本：由于发布前测试时，调用 API 失误，导致构建数据集功能无法使用，该版本主要用于修改构建数据集功能 --- 修复 logo 路径引用错误问题 --- 修复数据集无法构建问题 --- 修复初始化脚本丢失字段问题",
    "url": "https://datacap.devlive.org/en/release/2024.03.2.html",
    "lang": "en"
  },
  {
    "title": "2024.03.1",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.1 | 2024-04-11 | > 本次版本更新，我们主要针对 UI 进行了重构 --- 修复依赖包导入错误 [报表] 修复发布报表后跳转到报表列表的问题 [仪表盘] 支持显示报表名称 [仪表盘] 修复报表右下边框缺失的问题 [仪表盘] 修复修改跳转时报错的问题 --- 重构 UI 为 shadcn ui --- 移除旧版构建功能 支持指定启动脚本 --- 修复由于未配置生命周期列导致建表失败的问题 构建失败禁止即席查询 将即席查询分组合并到维度中 修复计算总行数和总大小 --- 支持删除功能 --- [Seatunnel] 支持 flink 引擎 [Seatunnel] 支持 seatunnel 引擎 --- 更新 testcontainers.version 1.17.6 到 1.19.5 更新 com.datastax.oss:java-driver-core 4.16.0 到 4.17.0",
    "url": "https://datacap.devlive.org/en/release/2024.03.1.html",
    "lang": "en"
  },
  {
    "title": "2024.02.1",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.02.1 | 2024-03-01 | --- 支持 ParadeDB (https://github.com/devlive-community/datacap/issues/589) 支持 TimescaleDB (https://github.com/devlive-community/datacap/issues/593) --- [图表] 添加面积图 (https://github.com/devlive-community/datacap/issues/611) [图表] 添加多面积图 (https://github.com/devlive-community/datacap/issues/610) [图表] 添加饼图 (https://github.com/devlive-community/datacap/issues/613) [图表] 支持折线图断点处理 [图表] 支持分组直方图 (https://github.com/devlive-community/datacap/issues/612) [图表] 支持词云图 (https://github.com/devlive-community/datacap/issues/614) 支持添加主键 (https://github.com/devlive-community/datacap/issues/624) 添加总行数和总大小 (https://github.com/devlive-community/datacap/pull/676) 支持自定义别名的即席查询 (https://github.com/devlive-community/datacap/issues/615) 支持维度排序 支持指标排序 支持数据采样列 (https://github.com/devlive-community/datacap/issues/605) 添加有符号号码类型 修复了小屏幕上列表中的显示问题 支持即席查询自定义函数 (https://github.com/devlive-community/datacap/issues/603) 添加筛选器分组 合并查询列 修复复制数据导致重复请求的问题 修复编辑报表分组异常问题 修复无限循环查询问题 支持过滤器 IS NULL， IS NOT NULL 修复了多列查询导致 expres 混淆的问题 支持过滤器 LIKE， NOT LIKE 支持过滤器 =， <> 支持过滤器 >， >= 支持过滤器 <， <= 支持历史错误信息同步 支持清空数据 (https://github.com/devlive-community/datacap/issues/622) 支持动态添加列 (https://github.com/devlive-community/datacap/issues/623) 重构数据集详细信息页面 添加日期时间类型 支持自定义生命周期 (https://github.com/devlive-community/datacap/issues/616) 禁用 BOOLEAN、DATETIME 类型分配 支持虚拟列 (https://github.com/devlive-community/datacap/issues/602) 支持docker compose (https://github.com/devlive-community/datacap/issues/648) 支持七牛云存储 (https://github.com/devlive-community/datacap/issues/618) --- 更新 io.crate:crate-jdbc 2.6.0 到 2.7.0 更新 Impala:ImpalaJDBC42 2.6.29.1035 到 2.6.32.1041 更新 org.yaml:snakeyaml 2.0 到 2.2 更新 org.apache.hive:hive-jdbc 3.1.2 到 3.1.3 更新 org.elasticsearch.plugin:x-pack-sql-jdbc 8.11.3 到 8.12.0 更新 jackson.version 2.16.0 到 2.16.1 更新 org.apache.maven.plugins:maven-javadoc-plugin 3.6.0 到 3.6.3 更新 com.microsoft.sqlserver:mssql-jdbc 11.2.1.jre8 到 12.6.0.jre8 更新 postgresql.version 42.6.0 到 42.7.2",
    "url": "https://datacap.devlive.org/en/release/2024.02.1.html",
    "lang": "en"
  },
  {
    "title": "2024.01.1",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 2024.01.1 | 2024-01-30 | --- [仪表盘] 支持移除报表 [仪表盘] 支持预览 [仪表盘] 支持修改 [元数据] 支持 PostgreSQL 数据库 [元数据] 支持同步错误消息 [元数据] 支持构建后元数据同步 (https://github.com/devlive-community/datacap/issues/585) [元数据] 支持数据源手动同步元数据 (https://github.com/devlive-community/datacap/issues/586) [元数据] 支持数据源同步历史记录 添加查询模式 修复了函数创建/更新时间问题 优化问题模板 支持 GitHub CI 本地化 添加系统架构图 (https://github.com/devlive-community/datacap/issues/491) 支持缓冲查询结果 (https://github.com/devlive-community/datacap/issues/490) --- 修复数据源并发异常问题 (https://github.com/devlive-community/datacap/issues/513) --- 添加 SQL 解析器 SPI 添加 Trino SQL 解析器 (https://github.com/devlive-community/datacap/issues/569) 添加 MySQL SQL 解析器 (https://github.com/devlive-community/datacap/issues/568) --- 添加调度 SPI 支持默认调度器 --- 修复流水线分组问题 支持 SqlServer 重构 Seatunnel 执行器 重构 SPI 使用 Kotlin 重写 重构流水线配置方法 --- 支持 MatrixOne (https://github.com/devlive-community/datacap/issues/574) 支持 ScyllaDB (https://github.com/devlive-community/datacap/issues/543) --- 添加列模型（指标｜维度） 添加即席查询可视化编辑器 添加折线图 添加数据表格 添加柱状图 支持展示查询 SQL 支持指标自定义表达式 支持维度分组 添加指标表达式提示 按类型指定表达式 支持指标别名 支持发布|编辑数据集报表 支持分区键 支持列别名 支持同步历史记录 支持即席查询 (https://github.com/devlive-community/datacap/issues/581) 支持定时数据同步 (https://github.com/devlive-community/datacap/issues/590) --- 新增文件系统集成开发文档 添加合作伙伴演示文档 (https://github.com/devlive-community/datacap/issues/582) --- 更新 org.apache.maven.plugins:maven-compiler-plugin 3.3 到 3.12.1 更新 com.dameng:DmJdbcDriver18 8.1.2.192 到 8.1.3.62 更新 mysql 到 8.0.30 更新 snowflake 到 3.14.4 更新 com.taosdata.jdbc:taos-jdbcdriver 3.0.0 到 3.2.7 更新 org.apache.kylin:kylin-jdbc 2.6.3 到 4.0.3 更新 slf4j.version 1.7.36 到 2.0.10 更新 org.apache.maven.plugins:maven-assembly-plugin 3.5.0 到 3.6.0 更新 org.duckdb:duckdbjdbc 0.8.1 到 0.9.2 更新 org.apache.ignite:ignite-core 2.8.1 到 2.16.0 更新 org.projectlombok:lombok 1.18.28 到 1.18.30",
    "url": "https://datacap.devlive.org/en/release/2024.01.1.html",
    "lang": "en"
  },
  {
    "title": "1.18.0",
    "content": "!!! note 当前版本涉及几项重大更新。 !!! DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.18.0 | 2023-12-22 | --- [元数据] 支持克隆行 [元数据] 支持截断表 [元数据] 支持刷新数据 [元数据] 支持删除表 [元数据] 支持展示表结构 [元数据] 支持展示表 ER 图 [元数据] 支持导出表数据 [元数据] 支持新建表 [元数据] 支持新建列 [元数据] 支持删除列 [查询] 添加自动添加 LIMIT [用户] 支持用户头像 [数据源] 优化数据源删除提示信息 [报表] 支持数据报表功能 [仪表盘] 支持仪表盘功能 [数据集] 支持数据集数据 --- 支持 FileSystem 支持 Local FileSystem --- 添加 SDK 使用文档 --- 支持多版本 修复连接具柄导致连接失败 --- 支持 MySQL JDBC 方式 支持拖拽构建 --- 更新 org.mongodb:mongodb-jdbc 2.0.2 到 2.0.3 更新 ch.qos.logback:logback-classic 1.2.11 到 1.2.13 更新 org.apache.kafka:kafka-clients 2.8.1 到 3.6.1 更新 ch.qos.logback:logback-core 1.2.11 到 1.2.13 更新 com.oracle.database.jdbc:ojdbc8 21.9.0.0 到 23.3.0.23.09 更新 org.elasticsearch.plugin:x-pack-sql-jdbc 7.10.0 到 8.11.3 更新 jackson.version 2.14.2 到 2.16.0",
    "url": "https://datacap.devlive.org/en/release/1.18.0.html",
    "lang": "en"
  },
  {
    "title": "1.17.0",
    "content": "!!! note 当前版本涉及几项重大更新。 !!! DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.17.0 | 2023-11-20 | --- 删除无效的元数据管理器 优化问题模板 将版本添加到应用程序 !img.png 支持在启动时检查java版本是否兼容 修复了在不选择表的情况下直接选择列的错误 支持自定义列显示 !Column Display 支持重置列位置 !Reset Column Position 查询表视图会导致无法转换的值 添加 jvm 配置 当数据为空时，不返回任何列信息 修复切换表查询数据导致查询列为空的问题 支持复位自动递增 !Reset Auto Increment 统一每个插件返回的数据类型 支持查看构建语句语句语句 !Build Statement 为表添加筛选器 !Filter 支持添加新的行数据 !Add Row --- 删除未使用的组件 --- 更新 com.h2database:h2 2.1.214 到 2.2.224 更新 org.jetbrains.dokka:dokka-maven-plugin 1.8.10 到 1.9.10",
    "url": "https://datacap.devlive.org/en/release/1.17.0.html",
    "lang": "en"
  },
  {
    "title": "1.16.0",
    "content": "!!! note 当前版本涉及几项重大更新。 !!! DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.16.0 | 2023-11-01 | --- 支持列顺序 !Column Order 支持删除行 支持删除多行 !Delete Rows 支持无主键数据更新 !Update Columns 支持根据主键更新 !Update Without Primary Key 支持预览待处理的更改 --- 支持选择查询 !Selection Query 支持自定义配置 !Custom Configure --- 添加用户配置文件文档 --- 更新 org.apache.maven.plugins:maven-javadoc-plugin 3.5.0 到 3.6.0 更新 com.oceanbase:oceanbase-client 2.4.2 到 2.4.5 更新 org.apache.maven.plugins:maven-javadoc-plugin 3.5.0 到 3.6.0",
    "url": "https://datacap.devlive.org/en/release/1.16.0.html",
    "lang": "en"
  },
  {
    "title": "1.15.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.15.0 | 2023-10-18 | --- 支持数据库同步 支持表同步 支持列同步 添加调度程序历史记录 重构元数据模块 改进 SQL 文件 --- 替换 monaco 为 ace --- 添加中文部署文档 优化片段和数据来源中文文档",
    "url": "https://datacap.devlive.org/en/release/1.15.0.html",
    "lang": "en"
  },
  {
    "title": "1.14.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.14.0 | 2023-09-14 | --- 修复数据源检查任务返回空的问题 添加验证码 支持登录验证码 支持验证码失败自动刷新 支持注册验证码 支持注册启用 移动 etc 到 configure --- 修复空编辑器破坏异常 公共页面增加布局 修复了个人资料页面错误 修复登录页面样式异常的问题 --- 支持 Kafka 输入｜输出 支持 ClickHouse 输入｜输出 支持删除 构建管道页面 支持提交 支持 SWITCH 字段类型 添加执行者标志 支持限流排队 支持停止 服务重启时重置流水线 添加日志界面并优化UI 支持字段描述 支持字段 SELECT 类型 支持字段检查 支持字段数组 支持 Redis 输出 支持指定运行时机制 --- 将 com.google.guava:guava 从 31.1-jre 更改为 32.1.2-jre 将 org.devlive.sdk:openai-java-sdk 从 1.5.0 升级到 1.9.0 将 com.h2database:h2 从 2.1.214 提升到 2.2.220 将 org.projectlombok:lombok 从 1.18.24 更改为 1.18.28 将 org.apache.kafka:kafka-clients 从 2.8.0 升级到 2.8.1 将 org.duckdb:duckdbjdbc 从 0.7.0 升级到 0.8.1 将 com.github.eirslett:frontend-maven-plugin 从 1.12.1 升级到 1.13.4 将 kotlin.version 从 1.8.20 升级到 1.9.10 将 org.sonatype.plugins:nexus-staging-maven-plugin 从 1.6 升级到 1.6.13",
    "url": "https://datacap.devlive.org/en/release/1.14.0.html",
    "lang": "en"
  },
  {
    "title": "1.13.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.13.0 | 2023-08-09 | --- 修复了 openai-java-sdk 版本 添加数据源版本 添加数据源扫描任务 优化状态图标 删除对内置 H2 数据库的支持 --- 支持文字 修复 div 空白 添加了日历热图中文 修复了数据贡献图错误 当数据源不可用时列表禁用选择 --- 修复无法执行set语法sql的问题 修复连接未关闭的问题 --- 支持获取DBName和TableName --- 修复了 default 数据库中没有查询错误的问题",
    "url": "https://datacap.devlive.org/en/release/1.13.0.html",
    "lang": "en"
  },
  {
    "title": "1.12.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.12.0 | 2023-07-11 | --- 删除日志默认调试级别 拆分各模块 修复错误依赖 支持菜单重定向 修复了 SQL 架构 支持是否功能菜单 将 openai sdk 替换为 openai-java-sdk 重构 chatgpt --- 修复缓存不清除，需要重新点击的问题 修复了无效令牌的错误 添加缓冲区标签名称 修复使用h2数据库的个人资料页面异常的问题 修复服务重启导致404",
    "url": "https://datacap.devlive.org/en/release/1.12.0.html",
    "lang": "en"
  },
  {
    "title": "1.11.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.11.0 | 2023-06-13 | --- 添加了查询历史返回的行数 修复了主页上的 404 错误 将插件剥离到单独的文件夹中 --- 重构文件夹 添加菜单面包屑 同步服务器路由 添加非登录页面 添加源管理器路由 修复路由构建失败回调异常 修复数据源类型标记异常 --- 支持 apache cassandra",
    "url": "https://datacap.devlive.org/en/release/1.11.0.html",
    "lang": "en"
  },
  {
    "title": "1.10.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.10.0 | 2023-05-30 | --- 修复服务启动默认连接 mongo 修复了 sql 模板的 h2 db updatetime 和 createtime 改进 H2 元数据管理获取类型 改进 mysql 元数据管理获取类型 固定元数据管理数据页默认为 1 重构数据渲染表 支持栏目类型 添加耗时和查看执行 SQL 支持可选择的每页总计 支持标题提示数据类型 支持复制选定的数据结果 支持选择指定列查询 支持过滤器 修复默认用户创建时间为空 支持权限 固定用户 createTime 为空 --- 修复不清除网络授权信息 优化数据管理获取数据 禁用警告输出到控制台 增加编辑器缓冲提示限制 删除默认排序规则 重命名用户仪表板路径 添加仪表板聊天风格 修复导航样式 添加数据源加载状态 --- 支持 apache pinot 支持 mongo 社区版 --- 升级 clickhouse-jdbc 0.3.2-patch9 到 0.4.6 升级 oracle-xe 1.17.6 到 1.18.1 升级 kyuubi-hive-jdbc-shaded 1.6.0-incubating 到 1.7.1",
    "url": "https://datacap.devlive.org/en/release/1.10.0.html",
    "lang": "en"
  },
  {
    "title": "1.9.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:-------:|:------------:| | 1.9.0 | 2023-05-04 | --- 支持 github packages 优化 docker 镜像发布流程 支持格式化日期 添加数据库连接指定时区 修复了默认的 h2 数据库未初始化的定时任务 将 admin 用户添加到 README.md 添加 docker 镜像标签 在 README.md 中添加微信二维码 添加 docker 徽章 修复数据源创建时间为空 --- 添加中文文档 添加 Rainbond 部署文档 添加插件文档 支持顶部滚动通知 --- 修复数据表无效分页 修复了无法正确渲染的问题 修复包含国际化数据的渲染缺失的翻译结果 支持复制多选行 修复数据源测试状态问题 支持关闭消息 添加定时任务链接 --- 支持 ceresdb 支持 greptimedb 支持 questdb 支持 apache doris 支持 starrocks 支持 hologres 支持 apache hadoop hdfs --- 移除 http 重试逻辑 --- 修复了 ydb 依赖冲突 --- 添加配置文件 --- 升级 trino-jdbc 397 到 414 (#331) 升级 iotdb-jdbc 0.13.0 到 1.1.0 (#309)",
    "url": "https://datacap.devlive.org/en/release/1.9.0.html",
    "lang": "en"
  },
  {
    "title": "1.8.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.8.0 | 2023-04-10 | --- Rename executor directory Optimize document release timing Fixed format connect url close #304 Support proxy for chatgpt close #299 ChatGPT is currently unable to associate context close #298 Support returning parsing error results Add schedule lib Support the code editor supports automatic prompts for data source library tables and columns close #301 Fix initialization sql script Support h2 database Remove some invalid jars Add docker publish ci --- Refactor install docs --- The code editor supports code fragments close #300 --- Support h2 for native (memory) --- Perfect test case Support SHOW DATABASES and SHOW TABLES ... --- Fixed validation sql content --- Bump jackson.version from 2.13.4 to 2.14.2 Bump postgresql from 42.5.0 to 42.6.0 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:-----------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.8.0.html",
    "lang": "en"
  },
  {
    "title": "1.7.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.7.0 | 2023-03-20 | --- Add other issues template Add role Upgrade JDK 8 to 11 Support chatgpt Add submit pipeline api --- Add seatunnel executor --- Support execute sql on source Fixed code bugs --- Add icon to connectors --- Add executor spi --- Fixed duplicate tree menu data Optimized type display icon Optimize data source testing｜save interaction Support query history display plug-in type Add system announcement display Fixed the 'keyword' is repeated with tab page addition bug #208 Replace markdown preview component --- Support kafka --- Upgrade redis version from 3.6.3 to 4.3.1 Bump maven-assembly-plugin from 3.1.1 to 3.5.0 #272 Bump antlr4.version from 4.9.3 to 4.12.0 #262 Bump jedis from 3.6.3 to 4.3.1 #254 Bump DmJdbcDriver18 from 8.1.2.141 to 8.1.2.192 #234 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:-----------:| | @why198852 | | @Stacey1018 | | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.7.0.html",
    "lang": "en"
  },
  {
    "title": "1.6.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.6.0 | 2023-03-02 | --- Add logo Support SHOW PATHS xxx Fixed function time field Refactor all module Add http lib Add logger lib --- JDBC: Repair Connection failure Do not close the connection --- Add default watermark Remove about page Add routing permission control Optimize lazy loading of the tree menu of the query page --- Support duckdb for jdbc close #249 Support alioss for native #250 --- Support SHOW PATHS --- Bump maven-javadoc-plugin from 3.4.1 to 3.5.1 Bump oceanbas-client from 2.4.0 to 2.4.2 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:----------:| | @why198852 | | @mlboy | | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.6.0.html",
    "lang": "en"
  },
  {
    "title": "1.5.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.5.0 | 2023-02-16 | --- Support dsl query Remove incubator Add sql parser Refactor the module directories Set port default value is 0 --- Fixed jdbc no password exception is configured --- Support multi column sort --- Support zookeeper for native --- Add powered by page --- Fixed mget,hget value is displayed as null #219 --- Bump maven-javadoc-plugin from 2.10.4 to 3.4.1 Bump ojdbc8 from 21.1.0.0 to 21.9.0.0 Bump mongodb-jdbc from 2.0.0 to 2.0.2 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:--------------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.5.0.html",
    "lang": "en"
  },
  {
    "title": "1.4.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.4.0 | 2023-01-31 | --- Fixed restart script Supports monitor process Do not modify the default system SQL template Fixed plugin template by name Support user login log Refactoring plug-in configuration extraction mode --- Support data source manager Add client cli --- Plug-in ICONS are displayed based on the plug-in type Optimize editor auto prompt Support watermark Templates are not supported for adding data sources Fixed footer link --- Support snowflake for jdbc Support ydb for jdbc --- Refactor some docs --- Fixed command multiple parameters --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:--------------:| | @qianmoQ | | @hometownglory | !!!",
    "url": "https://datacap.devlive.org/en/release/1.4.0.html",
    "lang": "en"
  },
  {
    "title": "1.3.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.3.0 | 2022-12-16 | --- Support change username Support custom sql template Support plugin function Add restart script --- Optimize the presentation of the data source list Add data source description and prompt Support query history id order Support quote query history --- Support oceanbase for jdbc Support redis for native Support neo4j for jdbc Support iotdb for jdbc Support auth for native --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.3.0.html",
    "lang": "en"
  },
  {
    "title": "1.2.0",
    "content": "!!! note The current release involves several major updates. The following link is Roadmap !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.2.0 | 2022-11-30 | --- Support http protocol --- Support for data result column header hiding (#139) Support data result filtering (#132 #140) Replace @antv/g2 to echarts Replace ant-design-vue to iview Replace @antv/s2 to ag-grid Optimize about page Optimize not found page Add not authorized page Add version badge Add not network page Support result visual line chart --- Support cratedb Support cratedb for http Support dameng Support clickhouse for http Support tdengine for jdbc Support impala for jdbc --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.2.0.html",
    "lang": "en"
  },
  {
    "title": "1.1.0.20221115",
    "content": "!!! note The current release involves several major updates. The following link is Roadmap !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:----------------:|:------------:| | 1.1.0.20221115 | 2022-11-15 | --- Replace plugin name to id Support for internationalization issues-82 Reduce size of docker image Switch bash docker image to eclipse-temurin:8-jdk-focal Support ssl issues-75 Extract the plug-in to get the global tool Support database write operation issues-70 Supports user rights management Support code snippet issues-74 Support editor auto completion Support to provide data source schema tree bar issues-106 Support multiple editor issues-110 Add profile for user Support change user password Add data source radar map within 7 days Add about page Add feedback issues-126 --- Add custom validator --- Support MongoDB Support Dremio Support HBase jdbc for Phoenix issues-103 Support H2 Support SqlServer Support Oracle --- Fix cannot init RedisConnection issues-71 --- Update version to 7.10.0 --- Bump Kyuubi 1.6.0-incubating --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| |@pan3793| |@javalover123| |@shuangzishuai| |@GtoCm| |@why198852| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.1.0.20221115.html",
    "lang": "en"
  },
  {
    "title": "1.0.0.20221015",
    "content": "!!! note This is the first new version we've released. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:----------------:|:------------:| | 1.0.0.20221015 | 2022-10-15 | --- Building SPI supports multiple data sources Supports web visualization based on Vue architecture Support Data source usage history Data statistics for data sources and history --- [x] Support ClickHouse [x] Support MySQL [x] Support Presto [x] Support Redis [x] Support PostgreSQL [x] Support Trino [x] Support ElasticSearch [x] Support Apache Druid [x] Support Apache Kyuubi [x] Support Apache Hive [x] Support Apache Kylin [x] Support Apache Ignite [x] Support IBM DB2 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| | @mlboy | | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/en/release/1.0.0.20221015.html",
    "lang": "en"
  },
  {
    "title": "开发规范",
    "content": "本文档主要用来介绍 DataCap 服务端和 UI 端的开发规范。 !!! danger 请仔细阅读该开发规范，并且遵守该规范，否则可能导致服务端和 UI 端的代码提交无法审核通过。 !!! 我们使用严格的提交信息格式： <类型>(<范围>): <描述> [可选 正文] [可选 脚注] 类型必须是以下之一： feat: 新功能 fix: Bug 修复 docs: 仅文档更改 style: 不影响代码含义的更改（空格、格式化等） refactor: 既不修复错误也不添加功能的代码更改 perf: 提高性能的代码更改 test: 添加或修正测试 chore: 构建过程或辅助工具的变动 revert: 撤销之前的提交 范围应该是受影响的模块名称。 示例： feat(auth): 实现基于 JWT 的身份验证 添加 JWT 中间件 实现令牌生成和验证 添加用户认证路由 集成 Redis 存储刷新令牌 close (#123) !!! warning 所有的 组件 | 字段名 必须都以驼峰命名法命名 !!! vue title=\"正确示例\" export default defineComponent({ name: 'RoleHome' }) const showName = ref(false) vue title=\"错误示例\" export default defineComponent({ name: 'rolehome' }) const showname = ref(false) !!! warning 组件命名必须以 Shadcn 开头，如 ShadcnButton、ShadcnInput、ShadcnSpace !!! vue title=\"正确示例\" <ShadcnButton size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <ShadcnIcon icon=\"Pencil\" size=\"15\"/> </ShadcnButton> vue title=\"错误示例\" <shadcn-button size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <shadcn-icon icon=\"pencil\" size=\"15\"/> </shadcn-button> !!! warning 属性命名首字母必须小写，如 size，多个单词必须已驼峰命名法命名，如 showName !!! vue title=\"正确示例\" const size = ref('small') const showName = ref(false) vue title=\"错误示例\" const Size = ref('small') const showname = ref(false) const show-name = ref(false) !!! warning 函数名必须符合以下规范 !!! 处理函数必须以 handle 开头，如 handleChangeInfo 回调函数必须以 on 开头，如 onChange 涉及到弹出层的函数必须以 visible 开头，如 visibleInfo vue title=\"正确示例\" const handleChangeInfo = () => {} const onChange = () => {} const visibleInfo = () => {} vue title=\"错误示例\" const changeinfo = () => {} const change = () => {} const aaaInfo = () => {} !!! note 多个组件内必须以新行分割 !!! vue title=\"正确示例\" <ShadcnSpace> <ShadcnTooltip :content=\"$t('common.editData')\"> <ShadcnButton size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <ShadcnIcon icon=\"Pencil\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> <ShadcnTooltip :content=\"$t('role.common.assignMenu').replace('$NAME', row?.name)\"> <ShadcnButton size=\"small\" circle @click=\"handlerAssignMenu(true, row)\"> <ShadcnIcon icon=\"Menu\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> </ShadcnSpace> vue title=\"错误示例\" <ShadcnSpace> <ShadcnTooltip :content=\"$t('common.editData')\"> <ShadcnButton size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <ShadcnIcon icon=\"Pencil\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> <ShadcnTooltip :content=\"$t('role.common.assignMenu').replace('$NAME', row?.name)\"> <ShadcnButton size=\"small\" circle @click=\"handlerAssignMenu(true, row)\"> <ShadcnIcon icon=\"Menu\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> </ShadcnSpace> !!! warning 请保证属性的顺序，以便于后续的维护 !!! 多属性的正常顺序是 系统属性 (必须都在第一位，如 v-model) 固定属性 (如 show-total) 动态变量 (如 :page-size=\"pageSize\") 动态事件 (如 @on-change=\"onPageChange\") vue title=\"正确示例\" <ShadcnPagination v-model=\"pageIndex\" show-total :page-size=\"pageSize\" @on-change=\"onPageChange\"/> vue title=\"错误示例\" <ShadcnPagination v-model=\"pageIndex\" @on-change=\"onPageChange\" show-total :page-size=\"pageSize\"/> !!! warning 多个属性且超过 3 个必须以新行分割 !!! vue title=\"正确示例\" <ShadcnPagination v-model=\"pageIndex\" show-total @on-change=\"onPageChange\"/> <ShadcnPagination v-model=\"pageIndex\" show-total :page-size=\"pageSize\" @on-change=\"onPageChange\"/> vue title=\"错误示例\" <ShadcnPagination v-model=\"pageIndex\" show-total @on-change=\"onPageChange\"/> <ShadcnPagination v-model=\"pageIndex\" show-total :page-size=\"pageSize\" @on-change=\"onPageChange\"/>",
    "url": "https://datacap.devlive.org/en/developer/specification/home.html",
    "lang": "en"
  },
  {
    "title": "服务端",
    "content": "本文章主要用来介绍我们如何来贡献 datacap 服务端源码。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------|:--------------|:---| | JDK | 1.8 \\| 11 | 必须 | | Maven | >= 3.5 | 可选 | | IDEA \\| Eclipse \\| 其他 | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 进入项目根目录执行以下代码 bash ./mvnw -T 1C clean install package -Dspotbugs.skip -Dgpg.skip -Dcheckstyle.skip -DskipTests=true --- 打开 IDEA 编辑器，弹出类似如下窗口 !img.png !!! note 不同的 IDEA 版本，各个按钮的所在位置可能不相同。 !!! 点击红框指出的菜单，选择 datacap 源码的根目录，点击打开即可，此时会进入源码的页面，加载完成后窗口如下 !img1.png 左侧展示的是 datacap 源码的各个目录。以下是每个目录的功能： .github: github 需要的一些 issues 模版，pull request 模版，还有一些 CI 自动化配置脚本等 .mvn: maven wapper 的配置 client: 客户端的相关源码 datacap-cli: datacap 命令行源码 configure: datacap 相关配置 assembly datacap 打包用到的各种配置 etc datacap 服务端配置文件 bin datacap 服务启动脚本 conf datacap 服务配置文件 plugins datacap 插件配置文件 git-forks datacap fork 主仓库脚本 git-hook datacap git 本地提交相关 hook proxy datacap 服务部署代理配置 publish datacap 用于发布的各脚本 core datacap 服务端核心源码 datacap-captcha datacap 验证码源码 datacap-common datacap 常用工具源码 datacap-parser datacap SQL 解析器源码 datacap-security datacap 安全相关源码 datacap-server datacap 服务端源码 datacap-service datacap 服务端服务源码 datacap-spi datacap 服务端 SPI 源码 datacap-web datacap Web 前端源码 docs: datacap 文档源码 driver: datacap 数据源驱动源码 executor: datacap 执行器源码 file: datacap 文件格式化转换器源码 fs: datacap 文件系统源码 lib: datacap 依赖包源码 notify: datacap 通知器源码 parser: datacap sql 解析器源码 plugin: datacap 插件源码 scheduler: datacap 调度器源码 shaded: datacap 重制依赖源码 --- 找到 DataCap 主文件，路径为 src/main/java/io/edurt/datacap/server/DataCap.java 在文件上右键，类似下图 !img2.png 弹出类似如下窗口 !img3.png 点击 Modify options, 找到 Program arguments，并点击它 !img4.png 此时窗口将会变成 !img5.png 在输入框中添加 --spring.config.location= 里面的值填写 configure/etc/conf 的绝对路径，比如项目在 /root/datacap 目录下，那么该值为 /root/datacap/configure/etc/conf/ !!! danger 注意 conf 后必须增加 / 否则配置会不生效 !!! 完成后点击 OK 按钮，即可启动服务，访问 http://localhost:9096/ 成功后，表示服务启动成功。",
    "url": "https://datacap.devlive.org/en/developer/server/home.html",
    "lang": "en"
  },
  {
    "title": "UI 端",
    "content": "本文章主要用来介绍我们如何来贡献 datacap UI 端源码。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------------|:---------|:---| | Node | v18.x+ | 必须 | | yarn | 1.22+ | 必须 | | IDEA \\| Eclipse \\| WebStrom | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 可参考 服务端 文档中的 加载源码到 IDEA 部分。 --- 进入源码目录 sql cd core/datacap-ui 安装依赖 (推荐使用 yarn 或 pnpm) bash pnpm install 启动服务 bash pnpm run dev 返回类似如下信息 !img.png 通过浏览器访问 http://localhost:8080 调试源代码，不要使用 Network 返回的地址，看到返回如下页面表示启动成功。 !img1.png !!! danger 如果服务端也启动后，系统将会跳转到登录页面。 !!!",
    "url": "https://datacap.devlive.org/en/developer/web/home.html",
    "lang": "en"
  },
  {
    "title": "文档端",
    "content": "本文章主要用来介绍我们如何来贡献 datacap UI 端源码。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------|:-----|:---| | mkdocs | 1.5+ | 必须 | | Python | 3+ | 必须 | | IDEA \\| Eclipse \\| 其他 | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 可参考 服务端 文档中的 加载源码到 IDEA 部分。 --- 进入源码目录 sql cd docs 安装依赖 bash pip install -r requirements.txt 启动服务 bash mkdocs serve --dev-addr=0.0.0.0:8001 返回类似如下信息 !img.png 通过浏览器访问 http://localhost:8001 调试源代码，看到返回如下页面表示启动成功。 !img1.png",
    "url": "https://datacap.devlive.org/en/developer/doc/home.html",
    "lang": "en"
  },
  {
    "title": "SDK 集成",
    "content": "本文章主要用来介绍如何集成 datacap 提供的 plugin 插件快速访问对应数据源。在本文中我们使用 datacap-jdbc-mysql SDK 来访问 MySQL 数据源。 --- 引入插件依赖 Apache Maven xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-jdbc-mysql</artifactId> <version>VERSION</version> </dependency> Gradle xml implementation group: 'io.edurt.datacap', name: 'datacap-jdbc-mysql', version: 'VERSION' !!! danger 请将 VERSION 替换为对应的版本号 !!! --- java String sql = \"SHOW TABLES\"; // 构建连接配置 Configure configure = new Configure(); configure.setHost(\"localhost\"); configure.setPort(3306); configure.setUsername(Optional.of(\"root\")); configure.setPassword(Optional.of(\"12345678\")); // 初始化 MySQL 插件 MySQLPlugin plugin = new MySQLPlugin(); plugin.connect(configure); // 获取执行结果 Response response = plugin.execute(sql); response.columns.forEach(item -> { log.info(item); }); !!! note 请注意 datacap-jdbc-mysql 插件仅支持 MySQL 数据源，如果需要其他 SDK 请使用其他的插件。 !!!",
    "url": "https://datacap.devlive.org/en/developer/sdk/home.html",
    "lang": "en"
  },
  {
    "title": "自定义功能",
    "content": "本文章主要用来介绍如何集成在 datacap 中开发新的功能。我们这里以 github 中的 issues-481 为例来讲解。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------|:--------------|:---| | JDK | 1.8 \\| 11 | 必须 | | Maven | >= 3.5 | 可选 | | IDEA \\| Eclipse \\| 其他 | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 可参考 服务端 文档中的 加载源码到 IDEA 部分。 --- 针对于源码开发分为以下部分： 服务端 [可选] UI 端 [可选] 文档 (如果增加了新功能，涉及到 UI 建议添加文档方便用户快速了解使用方式) --- 服务端的源码大致可以分为以下部分： entity ： 数据模型 repository： 数据库操作 service： 业务逻辑 controller： 接口 sql： SQL 语句 以上三个部分在 datacap 中已经提供相应的基础支持，如果没有特殊的需求可以直接使用提供的基础功能，该示例中我们不做特殊的需求开发，只用到 datacap 提供的基础功能。 --- 在 datacap 中提供了 BaseEntity 类，可以作为基础的实体类使用。它提供了以下属性： id：主键 name：名称 active：是否激活 createtime：创建时间 updatetime：更新时间 我们新建 ReportEntity 类，代码如下： java import edu.umd.cs.findbugs.annotations.SuppressFBWarnings; import io.edurt.datacap.service.enums.ReportType; import lombok.AllArgsConstructor; import lombok.Data; import lombok.EqualsAndHashCode; import lombok.NoArgsConstructor; import lombok.ToString; import lombok.experimental.SuperBuilder; import org.springframework.data.jpa.domain.support.AuditingEntityListener; import javax.persistence.Column; import javax.persistence.Entity; import javax.persistence.EntityListeners; import javax.persistence.EnumType; import javax.persistence.Enumerated; import javax.persistence.JoinColumn; import javax.persistence.JoinTable; import javax.persistence.ManyToOne; import javax.persistence.Table; @Data @SuperBuilder @ToString @NoArgsConstructor @AllArgsConstructor @EqualsAndHashCode(callSuper = true) @Entity @Table(name = \"datacapreport\") @EntityListeners(AuditingEntityListener.class) @SuppressFBWarnings(value = {\"EIEXPOSEREP\", \"EQOVERRIDINGEQUALSNOTSYMMETRIC\"}) public class ReportEntity extends BaseEntity { @Column(name = \"configure\") private String configure; @Column(name = \"type\") @Enumerated(EnumType.STRING) private ReportType type; @ManyToOne @JoinTable(name = \"datacapreportuserrelation\", joinColumns = @JoinColumn(name = \"reportid\"), inverseJoinColumns = @JoinColumn(name = \"userid\")) private UserEntity user; } !!! warning 代码中的 @SuperBuilder 一定要添加，不然我们无法使用父类提供的 builder 方法。 !!! @Table(name = \"datacapreport\") 这里我们需要修改表名，格式为 datacap${表名}。 在以上示例中我们增加了三个属性 (根据功能的情况而定需要的属性)： configure：配置 type：类型 user：用户关联信息 --- 我们新建 ReportRepository 接口，代码如下： java package io.edurt.datacap.service.repository; import io.edurt.datacap.service.entity.ReportEntity; import org.springframework.data.repository.PagingAndSortingRepository; public interface ReportRepository extends PagingAndSortingRepository<ReportEntity, Long> { } 如果没有特殊需求，这里我们使用 JPA 提供的默认方法，无需自己实现。 --- 在 datacap 中提供了 BaseService 类，可以作为基础的实体类使用。它提供了以下属性： getAll 根据指定的过滤器获取所有数据 getById 根据指定的 ID 获取数据 saveOrUpdate 保存或更新数据 deleteById 删除指定的 ID 数据 我们新建 ReportService 类，代码如下： java package io.edurt.datacap.service.service; import io.edurt.datacap.service.entity.ReportEntity; public interface ReportService extends BaseService<ReportEntity> { } 添加实现类 ReportServiceImpl，代码如下： java package io.edurt.datacap.service.service.impl; import io.edurt.datacap.service.service.ReportService; import org.springframework.stereotype.Service; @Service public class ReportServiceImpl implements ReportService { } !!! note 如果没有特殊的需求，实现类可以不添加。 !!! --- 在 datacap 中提供了 BaseController 类，可以作为基础的实体类使用。它提供了以下属性： list 根据指定的过滤器获取所有数据 saveOrUpdate 保存或更新数据 delete 删除指定的 ID 数据 deleteForPath 根据指定的路径删除数据 getInfoForPath 根据指定的路径获取数据 我们新建 ReportController 类，代码如下： java package io.edurt.datacap.server.controller; import io.edurt.datacap.service.entity.ReportEntity; import io.edurt.datacap.service.repository.ReportRepository; import io.edurt.datacap.service.service.ReportService; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController() @RequestMapping(value = \"/api/v1/report\") public class ReportController extends BaseController<ReportEntity> { private final ReportRepository repository; private final ReportService service; protected ReportController(ReportRepository repository, ReportService service) { super(repository, service); this.repository = repository; this.service = service; } } @RequestMapping(value = \"/api/v1/report\") 这里我们需要修改表名，格式为 /api/v1/${路由}. !!! danger 一定要将自定义 controller 的构造函数中的参数修改为相关的具体类。 !!! --- !!! note 如果没有新增 SQL 相关可以忽略此步骤。 !!! 通过以上步骤已经提供了 API，底层处理相关类，剩下的需要我们添加对于功能实现的相关 SQL 语句。 将 SQL 语句添加到 datacap.sql 文件中，格式如下： sql CREATE TABLE datacapreport ( id BIGINT AUTOINCREMENT PRIMARY KEY, name VARCHAR(255), active BOOLEAN DEFAULT TRUE, createtime DATETIME, updatetime DATETIME, configure LONGTEXT, type VARCHAR(255) ); CREATE TABLE datacapreportuserrelation ( reportid BIGINT, userid BIGINT ); !!! note 如果是新版本开发的话，需要在 core/datacap-server/src/main/schema 目录下新建 版本号 文件夹，在该文件夹下创建 schema.sql 文件。并将内容添加到 schema.sql 文件中。 !!! --- UI 端的源码大致可以分为以下部分： 构建菜单 构建 Vue 页面 --- !!! note 如果构建的功能不支持菜单，可以忽略此步骤。此操作需要登录管理员权限。 !!! 构建菜单可以参考 管理菜单。 该功能的配置如下 !img.png 跳转到权限模块进行新菜单权限的分配。 !img1.png --- 在修改国际化配置需要修改两个配置文件(如果增加了其他的国际化文件修改的会更多) core/datacap-web/src/i18n/langs/en/common.ts core/datacap-web/src/i18n/langs/zhCn/common.ts 在文件中增加以下代码 java report: 'Report' report: '报表' --- 在 core/datacap-web/src/services/admin 目录下新建 ReportService.ts 文件，代码如下： ts import {ResponseModel} from '@/model/ResponseModel'; import {BaseService} from '@/services/BaseService'; const baseUrl = '/api/v1/report'; class ReportService extends BaseService<any> { constructor() { super(baseUrl); } deleteById(id: number): Promise<ResponseModel> { throw new Error('Method not implemented.'); } getByName<T>(name: string): Promise<ResponseModel> { return Promise.resolve(undefined); } } export default new ReportService(); --- 在 core/datacap-web/src/views/admin 目录下新建 report 目录，并在该文件夹下创建 ReportUtils.ts 文件，代码如下： ts const createHeaders = (i18n: any) => { return [ { title: i18n.t('common.no'), key: 'id', sortable: 'custom' }, { title: i18n.t('common.name'), key: 'name' }, { title: i18n.t('common.type'), key: 'type' }, { title: i18n.t('common.createTime'), key: 'createTime', ellipsis: true, tooltip: true }, { title: i18n.t('common.endTime'), key: 'endTime', ellipsis: true, tooltip: true }, { title: i18n.t('common.action'), slot: 'action', key: 'action' } ]; } export { createHeaders }; 在 core/datacap-web/src/views/admin 目录下新建 report 目录，并在该文件夹下创建 AdminReport.vue 文件，代码如下： vue <template> <div> <Card style=\"width:100%\" :title=\"$t('common.report')\" dis-hover> <Table :loading=\"loading\" :columns=\"headers\" :data=\"data.content\"> <template #action=\"{ row }\"> <Space> </Space> </template> </Table> <p v-if=\"!loading\" style=\"margin-top: 10px;\"> <Page v-model=\"pagination.current\" show-sizer show-elevator show-total :total=\"pagination.total\" :page-size=\"pagination.pageSize\" @on-page-size-change=\"handlerSizeChange\" @on-change=\"handlerIndexChange\"> </Page> </p> </Card> </div> </template> <script lang=\"ts\"> import {defineComponent} from \"vue\"; import {useI18n} from 'vue-i18n'; import Common from \"@/common/Common\"; import {ResponsePage} from \"@/model/ResponsePage\"; import {createHeaders} from \"@/views/admin/report/ReportUtils\"; import ReportService from \"@/services/admin/ReportService\"; import {Filter} from \"@/model/Filter\"; import {Pagination, PaginationBuilder} from \"@/model/Pagination\"; const filter: Filter = new Filter(); const pagination: Pagination = PaginationBuilder.newInstance(); export default defineComponent({ name: \"ReportAdmin\", setup() { const i18n = useI18n(); const headers = createHeaders(i18n); const currentUserId = Common.getCurrentUserId(); return { headers, filter, currentUserId } }, data() { return { data: ResponsePage, loading: false, pagination: { total: 0, current: 1, pageSize: 10 } } }, created() { this.handlerInitialize(this.filter); }, methods: { handlerInitialize(filter: Filter) { this.loading = true; ReportService.getAll(filter) .then((response) => { if (response.status) { this.data = response.data; this.pagination.total = response.data.total; } this.loading = false; }) }, handlerSizeChange(size: number) { this.pagination.pageSize = size; this.handlerTableChange(this.pagination); }, handlerIndexChange(index: number) { this.pagination.current = index; this.handlerTableChange(this.pagination); }, handlerTableChange(pagination: any) { this.pagination.current = pagination.current; this.pagination.pageSize = pagination.pageSize; this.handlerInitialize(this.filter) } } }); </script>",
    "url": "https://datacap.devlive.org/en/developer/feature/home.html",
    "lang": "en"
  },
  {
    "title": "Java 实现",
    "content": "DataCap 支持自定义插件，使用者可以编写自己的插件集成到系统中。该文档主要讲解如何快速集成一个插件到 DataCap 系统中。 !!! note 本文使用集成基于 HTTP 协议的 QuestDB 数据存储系统来演示。 !!! xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-spi</artifactId> <scope>provided</scope> </dependency> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-common</artifactId> </dependency> <dependency> <groupId>commons-beanutils</groupId> <artifactId>commons-beanutils</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>org.testcontainers</groupId> <artifactId>testcontainers</artifactId> <scope>test</scope> </dependency> 以上配置添加了 datacap-spi 和 datacap-common 模块，其他的是一些辅助依赖。 !!! warning 需要注意的是如果您是单独开启的项目需要指定各个依赖的版本号。 !!! java public class QuestDBPluginModule extends AbstractPluginModule implements PluginModule { @Override public String getName() { return \"QuestDB\"; } @Override public PluginType getType() { return PluginType.HTTP; } @Override public AbstractPluginModule get() { return this; } protected void configure() { Multibinder<Plugin> plugin = Multibinder.newSetBinder(this.binder(), Plugin.class); plugin.addBinding().to(QuestDBPlugin.class); } } 加载器需要继承 AbstractPluginModule 类，并实现 PluginModule 接口，这样系统会在启动时自动将插件加载到系统中。 !!! note 需要注意的是，需要覆盖父类中的 configure() 方法，并将插件绑定到系统中。 !!! java @Slf4j public class QuestDBPlugin implements Plugin { private HttpConfigure configure; private HttpConnection connection; private Response response; @Override public String name() { return \"QuestDB\"; } @Override public String description() { return \"Integrate QuestDB data sources\"; } @Override public PluginType type() { return PluginType.HTTP; } @Override public void connect(Configure configure) { try { this.response = new Response(); this.configure = new HttpConfigure(); BeanUtils.copyProperties(this.configure, configure); this.connection = new HttpConnection(this.configure, this.response); } catch (Exception ex) { this.response.setIsConnected(Boolean.FALSE); this.response.setMessage(ex.getMessage()); } } @Override public Response execute(String content) { if (ObjectUtils.isNotEmpty(this.connection)) { log.info(\"Execute questdb plugin logic started\"); this.response = this.connection.getResponse(); QuestDBAdapter processor = new QuestDBAdapter(this.connection); this.response = processor.handlerExecute(content); log.info(\"Execute questdb plugin logic end\"); } this.destroy(); return this.response; } @Override public void destroy() { if (ObjectUtils.isNotEmpty(this.connection)) { this.connection.destroy(); } } } 执行器需要实现 Plugin 接口，该接口中提供了以下方法 name(): 插件有唯一名称，同名插件只会在第一次加载时生效 description(): 对于该插件的描述 type(): 插件类型 connect(Configure configure): 插件需要提前连接信息，比如当前插件插件，就是插件的连接阶段（系统预设 HTTP 连接方式直接使用）。 execute(String content): 具体执行操作逻辑 destroy(): 插件最后的销毁，注意销毁需要包含连接中的信息 java @Slf4j public class QuestDBAdapter extends HttpAdapter { public QuestDBAdapter(HttpConnection connection) { super(connection); } @Override public Response handlerExecute(String content) { Time processorTime = new Time(); processorTime.setStart(new Date().getTime()); Response response = this.httpConnection.getResponse(); HttpConfigure configure = new HttpConfigure(); if (response.getIsConnected()) { List<String> headers = new ArrayList<>(); List<String> types = new ArrayList<>(); List<Object> columns = new ArrayList<>(); try { BeanUtils.copyProperties(configure, this.httpConnection.getConfigure()); configure.setAutoConnected(Boolean.FALSE); configure.setRetry(0); configure.setMethod(HttpMethod.GET); configure.setPath(\"exec\"); Map<String, String> parameters = Maps.newHashMap(); parameters.put(\"query\", content); configure.setParams(parameters); configure.setDecoded(true); HttpConnection httpConnection = new HttpConnection(configure, new Response()); HttpClient httpClient = HttpClient.getInstance(configure, httpConnection); String body = httpClient.execute(); QuestDBResponse requestResponse = JSON.objectmapper.readValue(body, QuestDBResponse.class); if (ObjectUtils.isNotEmpty(requestResponse.getQuery())) { response.setIsSuccessful(true); if (ObjectUtils.isNotEmpty(requestResponse.getColumns())) { requestResponse.getColumns() .forEach(schema -> { headers.add(schema.getName()); types.add(schema.getType()); }); } requestResponse.getDataset() .forEach(record -> columns.add(handlerFormatter(configure.getFormat(), headers, record))); } else { response.setIsSuccessful(Boolean.FALSE); response.setMessage(requestResponse.getError()); } } catch (Exception ex) { log.error(\"Execute content failed content {} exception \", content, ex); response.setIsSuccessful(Boolean.FALSE); response.setMessage(ex.getMessage()); } finally { response.setHeaders(headers); response.setTypes(types); response.setColumns(columns); } } processorTime.setEnd(new Date().getTime()); response.setProcessor(processorTime); return response; } } 插件转换器用于对当前插件执行后的结果的转化，将其转换为 DataCap 中可以使用的逻辑。主要是用于封装 Response 返回结果。 本文是基于 JDBC 的插件所以直接继承 HttpAdapter 父类即可实现部分功能。 在 resources 源目录下添加 META-INF 和 services 目录 !!! warning services 在 resources 目录中需要 !!! 创建 io.edurt.datacap.spi.PluginModule 文件，内容如下 java io.edurt.datacap.plugin.http.questdb.QuestDBPluginModule 该文件的内容是我们定义好的插件加载模块。 !!! warning 插件的单元测试可以参考已经发布的插件进行测试 !!!",
    "url": "https://datacap.devlive.org/en/developer/plugin/java/home.html",
    "lang": "en"
  },
  {
    "title": "自定义 Plugin 连接器",
    "content": "DataCap 支持自定义插件，使用者可以编写自己的插件集成到系统中。该文档主要讲解如何快速集成一个插件到 DataCap 系统中。 !!! note 本文使用集成基于 JDBC 协议的 StarRocks 数据存储系统来演示。 !!! xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-spi</artifactId> <scope>provided</scope> </dependency> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-common</artifactId> </dependency> <dependency> <groupId>commons-beanutils</groupId> <artifactId>commons-beanutils</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>org.testcontainers</groupId> <artifactId>testcontainers</artifactId> <scope>test</scope> </dependency> 以上配置添加了 datacap-spi 和 datacap-common 模块，其他的是一些辅助依赖。 !!! warning 需要注意的是如果您是单独开启的项目需要指定各个依赖的版本号。 !!! kotlin class StarRocksPluginModule : AbstractPluginModule(), PluginModule { override fun getName(): String { return \"StarRocks\" } override fun getType(): PluginType { return PluginType.JDBC } override fun get(): AbstractPluginModule { return this } override fun configure() { val module = Multibinder.newSetBinder(binder(), String::class.java) module.addBinding().toInstance(this.javaClass.simpleName) val plugin: Multibinder<Plugin> = Multibinder.newSetBinder(binder(), Plugin::class.java) plugin.addBinding().to(StarRocksPlugin::class.java) } } 加载器需要继承 AbstractPluginModule 类，并实现 PluginModule 接口，这样系统会在启动时自动将插件加载到系统中。 !!! note 需要注意的是，需要覆盖父类中的 configure() 方法，并将插件绑定到系统中。 !!! kotlin class StarRocksPlugin : Plugin { private val log = getLogger(StarRocksPlugin::class.java) private var jdbcConfigure: JdbcConfigure? = null private var jdbcConnection: JdbcConnection? = null private var jdbcResponse: Response? = null override fun name(): String { return \"StarRocks\" } override fun description(): String { return \"Integrate StarRocks data sources\" } override fun type(): PluginType { return PluginType.JDBC } override fun connect(configure: Configure?) { try { log.info(\"Connecting to StarRocks\") jdbcResponse = Response() jdbcConfigure = JdbcConfigure() BeanUtils.copyProperties(jdbcConfigure, configure) jdbcConfigure!!.jdbcDriver = \"com.mysql.cj.jdbc.Driver\" jdbcConfigure!!.jdbcType = \"mysql\" jdbcConnection = object : JdbcConnection(jdbcConfigure, jdbcResponse) {} } catch (ex: Exception) { jdbcResponse!!.isConnected = false jdbcResponse!!.message = ex.message } } override fun execute(content: String?): Response { if (ObjectUtils.isNotEmpty(jdbcConnection)) { log.info(\"Execute starrocks plugin logic started\") jdbcResponse = jdbcConnection?.response val processor = JdbcAdapter(jdbcConnection) jdbcResponse = processor.handlerExecute(content) log.info(\"Execute starrocks plugin logic end\") } destroy() return jdbcResponse!! } override fun destroy() { if (ObjectUtils.isNotEmpty(jdbcConnection)) { jdbcConnection?.destroy() jdbcConnection = null } } } 执行器需要实现 Plugin 接口，该接口中提供了以下方法 name(): 插件有唯一名称，同名插件只会在第一次加载时生效 description(): 对于该插件的描述 type(): 插件类型 connect(Configure configure): 插件需要提前连接信息，比如当前插件插件，就是插件的连接阶段（系统预设 HTTP 连接方式直接使用）。 execute(String content): 具体执行操作逻辑 destroy(): 插件最后的销毁，注意销毁需要包含连接中的信息 插件转换器用于对当前插件执行后的结果的转化，将其转换为 DataCap 中可以使用的逻辑。主要是用于封装 Response 返回结果。 本文是基于 JDBC 的插件所以直接继承 JdbcAdapter 父类即可实现部分功能。 在 resources 源目录下添加 META-INF 和 services 目录 !!! warning services 在 resources 目录中需要 !!! 创建 io.edurt.datacap.spi.PluginModule 文件，内容如下 java io.edurt.datacap.plugin.jdbc.starrocks.StarRocksPluginModule 该文件的内容是我们定义好的插件加载模块。 !!! warning 插件的单元测试可以参考已经发布的插件进行测试 !!!",
    "url": "https://datacap.devlive.org/en/developer/plugin/kotlin/home.html",
    "lang": "en"
  },
  {
    "title": "自定义 Fs 文件系统",
    "content": "DataCap 支持自定义文件系统，使用者可以编写自己的文件存储系统集成到 DataCap 中。该文档主要讲解如何快速集成一个文件存储系统到 DataCap 系统中。 该模块我们主要使用到的是 fs 模块内的代码，我们本文使用本地存储来做示例。 --- 新建项目后在 pom.xml 文件中增加以下内容： xml <dependencies> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-spi</artifactId> <version>${project.version}</version> </dependency> </dependencies> 我们添加 datacap-fs-spi 依赖，这样我们就可以实现集成文件系统。 --- kotlin package io.edurt.datacap.fs import com.google.inject.multibindings.Multibinder class LocalModule : FsModule() { override fun configure() { Multibinder.newSetBinder(binder(), Fs::class.java) .addBinding() .to(LocalFs::class.java) } } --- kotlin package io.edurt.datacap.fs import org.slf4j.LoggerFactory import java.io.File class LocalFs : Fs { private val log = LoggerFactory.getLogger(this.javaClass) override fun writer(request: FsRequest): FsResponse { log.info(\"LocalFs writer origin path [ {} ]\", request.localPath) val targetPath = listOf(request.endpoint, request.bucket, request.fileName).joinToString(File.separator) val response = FsResponse.builder() .origin(request.localPath) .remote(targetPath) .successful(true) .build() log.info(\"LocalFs writer target path [ {} ]\", targetPath) try { if (request.localPath == null || request.localPath.isEmpty()) { IOUtils.copy(request.stream, targetPath, true) } else { IOUtils.copy(request.localPath, targetPath, true) } log.info(\"LocalFs writer [ {} ] successfully\", targetPath) } catch (e: Exception) { log.error(\"LocalFs writer error\", e) response.successful = false response.message = e.message } return response } override fun reader(request: FsRequest): FsResponse { val targetPath = listOf(request.endpoint, request.bucket, request.fileName).joinToString(File.separator) log.info(\"LocalFs reader origin path [ {} ]\", targetPath) val response = FsResponse.builder() .remote(targetPath) .successful(true) .build() try { response.context = IOUtils.reader(targetPath) log.info(\"LocalFs reader [ {} ] successfully\", targetPath) } catch (e: Exception) { log.error(\"LocalFs reader error\", e) response.successful = false response.message = e.message } return response } } 在转换器中我们只需要实现以下两个方法： public FsResponse writer(FsRequest request) 用于写数据到文件系统 public FsResponse reader(FsRequest request) 用于读取文件系统中的数据 --- kotlin package io.edurt.datacap.fs import java.io.InputStream import java.nio.file.Files import java.nio.file.Path import java.nio.file.Paths import java.nio.file.StandardCopyOption import java.nio.file.StandardOpenOption object IOUtils { / Copies a file from the source path to the target path. @param source the path of the file to be copied @param target the path where the file should be copied to @param createdDir a flag indicating whether the parent directories of the target path should be created if they do not exist @return true if the file was successfully copied, false otherwise / fun copy(source: String, target: String, createdDir: Boolean): Boolean { try { val targetPath: Path = Paths.get(target) if (createdDir) { Files.createDirectories(targetPath.parent) } Files.copy(Paths.get(source), targetPath, StandardCopyOption.REPLACEEXISTING) return true } catch (e: Exception) { throw RuntimeException(e) } } / Copies the contents of the input stream to the specified target file path. @param stream the input stream containing the contents to be copied @param target the path of the target file where the contents will be copied to @param createdDir indicates whether the parent directory of the target file has been created @return true if the contents were successfully copied, false otherwise / fun copy(stream: InputStream, target: String, createdDir: Boolean): Boolean { try { val targetPath: Path = Paths.get(target) if (createdDir) { Files.createDirectories(targetPath.parent) } Files.copy(stream, targetPath, StandardCopyOption.REPLACEEXISTING) return true } catch (e: Exception) { throw RuntimeException(e) } } / This function takes a source file path and returns an InputStream object that can be used to read the contents of the file. @param source the path of the source file to read @return an InputStream object representing the source file / fun reader(source: String): InputStream { try { return Files.newInputStream(Paths.get(source), StandardOpenOption.READ) } catch (e: Exception) { throw RuntimeException(e) } } } 在转换器中我们主要实现本地数据的读写操作。 --- 在 resources 源目录下添加 META-INF 和 services 目录，格式为 resources/META-INF/services，创建 io.edurt.datacap.fs.FsModule 文件，内容如下 kotlin io.edurt.datacap.fs.LocalModule !!! note 通过以上内容我们实现了本地数据存储的模块支持。我们只需要在要使用存储的地方引用该模块即可。 !!! !!! warning 我们建议添加对模块的测试用例，可参考已经提交的代码进行模拟测试。 !!!",
    "url": "https://datacap.devlive.org/en/developer/filesystem/home.html",
    "lang": "en"
  },
  {
    "title": "自定义 File 转换器",
    "content": "DataCap 支持自定义 File 转换器，使用者可以编写自己的文件转换器集成到 DataCap 中。该文档主要讲解如何快速集成一个文件转换器到 DataCap 系统中。 该模块我们主要使用到的是 file 模块内的代码，我们本文使用 json 来做示例。 --- 新建项目后在 pom.xml 文件中增加以下内容： xml <dependencies> <dependency> <groupId>org.jetbrains.kotlin</groupId> <artifactId>kotlin-reflect</artifactId> </dependency> <dependency> <groupId>com.google.inject</groupId> <artifactId>guice</artifactId> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> </dependency> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-file-spi</artifactId> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-core</artifactId> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.jetbrains.dokka</groupId> <artifactId>dokka-maven-plugin</artifactId> </plugin> </plugins> </build> 我们添加 datacap-file-spi 依赖，这样我们就可以实现集成文件转换器。 --- kotlin package io.edurt.datacap.convert.json import com.google.inject.multibindings.Multibinder import io.edurt.datacap.convert.File import io.edurt.datacap.convert.FileModule class JsonModule : FileModule() { override fun configure() { Multibinder.newSetBinder(this.binder(), File::class.java) .addBinding() .to(JsonFile::class.java) } } --- kotlin package io.edurt.datacap.convert.json import com.fasterxml.jackson.core.JsonEncoding import com.fasterxml.jackson.core.JsonFactory import com.fasterxml.jackson.core.JsonGenerationException import com.fasterxml.jackson.databind.JsonNode import com.fasterxml.jackson.databind.ObjectMapper import com.fasterxml.jackson.databind.node.ObjectNode import io.edurt.datacap.common.utils.DateUtils import io.edurt.datacap.convert.File import io.edurt.datacap.convert.FileConvert.formatFile import io.edurt.datacap.convert.model.FileRequest import io.edurt.datacap.convert.model.FileResponse import org.slf4j.LoggerFactory.getLogger import java.io.BufferedReader import java.io.IOException import java.io.InputStreamReader import java.util.Objects.requireNonNull class JsonFile : File { private val log = getLogger(this::class.java) override fun format(request: FileRequest): FileResponse { val response = FileResponse() try { log.info(\"${name()} format start time [ ${DateUtils.now()} ]\") log.info(\"${name()} format headers start\") response.headers = request.headers log.info(\"${name()} format headers end\") log.info(\"${name()} format columns start\") val mapper = ObjectMapper() val columns = mutableListOf<Any>() request.columns .forEach { column -> val jsonNode = mapper.createObjectNode() for (headerIndex in request.headers.indices) { val header = request.headers[headerIndex] as String when (column) { is List<> -> jsonNode.putPOJO(header, column[headerIndex]) else -> jsonNode.putPOJO(header, column) } } columns.add(jsonNode) } response.columns = columns log.info(\"${name()} format columns end\") log.info(\"${name()} format end time [ ${DateUtils.now()} ]\") response.successful = true } catch (e: IOException) { response.successful = false response.message = e.message } return response } override fun formatStream(request: FileRequest): FileResponse { val response = FileResponse() try { requireNonNull(\"Stream must not be null\") log.info(\"${name()} format stream start time [ ${DateUtils.now()} ]\") val mapper = ObjectMapper() request.stream ?.let { BufferedReader(InputStreamReader(it, Charsets.UTF8)).use { reader -> val jsonNode: JsonNode = mapper.readTree(reader) log.info(\"${name()} format stream json node count [ ${jsonNode.size()} ]\") val headers = mutableListOf<Any>() if (jsonNode.isArray && jsonNode.size() > 0) { jsonNode[0].fieldNames() .forEachRemaining { headers.add(it) } } response.headers = headers val columns = mutableListOf<Any>() if (jsonNode.isArray) { jsonNode.elements() .forEachRemaining { node -> val column = mutableMapOf<String, Any?>() node.fields() .forEachRemaining { field -> column[field.key] = field.value } columns.add(column) } } response.columns = columns it.close() } } log.info(\"${name()} format stream end time [ ${DateUtils.now()} ]\") response.successful = true } catch (e: IOException) { response.successful = false response.message = e.message } return response } override fun writer(request: FileRequest): FileResponse { val response = FileResponse() try { log.info(\"${name()} writer origin path [ ${request.path} ]\") log.info(\"${name()} writer start time [ ${DateUtils.now()} ]\") val file = formatFile(request, name()) log.info(\"${name()} writer file absolute path [ ${file.absolutePath} ]\") val factory = JsonFactory() factory.createGenerator(file, JsonEncoding.UTF8) .use { generator -> generator.writeStartArray() request.columns .forEach { column -> generator.writeStartObject() for (headerIndex in request.headers.indices) { when (column) { is List<> -> generator.writeObjectField(request.headers[headerIndex] as String, column[headerIndex]) is ObjectNode -> { generator.codec = ObjectMapper() val header = request.headers[headerIndex] as String generator.writeObjectField(header, column.get(header)) } else -> generator.writeObjectField(request.headers[headerIndex] as String, column) } } generator.writeEndObject() } generator.writeEndArray() } log.info(\"${name()} writer end time [ ${DateUtils.now()} ]\") response.path = file.absolutePath response.successful = true } catch (e: IOException) { response.successful = false response.message = e.message } catch (e: JsonGenerationException) { response.successful = false response.message = e.message } return response } override fun reader(request: FileRequest): FileResponse { val response = FileResponse() try { log.info(\"${name()} reader origin path [ ${request.path} ]\") log.info(\"${name()} reader start time [ ${DateUtils.now()} ]\") val file = formatFile(request, name()) log.info(\"${name()} reader file absolute path [ ${file.absolutePath} ]\") val mapper = ObjectMapper() val jsonNode: JsonNode = mapper.readTree(file) log.info(\"${name()} reader file json node count [ ${jsonNode.size()} ]\") log.info(\"${name()} reader file headers start\") val headers = mutableListOf<Any>() if (jsonNode.isArray && jsonNode.size() > 0) { jsonNode[0].fieldNames() .forEachRemaining { headers.add(it) } } response.headers = headers log.info(\"${name()} reader file headers end\") log.info(\"${name()} reader file columns start\") val columns = mutableListOf<Any>() if (jsonNode.isArray) { jsonNode.elements() .forEachRemaining { node -> val column = mutableMapOf<String, Any?>() node.fields() .forEachRemaining { field -> column[field.key] = field.value } columns.add(column) } } response.columns = columns log.info(\"${name()} reader file columns end\") response.successful = true } catch (e: Exception) { response.successful = false response.message = e.message } return response } } --- 在 resources 源目录下添加 META-INF 和 services 目录，格式为 resources/META-INF/services，创建 io.edurt.datacap.convert.FileModule 文件，内容如下 kotlin io.edurt.datacap.convert.json.JsonModule > 通过以上内容我们实现了 Json 文件转换器的支持。我们只需要在要使用 Json 文件转换器的地方引用该模块即可。比如我们在 server 模块中使用到该模块，则在 server/pom.xml 文件中增加以下内容 xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-file-json</artifactId> <version>${project.version}</version> </dependency> --- kotlin package io.edurt.datacap.convert.json import com.google.inject.Guice.createInjector import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.convert.File import io.edurt.datacap.convert.FileManager import org.junit.Assert.assertEquals import org.junit.Test class JsonModuleTest { private val injector: Injector = createInjector(FileManager()) @Test fun test() { injector.getInstance(Key.get(object : TypeLiteral<Set<File>>() {})) .stream() .findFirst() .ifPresent { assertEquals(\"Json\", it.name()) } } } --- kotlin package io.edurt.datacap.convert.json import com.google.inject.Guice.createInjector import com.google.inject.Injector import io.edurt.datacap.convert.FileFilter import io.edurt.datacap.convert.FileManager import io.edurt.datacap.convert.model.FileRequest import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory.getLogger import java.io.File import java.io.FileInputStream import kotlin.test.assertTrue class JsonFileTest { private val log = getLogger(this::class.java) private val name = \"Json\" private var injector: Injector? = null private val request: FileRequest = FileRequest() @Before fun before() { injector = createInjector(FileManager()) request.name = \"test\" request.path = System.getProperty(\"user.dir\") request.headers = listOf(\"name\", \"age\") val l1 = listOf(\"Test\", 12) val l2 = listOf(\"Test1\", 121) request.columns = listOf(l1, l2) } @Test fun testFormat() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> val response = file.format(request) log.info(\"headers: [ ${response.headers} ]\") response.columns .let { columns -> columns.forEachIndexed { index, line -> log.info(\"index: [ $index ], line: [ $line ]\") } } assertTrue { response.successful == true } } } } @Test fun testFormatStream() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> request.stream = FileInputStream(File(\"${System.getProperty(\"user.dir\")}/${request.name}.json\")) val response = file.formatStream(request) log.info(\"headers: [ ${response.headers} ]\") response.columns .let { columns -> columns.forEachIndexed { index, line -> log.info(\"index: [ $index ], line: [ $line ]\") } } assertTrue { response.successful == true } } } } @Test fun testWriter() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> assertTrue { file.writer(request) .successful == true } } } } @Test fun testReader() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> val response = file.reader(request) log.info(\"headers: ${response.headers}\") response.columns .forEach { log.info(\"columns: $it\") } assertTrue { response.successful == true } } } } }",
    "url": "https://datacap.devlive.org/en/developer/file/kotlin/home.html",
    "lang": "en"
  },
  {
    "title": "自定义 Notify 通知器",
    "content": "DataCap 支持自定义通知器，使用者可以编写自己的通知器集成到 DataCap 中。该文档主要讲解如何快速集成一个通知器到 DataCap 系统中。 该模块我们主要使用到的是 notify 模块内的代码，我们本文使用钉钉通知器来做示例。 --- 新建项目后在 pom.xml 文件中增加以下内容： xml <dependencies> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-notify-spi</artifactId> <version>${project.version}</version> </dependency> </dependencies> 我们添加 datacap-notify-spi 依赖，这样我们就可以实现集成通知器。 --- kotlin package io.edurt.datacap.notify.dingtalk import com.google.inject.multibindings.Multibinder import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.NotifyModule class DingTalkModule : NotifyModule() { override fun configure() { Multibinder.newSetBinder(this.binder(), Notify::class.java) .addBinding() .to(DingTalkNotify::class.java) } } --- kotlin package io.edurt.datacap.notify.dingtalk import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.model.NotifyRequest import io.edurt.datacap.notify.model.NotifyResponse class DingTalkNotify : Notify { override fun send(request: NotifyRequest): NotifyResponse { return DingTalkUtils.send(request) } } 在转换器中我们只需要实现以下两个方法： fun send(request: NotifyRequest): NotifyResponse 用于实现发送器执行逻辑 --- kotlin package io.edurt.datacap.notify.dingtalk import io.edurt.datacap.common.utils.JsonUtils import io.edurt.datacap.common.utils.SignUtils import io.edurt.datacap.lib.http.HttpClient import io.edurt.datacap.lib.http.HttpConfigure import io.edurt.datacap.lib.http.HttpMethod import io.edurt.datacap.notify.NotifyType import io.edurt.datacap.notify.dingtalk.model.ReturnModel import io.edurt.datacap.notify.dingtalk.model.TextModel import io.edurt.datacap.notify.model.NotifyRequest import io.edurt.datacap.notify.model.NotifyResponse import org.apache.commons.lang3.StringUtils.isNotEmpty import org.slf4j.Logger import org.slf4j.LoggerFactory.getLogger object DingTalkUtils { private val log: Logger = getLogger(DingTalkUtils::class.java) @JvmStatic fun send(request: NotifyRequest): NotifyResponse { val configure = HttpConfigure() configure.autoConnected = false configure.retry = 0 configure.protocol = \"https\" configure.host = \"oapi.dingtalk.com\" configure.port = 443 configure.path = \"robot/send\" configure.method = HttpMethod.POST val params = mutableMapOf(\"accesstoken\" to request.access) if (isNotEmpty(request.secret)) { val signResponse = SignUtils.sign(request.secret) log.info(\"Sign response: ${JsonUtils.toJSON(signResponse)}\") params[\"sign\"] = signResponse.sign params[\"timestamp\"] = signResponse.timestamp.toString() } configure.params = params log.info(\"Notify request params: ${JsonUtils.toJSON(params)}\") val text = TextModel() text.content = request.content configure.body = JsonUtils.toJSON(mapOf(\"text\" to text, \"msgtype\" to formatMessageType(request))) log.info(\"Notify request body: ${configure.body}\") val client = HttpClient(configure) val returnModel = JsonUtils.toObject(client.execute(), ReturnModel::class.java) val response = NotifyResponse() if (returnModel.code == 0) { response.successful = true response.message = null } else { response.successful = false response.message = returnModel.message } return response } private fun formatMessageType(request: NotifyRequest): String { if (request.type == NotifyType.TEXT) { return \"text\" } else { return \"markdown\" } } } 在工具类中我们主要实现发送钉钉消息操作。 --- 在 resources 源目录下添加 META-INF 和 services 目录，格式为 resources/META-INF/services，创建 io.edurt.datacap.fs.FsModule 文件，内容如下 kotlin io.edurt.datacap.notify.dingtalk.DingTalkModule > 通过以上内容我们实现了 DingTalk 通知器的支持。我们只需要在要使用 DingTalk 通知器器的地方引用该模块即可。比如我们在 server 模块中使用到该模块，则在 server/pom.xml 文件中增加以下内容 xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-notify-dingtalk</artifactId> <version>${project.version}</version> </dependency> kotlin package io.edurt.datacap.notify.dingtalk import io.edurt.datacap.notify.model.NotifyRequest import org.junit.Assert import org.junit.Before import org.junit.Test class DingTalkUtilsTest { private val request: NotifyRequest = NotifyRequest() @Before fun before() { request.access = \"ACCESS\" request.content = \"Test Message\" request.secret = \"SECRET\" } @Test fun testSend() { Assert.assertFalse( DingTalkUtils.send(request) .successful ) } } kotlin package io.edurt.datacap.notify.dingtalk import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.NotifyManager import org.junit.Assert.assertNotNull import org.junit.Before import org.junit.Test class DingTalkModuleTest { private val name = \"DingTalk\" private var injector: Injector? = null @Before fun before() { injector = Guice.createInjector(NotifyManager()) } @Test fun test() { val notify: Notify? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Notify>>() {})) ?.first { it.name() == name } assertNotNull(notify) } } kotlin package io.edurt.datacap.notify.dingtalk import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.NotifyManager import io.edurt.datacap.notify.model.NotifyRequest import org.junit.Assert.assertNotNull import org.junit.Before import org.junit.Test class DingTalkNotifyTest { private val name = \"DingTalk\" private var injector: Injector? = null private val request: NotifyRequest = NotifyRequest() @Before fun before() { injector = Guice.createInjector(NotifyManager()) request.access = \"ACCESS\" request.content = \"Test Message\" request.secret = \"SECRET\" } @Test fun test() { val notify: Notify? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Notify>>() {})) ?.first { it.name() == name } assertNotNull(notify?.send(request)) } }",
    "url": "https://datacap.devlive.org/en/developer/notify/kotlin/home.html",
    "lang": "en"
  },
  {
    "title": "流水线",
    "content": "在 DataCap 系统中，流水线功能用户可以随意配置，根据低层执行器的版本可以任意调整，系统会根据配置自动识别并编译最终的配置并发送到执行器中。 我们可提供的配置有: | 字段 | 类型 | 描述 | |:---------------:|:-----------:|:------------------------------------------------------:| | field | String | 字段名 | | origin | String | 默认等于字段值，自定义列名使用, 如果是 host|port 格式，系统将会将字段通过 : 进行拼接 | | required | Boolean | 当值为 true 时，表示该字段为必填项 | | override | Boolean | 如果该标志为 true，则表示通过用户配置提取该字段，默认数据将被丢弃 | | input | Boolean | 是否为输入参数 | | width | Integer | 组件宽度, 默认 300 | | type | FieldType | 字段类型, 默认 INPUT | | tooltip | String | 提示信息 | | description | String | 描述信息 | | value | Object | 当前配置输入的结果 | | hidden | Boolean | 如果该配置项为 true，则前端不会显示，启用后才会显示。 | | defaultValues | Array | 如果类型为 SELECT ，则需要传入默认数据 | !!! danger 我们不建议用户自行修改以上配置，如有出现配置异常将会导致插件无法使用该功能 !!!",
    "url": "https://datacap.devlive.org/en/developer/pipeline/home.html",
    "lang": "en"
  },
  {
    "title": "使用案例",
    "content": "",
    "url": "https://datacap.devlive.org/en/useCases.html",
    "lang": "en"
  },
  {
    "title": "合作伙伴",
    "content": "",
    "url": "https://datacap.devlive.org/en/partners.html",
    "lang": "en"
  },
  {
    "title": "自主机部署",
    "content": "DataCap 是支持用户将服务部署到自主机中。通过本文档用户可以了解如何在自主机中部署 DataCap。 --- !!! warning \"注意\" 该软件的二进制包基于以下系统进行编译和测试。它尚未在其他版本上进行测试，理论上受支持。 如果有不支持的系统，请使用源码编译方法主动编译二进制文件。 !!! | System | Version | |--------|------------| | JDK | \\>=11 | | MySQL | \\>=5.6.x | --- !!! info \"提示\" 从以下地址下载相应系统的二进制软件包进行安装。如果您需要使用源码安装请前往查看开发者文档模块。 !!! 1.下载最新发布版本 2.将二进制文件下载到本地后运行以下命令 bash tar -xvzf datacap-<VERSION>-bin.tar.gz 进入软件根目录 bash cd datacap-<VERSION> --- 对于软件的首次安装，您需要将 schema/datacap.sql 文件中的sql脚本导入MySQL服务器。注意需要导入的脚本根据下载的软件包进行匹配 !!! danger \"注意\" 如果您是通过其他版本升级，请执行 schema/<VERSION>/schema.sql !!! datacap 软件中的所有配置均在 configure/application.properties 文件中。 导入 SQL 脚本后，修改 configure/application.properties 配置文件以修改MySQL服务器的配置信息 properties server.port=9096 server.address=localhost spring.jackson.time-zone=GMT+8 spring.jackson.date-format=yyyy-MM-dd HH:mm:ss datacap.security.secret=DataCapSecretKey datacap.security.expiration=86400000 datacap.editor.sugs.maxSize=1000 server.port: 用于配置服务在服务器中启动监听的端口，默认为 9096 server.address: 用于配置服务在本地的监听地址，如果需要使用 IP+端口 方便外部机器访问，请不要设置为 localhost，建议设置为 0.0.0.0 spring.jackson.time-zone: 用于配置时区，默认为 GMT+8 spring.jackson.date-format: 用于配置日期格式，默认为 yyyy-MM-dd HH:mm:ss datacap.security.secret: 用于配置数据安全管理的密钥，默认为 DataCapSecretKey datacap.security.expiration: 用于配置数据安全管理的过期时间，单位为毫秒，默认为 86400000 datacap.editor.sugs.maxSize: 用于配置数据编辑器的最大行数，默认为 1000 已经失效不在使用 properties spring.mvc.throw-exception-if-no-handler-found=true spring.resources.add-mappings=false spring.web.resources.add-mappings=true spring.mvc.throw-exception-if-no-handler-found: 用于配置是否抛出异常 spring.resources.add-mappings: 用于配置是否启用静态资源映射 spring.web.resources.add-mappings: 用于配置是否启用静态资源映射 !!! danger \"注意\" 如果版本 >=8.x，请设置 allowPublicKeyRetrieval=true !!! properties spring.datasource.url=jdbc:mysql://localhost:3306/datacap?useUnicode=true&characterEncoding=UTF-8&zeroDateTimeBehavior=convertToNull&allowMultiQueries=true&useSSL=false&useOldAliasMetadataBehavior=true&jdbcCompliantTruncation=false&allowPublicKeyRetrieval=true spring.datasource.username=root spring.datasource.password=12345678 spring.datasource.url: 用于配置数据库连接地址 spring.datasource.username: 用于配置数据库用户名 spring.datasource.password: 用于配置数据库密码 !!! info \"提示\" 支持所有 Spring Data 的配置参数 !!! properties datacap.executor.data= datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=SPARK datacap.executor.startScript=start-seatunnel-spark-connector-v2.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel datacap.executor.data: 用于配置执行器的数据缓冲路径 datacap.executor.way: 用于配置执行器的执行方式，不同的执行器拥有不同的执行方式 datacap.executor.mode: 用于配置执行器的执行模式，不同的执行器拥有不同的执行模式 datacap.executor.engine: 用于配置执行器的执行引擎 datacap.executor.startScript: 用于配置执行器的启动脚本 datacap.executor.seatunnel.home: 用于配置执行器的 Apache Seatunnel 主目录 ::: tabs === \"Spark 引擎配置\" properties datacap.executor.data= datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=SPARK datacap.executor.startScript=start-seatunnel-spark-connector-v2.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel === \"Flink 引擎配置\" properties datacap.executor.data= datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=FLINK datacap.executor.startScript=start-seatunnel-flink-13-connector-v2.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel === \"Seatunnel 引擎配置\" properties datacap.executor.data= # Only support LOCAL datacap.executor.way=LOCAL datacap.executor.mode=CLIENT datacap.executor.engine=SEATUNNEL datacap.executor.startScript=seatunnel.sh datacap.executor.seatunnel.home=/opt/lib/seatunnel ::: properties datacap.config.data= datacap.cache.data= datacap.config.data: 用于配置上传配置文件的路径 datacap.cache.data: 用于配置上传缓存文件的路径 properties datacap.openai.backend=https://api.openai.com datacap.openai.token= datacap.openai.model=GPT35TURBO0613 datacap.openai.timeout=30 datacap.openai.backend: 用于配置 OpenAI 的后端地址 datacap.openai.token: 用于配置 OpenAI 的 token datacap.openai.model: 用于配置 OpenAI 的模型 datacap.openai.timeout: 用于配置 OpenAI 的超时时间，单位为秒 properties datacap.registration.enable= datacap.captcha.enable= datacap.cache.maximum=100000 datacap.cache.expiration=5 datacap.audit.sql.print=false datacap.registration.enable: 用于配置是否开启注册 datacap.captcha.enable: 用于配置是否开启验证码 datacap.cache.maximum: 用于配置缓存最大值 datacap.cache.expiration: 用于配置缓存过期时间，单位为分钟 datacap.audit.sql.print: 用于配置是否打印 SQL properties datacap.pipeline.maxRunning=100 datacap.pipeline.maxQueue=200 datacap.pipeline.reset=STOPPED datacap.pipeline.maxRunning: 用于配置最大运行数 datacap.pipeline.maxQueue: 用于配置最大队列 datacap.pipeline.reset: 用于配置重置策略 --- 支持的存储类型详见 https://github.com/devlive-community/datacap/tree/dev/fs properties datacap.fs.type=Local datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 用于配置文件系统类型 datacap.fs.access: 用于配置文件系统访问，该类型可以为空 datacap.fs.secret: 用于配置文件系统密钥，该类型可以为空 datacap.fs.endpoint: 用于配置文件系统端点，如果填写后将追加为目录 该类型可以为空 datacap.fs.bucket: 用于配置文件系统存储桶，该类型可以为空 properties datacap.fs.type=AliOss datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: AliOss properties datacap.fs.type=Qiniu datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: Qiniu properties datacap.experimental.autoLimit=true datacap.experimental.data={user.dir}/data datacap.experimental.avatarPath={username}/avatar/ datacap.experimental.autoLimit: 用于配置是否自动增加 LIMIT datacap.experimental.data: 用于配置实验性功能的数据路径 datacap.experimental.avatarPath: 用于配置实验性功能的头像路径 !!! warning \"警告\" 如果需要修改日志配置，只需修改 configure/logback.xml 配置文件即可 !!! !!! warning \"警告\" 如果您需要定制化 JVM 配置，只需修改 configure/jvm.conf 配置文件即可 !!! properties plugin.manager.extend.packages=com.fasterxml.jackson plugin.manager.extend.packages: 用于配置插件扩展包，配置后将优先加载父类加载器中的依赖 --- > 启动服务前请安装系统需要的各种插件，执行命令 ./bin/install-plugin.sh，也可以到服务商店中进行安装。 DataCap服务启动非常简单，执行以下脚本 bash ./bin/startup.sh 停止服务并执行以下脚本 bash ./bin/shutdown.sh !!! info \"提示\" 如果要调试系统，可以使用 ./bin/debug.sh 启动服务，但关闭窗口时它将停止 !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/install.html",
    "lang": "zh-CN"
  },
  {
    "title": "Docker 部署",
    "content": "DataCap 项目提供 devliveorg/datacap 包含 DataCap 服务器和默认配置的 Docker 映像。Docker 映像发布到 Docker Hub，可以与 Docker 运行时等一起使用。 要在 Docker 中运行 DataCap，您必须在计算机上安装 Docker 引擎。您可以从 Docker website, 或使用操作系统的打包系统。 使用 docker 命令从 devliveorg/datacap 图像。为其分配数据帽名称，以便以后更容易引用它。在后台运行它，并将默认 DataCap 端口（即 9096）从容器内部映射到工作站上的端口 9096。 bash docker run -d -p 9909:9096 --name datacap devliveorg/datacap 如果不指定容器映像标记，则默认为 latest ，但可以使用许多已发布的 DataCap 版本，例如 devliveorg/datacap:2024.4。 !!! danger \"注意\" 需要挂在外接 MySQL 配置，服务启动的方式为 bash docker run -d -p 9096:9096 -v /root/application.properties:/opt/app/datacap/configure/application.properties --name datacap devliveorg/datacap 假设您的配置文件在 /root/application.properties，如需要其他路径请指定绝对路径即可。 !!! 运行 docker ps 以查看在后台运行的所有容器。 bash -> % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2096fba19e2a devliveorg/datacap:latest \"sh ./bin/debug.sh\" 5 days ago Up 14 seconds 0.0.0.0:9909->9096/tcp datacap 您可以使用 docker stop datacap 和 docker start datacap 命令停止和启动容器。要完全删除已停止的容器，请运行 docker rm datacap。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/installContainers.html",
    "lang": "zh-CN"
  },
  {
    "title": "Docker Compose 部署",
    "content": "DataCap 项目提供 Docker Compose 方式部署，通过下载 docker-compose.yml 文件，或者使用以下代码进行服务部署。 --- > 只有基础的一些功能 yaml version: '3.8' services: app-mysql: image: mysql:latest environment: MYSQLROOTPASSWORD: 12345678 MYSQLDATABASE: datacap ports: \"3306:3306\" volumes: ./configure/schema/datacap.sql:/docker-entrypoint-initdb.d/schema.sql app-datacap: image: qianmoq/datacap:latest restart: always ports: \"9099:9099\" dependson: app-mysql app-clickhouse volumes: ./configure/docker/application.properties:/opt/app/datacap/configure/application.properties --- > 该方式包含了 数据集 功能 yaml version: '3.8' services: app-mysql: image: mysql:latest environment: MYSQLROOTPASSWORD: 12345678 MYSQLDATABASE: datacap ports: \"3306:3306\" volumes: ./configure/schema/datacap.sql:/docker-entrypoint-initdb.d/schema.sql app-clickhouse: image: clickhouse/clickhouse-server:latest restart: always ports: \"8123:8123\" environment: CLICKHOUSEDB=datacap app-datacap: image: qianmoq/datacap:latest restart: always ports: \"9099:9099\" dependson: app-mysql app-clickhouse volumes: ./configure/docker/application.properties:/opt/app/datacap/configure/application.properties !!! warning \"注意\" 需要同时下载一下多个文件： datacap.sql application.properties 下载完成后将他们放置到指定目录，也就是 ./configure/docker/ 和 ./configure/schema/ 如果需要自定义目录，可以修改 docker-compose.yml 文件中挂载的 volumes 配置即可。 !!! --- 以上工作完成后，使用以下命令进行启动服务。必须在包含 docker-compose.yml 文件的目录下执行 bash docker-compose up 如果需要后台启动使用以下命令 bash docker-compose up -d 启动成功后，浏览器打开 http://localhost:9096/ 即可看到网站。 --- 停止服务需要使用以下命令 bash docker-compose down",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/installFromDockerCompose.html",
    "lang": "zh-CN"
  },
  {
    "title": "Rainbond 部署",
    "content": "如果您不熟悉 Kubernetes，想在 Kubernetes 中安装 DataCap，可以使用 Rainbond 来部署。Rainbond 是一个基于 Kubernetes 构建的云原生应用管理平台，可以很简单的将你的应用部署到 Kubernetes 中。 安装 Rainbond, 请参阅 Rainbond 快速安装. DataCap 已发布到 Rainbond 开源应用商店，可通过 Rainbond 开源应用商店一键部署 DataCap。 进入 Rainbond 控制台的 平台管理 -> 应用市场 -> 开源应用商店 中搜索 DataCap 并安装。 ![](https://static.goodrain.com/wechat/datacap/1.png) 填写以下信息，然后点击确认按钮进行安装。 团队：选择现有团队或创建新的团队 集群：选择对应的集群 应用：选择现有应用或创建新的应用 版本：选择要安装的版本 安装完成后，可通过 Rainbond 提供的默认域名访问 DataCap，默认用户密码 admin/12345678 ![](https://static.goodrain.com/wechat/datacap/topology.png)",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/installRainbond.html",
    "lang": "zh-CN"
  },
  {
    "title": "即席查询",
    "content": "软件安装完成后点击顶部的 查询 菜单，进入即席查询页面。 !!! info \"提示\" 即席查询需要添加一个数据源，如何添加数据源请前往 数据源管理 !!! !img.png --- 进入查询页面后，窗口大致如下 !img1.png 查询页面分为左右两部分： 左侧为数据源 + 数据源元数据（需要选择数据源后才会展示） 右侧为 SQL 编辑器 + 结果展示（需要查询成功后才会展示） 当我们选择数据源后，左侧会展示类似下面的数据源元数据： !img2.png 此时右侧编辑器上方的工具栏也可以使用，我们在 SQL 编辑器中输入如下语句 sql SELECT FROM datacap.datacapsourcequery 点击编辑器顶部的 运行 按钮，即可查询。当查询成功后，编辑器下方会展示本次查询结果，类似如下 !img3.png --- 当我们在编辑器中选择执行的 SQL 后，顶部工具栏中的 运行 将会变为 运行选择内容 !img4.png!img4.png --- 当我们在编辑器中输入 SQL 后，点击顶部工具栏中的 格式化 按钮，即可格式化我们输入的 SQL !img5.png --- 当我们在编辑器中输入 SQL 并执行后，点击顶部工具栏中的 取消 按钮，即可取消本次查询。 !!! danger \"注意\" 取消功能并不意味着实际查询结束，查询将继续在后台运行。只是本次查询将不会在接受后续返回的结果。 !!! --- 这是一个片段模块的快捷功能，可以将编辑器中执行成功后的 SQL 快速添加到片段中。后续也可以在编辑器中实现片段的自动填充。 点击按钮后，会在右侧展示如下窗口，填写完成后，保存即可。 !img6.png --- 当查询完成后，会在 片段 按钮右侧出现一个展示耗时的按钮，点击后可以查看本次查询的消耗时间详情 !img7.png --- 在 datacap 中接入了 ai 模型，需要用户配置相应的信息方可使用，AI 模型支持 解析 优化 修复问题（只有查询出现错误后，才会出现该功能） 这里我们不多展示，可以自己体验。 --- 在 AI 按钮右侧有一个用于输入数字的输入框，他主要用于输入自动添加 LIMIT 的总数量（需要启动该功能，目前为实验性功能） --- 在编辑器上方的右侧有个 !btn[创建编辑器](){bg-white border} 按钮，用于添加编辑器，点击后可以增加一个编辑窗口 !img8.png --- 当我们添加新的编辑器后，在编辑器名称后有个 :circle-x: 按钮，点击后可以关闭该编辑器 !img9.png 默认查询后渲染为普通查询表格，可以在表格的头部做排序，筛选等操作。 !img10.png 当点击!btn[可视化]{bg-white border}按钮后的开关，会重新渲染下方表格，对表格中的数据进行分页的转换，当关闭分页后回会展示如下 !img11.png 会展示所有的数据不在进行分页。 目前只支持导出 CSV ，该操作并不会访问后端服务，点击后会导出当前查询的所有返回结果数据。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/query/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "仪表盘",
    "content": "软件安装完成后点击顶部的 仪表盘 菜单，进入仪表盘页面。 !img.png --- 点击页面左侧的 :circle-plus: 按钮弹出如下窗口 !img1.png 点击!btn[添加图表]{bg-white border}按钮，弹出数据报表窗口 !img2.png 选择需要添加的图表后点击 !btn[保存]{bg-blue-500 text-white} 按钮，出现类似如下页面 !img3.png 可以调整报表的位置大小等，然后点击右侧的 !btn[保存]{bg-blue-500 text-white} 按钮，弹出保存配置窗口 !img4.png 配置相关信息后，点击 !btn[保存]{bg-blue-500 text-white} 按钮，保存成功后将跳转到仪表盘预览页面。 !img5.png 这里会显示当前仪表盘的报表数据。 --- 在仪表盘列表页面，点击仪表盘的名称，可跳转到当前选择的仪表盘。 !img6.png !!! info \"提示\" 该操作将会跳转到新页面进行仪表盘的显示。 !!! --- 在仪表盘列表页面，点击 :cog: 图标，可出现操作列表 !img7.png 点击 !btn[修改仪表盘] 菜单，即可跳转到编辑仪表盘页面，该操作和新建仪表盘操作一致。 --- 在仪表盘列表页面，点击 :cog: 图标，可出现操作列表 点击 !btn[删除仪表盘] 菜单，即可弹出如下窗口 !img8.png 在输入框中输入仪表盘名称，点击删除即可删除该仪表盘。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/dashboard/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "新建 & 编辑",
    "content": "在查询页面进行查询后会在结果表格上方显示出如下窗口 !img.png 点击图中红框选择的按钮，跳转到新建数据集页面 !img1.png 该页面中我们可以修改要执行的 SQL 点击 !btn[执行] 按钮，会在下方出现预览数据的列表，点击顶部的 !btn[配置] 按钮，窗口如下 !img2.png 在页面中我们可以配置 数据列 和 基本信息，根据选择不同的 tab 进行相关配置。 !!! info \"提示\" 在数据集中列分为两种 虚拟列 和 真实列。 虚拟列 不会在实际的底层存储构建（只是用函数比较，只会在查询中实时展示），真实列 需要在底层存储中进行构建，这样在查询的时候会有更好的性能。 !!! 我们点击每行数据中的 操作 配置下的 :CirclePlus: 按钮，可以添加虚拟列 !img3.png 在虚拟列中部分配置无法使用，因为它不做具体的存储操作。 !!! danger \"警告\" 虚拟列 是 DataCap 中的一个新特性，该列只会在运行时生效，一般列名都是函数的表示，比如 SUM(id)，在查询的时候会将该 SQL 转换。 !!! 点击顶部的 数据配置 标签，配置项显示如下 !img4.png 我们完成基本的配置信息后点击顶部的 创建数据集 按钮，即可在后台构建数据集。届时会跳转到数据集列表。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/dataset/create/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "即席查询",
    "content": "在数据集列表中在每行数据的末尾有 操作 按钮，大概如下 !img.png 我们点击 :ChartNoAxesColumn: 图标后会跳转到 即席查询 页面 !img1.png 页面分为左右两侧，左侧是当前数据集的 指标 & 维度 配置，右侧是 查询 配置 当拖拽左侧的 指标 & 维度 时会在右侧显示查询结果 !img2.png 当查询列包含指标时，点击指标后面的 :cog:，弹出如下窗口 !img3.png 可以配置当前指标的 表达式，别名，排序。 !!! warning \"注意\" 不同的类型指标包含不同的表达式 !!! 当查询列包含维度时，点击 :cog:，弹出如下窗口 !img4.png 可以配置当前维度的 别名，排序，自定义函数。 当查询成功后，可以配置多种图表类型。 !img5.png 可以根据自己的需求定制目前已经支持的图表。 图表配置完成后，点击 发布 按钮，弹出如下窗口 !img6.png 配置图表的名称后点击 发布 按钮，图表发布成功后，可以在图表列表中查看。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/dataset/adhoc/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "同步数据",
    "content": "数据集提供了手动同步数据的操作，可以通过数据集列表中在每行数据的末尾有 同步数据 按钮，如下窗口 !img.png 点击 同步数据 按钮后会弹出 同步数据 窗口 !img1.png 确定同步数据后，点击 同步数据 按钮后，系统会在后台创建同步数据任务，进行数据同步。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/dataset/sync/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "同步历史",
    "content": "数据集提供了查看同步历史的操作，可以通过数据集列表中在每行数据的末尾有 同步历史 按钮，如下窗口 !img.png 点击 同步历史 按钮后会弹出 同步历史 窗口 !img1.png 在该窗口中会展示当前数据集的所有同步历史，包含手动同步和定时任务同步的历史记录。 如果任务同步失败会展示任务的错误信息，点击任务的状态按钮即可看到错误信息。 !img2.png 该操作只会在同步失败的状态下生效。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/dataset/history/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "清空数据",
    "content": "数据集提供了清空数据的操作，可以通过数据集列表中在每行数据的末尾有 清空数据 按钮，如下窗口 !img.png !!! info \"提示\" 该操作只会在当前数据集有数据的情况下可以操作。 !!! 当数据集无数据的情况下，该菜单将不可操作。 点击 清空数据 按钮后会弹出 清空数据 窗口 !img1.png 该窗口中会展示当前数据集的 总行数 和 总大小 确定清空数据后，点击 清空数据 按钮后，系统会在后台创建清空数据任务，进行清空数据。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/dataset/clear/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "新建 & 修改",
    "content": "!img.png 鼠标打开管理员菜单，点击 工作流 子菜单，跳转到工作流列表页面 !img1.png 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加工作流窗口 !img2.png 我们通过左侧拖拽需要的节点，拖拽后系统会自动进行配置的验证，默认验证失败会出现以下页面 !img3.png 如果验证成功，页面如下 !img4.png 配置完成后，左上角 发布 按钮可以点击，点击后弹出配置窗口 !img5.png 输入流程名称点击 提交 按钮即可完成工作流的创建，创建完成后跳转到工作流列表页面。 在工作流列表中，我们找到需要修改的工作流对应的数据行，点击操作栏的 设置 图标 !img6.png 点击修改流程选项，即可进入修改流程页面，操作方式和新建一致。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/create/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "查看工作流错误",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的叹号图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中会显示当前工作流的错误信息。 !!! warning 只有工作流在执行失败的情况下，才可以查看错误信息。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/error/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "停止工作流",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，按照提示输入，点击 停止流程 按钮，即可停止当前工作流的执行。 !!! warning 只有工作流在执行中的情况下，才可以停止工作流。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/stop/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "重启工作流",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，按照提示输入，点击 重新启动 按钮，即可重新启动当前工作流的执行。 !!! warning 只有工作流在非执行中的情况下，才可以重新启动工作流。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/restart/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "工作流日志",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，显示的是当前工作流的日志。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/logger/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "删除工作流",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，按照提示输入，点击 删除流程 按钮，即可删除流程。 !!! warning 只有工作流在非执行中的情况下，才可以删除工作流。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/delete/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "工作流视图",
    "content": "在工作流列表中，我们找到需要工作流对应的数据行，点击操作栏的 设置 图标 !img.png 点击后会弹出如下窗口 !img1.png 在这个窗口中，显示的是当前工作流的视图。",
    "url": "https://datacap.devlive.org/zh-CN/reference/get-started/workflow/dag/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "命令行界面",
    "content": "DataCap CLI 提供基于终端的交互式 shell 来运行查询。 CLI 是一个自动执行的 JAR 文件，这意味着它的行为就像普通的 UNIX 可执行文件一样。 --- CLI 需要路径上有可用的 Java 虚拟机。它可以与 Java 版本 8 及更高版本一起使用。 CLI 使用基于 HTTP/HTTPS 的 DataCap 客户端 REST API 与系统进行通信。 CLI 版本应与系统版本相同或更高。 --- 下载 datacap-client-cli-1.6.0.jar, 将其重命名为 datacap，使用 chmod +x 命令将其设置为可执行。 --- bash ./datacap connect -h 127.0.0.1 -p 9096 -u username -P password 如果成功，您将收到执行命令的提示。使用 help 命令查看支持的命令列表。 | 命令 | 描述 | |-------------------------------|---------------------| | source info | 获取数据源详细信息 | | source list | 获取远程服务器数据源列表 | | source use <SourceID> | 设置数据源标志，以便后续对数据源的操作 | | source execute \"<QuerySQL>\" | 执行远程SQL |",
    "url": "https://datacap.devlive.org/zh-CN/reference/clients/cli.html",
    "lang": "zh-CN"
  },
  {
    "title": "个人资料",
    "content": "!!! info \"提示\" 通过个人资料功能，可以对个人用户的一些查询历史和数据源的使用进行一些概览查看。 !!! 鼠标移向顶部菜单的最右侧的头像菜单，会弹出下拉框，点击下拉框中的第一个子菜单。弹出类似如下窗口： !img.png 首先我们看到的最上面的一个图表是近一年的查询日历图。它根据每天的查询总数汇总来计算。 第二个图表是近7日的数据源使用情况。它根据每天的数据源使用情况汇总来计算。 --- 当点击个人资料按钮后（也就是左侧菜单的第一个按钮），会弹出如下窗口： !img1.png 该页面主要展示了个人资料的基本信息。 --- 当点击登录日志按钮后（也就是左侧菜单的第二个按钮），会弹出如下窗口： !img2.png 该页面主要展示了当前用户的详细登录日志，包括了登录时间、登录地点、登录方式等。 --- 当点击账号设置按钮后（也就是左侧菜单的第三个按钮），会弹出如下窗口： !img3.png 该页面主要展示了一些用户可以进行的配置功能。 --- 当点击修改用户名按钮后，会弹出如下窗口： !img4.png 输入修改后的用户名和当前密码，点击确定按钮即可完成修改。 !!! danger \"警告\" 需要注意的是：修改用户名后，需要重新登录。系统会默认退出当前账号。 !!! --- 当点击修改密码按钮后，会弹出如下窗口： !img5.png 输入原密码和新密码，点击确定按钮即可完成修改。 !!! danger \"警告\" 需要注意的是：修改密码后，需要重新登录。系统会默认退出当前账号。 !!! --- 当点击 ChatGPT 按钮后，会弹出如下窗口： !img6.png 该页面主要展示了一些 ChatGPT 可以进行的配置功能。 --- 当点击编辑器按钮后，会弹出如下窗口： !img7.png 在修改编辑器时，修改的配置会同步到系统中的所有用到编辑器的位置。 编辑器的修改是所见即所得的状态，可以实时显示出来当前的配置方案。",
    "url": "https://datacap.devlive.org/zh-CN/reference/manager/profile/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "数据源",
    "content": "!!! info \"提示\" 通过数据源功能，可以添加对各种自定义数据源的支持，并执行后续的数据源操作等。 !!! 鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的第一个子菜单。弹出类似如下窗口，默认列表为空，需要自行添加。 !img.png 如果您添加了数据源会显示类似如下页面 !img1.png --- 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加数据源窗口 !img2.png 当我们选择某种类型的数据源时，数据源配置信息将显示在顶部标签栏中，不同的数据源有不同的配置项，它的配置单在服务启动是的指定目录中。 当我们选择类型为 MySQL 的源时，弹出类似以下窗口 !img3.png 在配置页面中出现了4个选项卡，点击不同的选项卡填充相关信息，然后点击底部的 Test 按钮，会弹出如下页面： !img4.png 当数据源测试成功后顶部会展示当前服务测试后的版本号，此时点击地步的 Save 按钮即可保存数据。 !!! info \"提示\" 数据源保存后，数据源列表会自动刷新。 !!! --- 点击列表中某个数据源中 Action 中的第一个按钮即可修改数据源，操作类似于 添加数据源 操作 --- 点击列表中某个数据源的 Action 中的第二个按钮，删除该数据源，点击后会弹出以下内容 !img5.png 单击弹出的小窗口，然后单击 OK 以删除选中的数据源。 !!! danger \"注意\" 需要注意的是，删除数据源后，与数据源相关的查询历史记录将被删除。 !!! --- 单击列表中数据源的 Action 中的第三个或者第四个按钮，跳转到数据源管理页面。 !img6.png 页面分为左右两部分。左侧主要展示数据源的基本信息，包括： 选中数据源的相关元数据 当我们在左侧选择数据库和数据表时，右侧的内容显示如下 !img7.png 在右侧内容中出现两个选项卡: Info（默认选项） Data !!! info \"提示\" 默认当前选项卡下显示关于当前表的相关信息。 !!! 点击 Data 选项卡，会出现类似如下页面，它展示了当前选中表的相关数据。 !img8.png 在顶部的四个按钮分别是： First Page Previous Page Next Page Last Page 接下来后面的按钮是用于设置数据查询的配置： !img9.png Jump to Page Show Page Size 填充配置后，点击 Apply 按钮即可应用当前配置信息。 在右侧还有一个按钮，点击后会展示当前查询使用到的详细 SQL 内容 !img10.png !!! info \"提示\" 当前 SQL 生成是根据同步到元数据的顺序而定。 !!! !!! danger \"注意\" 目前并不是所有的数据源都支持管理，如果需要可自行添加模版。如果有兴趣可将源码贡献给我们。 !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/manager/datasource/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "代码片段",
    "content": "!!! note 通过代码段功能，可以添加对各种自定义代码段的支持，并执行后续的代码段操作等。添加的代码片段后续会添加到编辑器中。 !!! 鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 Snippet 子菜单。弹出类似如下窗口，默认列表为空，需要自行添加。 !img.png 如果您添加了片段会显示类似如下页面 !img1.png --- 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加数据源窗口 !img2.png 在窗口中，我们需要输入以下内容 | 属性 | 描述 | |:-------------:|:----------------:| | Name | 标记当前代码段的名称 | | Description | 当前代码片段的说明 | | Snippet | 当前代码片段的特定 SQL 内容 | 填写完以上内容后，点击底部的 Submit 按钮保存代码片段。 !!! note 数据保存后，数据源列表会自动刷新。 !!! --- 点击列表中某个数据中 Action 中的第一个按钮，查看具体的代码片段内容，会弹出一个对话框，大致如下 !img3.png 单击 OK 或 Cancel 关闭对话框 --- 点击列表中某个数据中 Action 中的第二个按钮即可修改代码段，该操作类似于 添加片段 操作。 --- 点击列表中某个数据的 Action 中的第三个按钮，引用当前代码片段，会跳转到查询页面，片段内容会直接输入到编辑器中。 --- 点击列表中某个数据的 Action 中的第四个按钮，删除代码段，点击后会弹出以下内容 !img4.png 单击弹出的小窗口，然后单击 OK 以删除代码段。",
    "url": "https://datacap.devlive.org/zh-CN/reference/manager/snippet/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "查询历史",
    "content": "鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 History 子菜单。弹出类似如下窗口，默认列表为空，通过查询页面进行查询即可自动添加记录。 !img.png --- 点击列表中某个数据中 Action 中的第一个按钮，查看具体的代码片段内容，会弹出一个对话框，大致如下 !img1.png 此查询中查询的特定 SQL 语句将显示在窗口中。 --- !!! danger 只有查询出现异常情况下，才可以查看错误信息。 !!! 点击列表中某个数据中 Action 中的第二个按钮即可查看错误信息 !img2.png",
    "url": "https://datacap.devlive.org/zh-CN/reference/manager/history/query/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "流水线",
    "content": "在 DataCap 软件中流水线用于用户针对数据进行迁移等一些数据操作的工具。 --- 进入系统后，点击顶部 Admin 菜单下对应的 Pipeline 子菜单，默认进入流水线列表。类似下图： !img.png 点击列表右侧的 + Create 按钮，系统会弹出配置页面： !img1.png 配置页面分为三个配置模块，分别是： Description 主要配置用户执行的 SQL From 配置数据接入源 To 配置数据输出源 每个数据源具有不同的配置属性，当选择数据源后按照提示配置相关属性即可。 当任务发布成功后默认会刷新任务列表，如下图所示： !img2.png --- 任务发布后默认将会启动，在任务的右侧 Action 操作中包含以下功能： 当我们点击 Action 中的第一个按钮时，系统会弹出错误信息页面。 > 只有任务运行失败后该功能才会被启用 !img3.png 当我们点击 Action 中的第二个按钮时，系统会弹出日志页面。 !img4.png 当我们点击 Action 中的第三个按钮时，系统会弹出停止任务页面。 > 只有任务运行中该功能才会被启用 !img5.png 我们按照窗口提示的信息输入任务名字后点击 Stop 按钮即可。 当我们点击 Action 中的第四个按钮时，系统会弹出删除任务页面。 !img6.png 我们按照窗口提示的信息输入任务名字后点击 Delete 按钮即可。",
    "url": "https://datacap.devlive.org/zh-CN/reference/manager/pipeline/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "报表",
    "content": "鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 Report 子菜单。弹出类似如下窗口，默认列表为空，通过查询页面进行查询即可添加报表。 !img.png --- 新建报表功能在查询页面，当我们查询成功后会在结果表格上方显示出如下窗口 !img1.png 点击图中红框选择的按钮，弹出如下窗口 !img2.png 顶部为报表名称和是否实时报表 !!! note 如果是实时报表，那么报表数据会在展示时进行后端服务查询从而渲染，不同的查询引擎这可能会造成报表渲染慢的问题。非实时报表只会展示当前查询的静态数据。 !!! 左侧是报表渲染区域，右侧为报表的配置，目前支持报表的类型： 折线图 柱状图 以下是一个我们配置好的报表： !img3.png 报表配置完成后，点击发布即可将报表保留下来。 --- 在报表列表页面查询记录后有操作按钮，操作按钮的第一个 !img4.png{ width=\"20\" } 点击后可以查看报表 !img5.png --- 在报表列表页面查询记录后有操作按钮，操作按钮的第二个 !img6.png{ width=\"20\" } 点击后可以弹出如下窗口 !img7.png 在输入框中输入报表名称，点击删除即可删除该报表。",
    "url": "https://datacap.devlive.org/zh-CN/reference/manager/report/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Functions",
    "content": "!!! note We can enhance the automatic prompt function of the code editor through the function module provided by the system. !!! --- The system has built-in the following data source functions by default (it does not mean that it is the most complete function of the new version, if there is something missing, please submit issues or pr to fix it): ClickHouse MySQL Hive Trino & Presto (fit a part) --- After entering the system, click the corresponding Function submenu under the top Settings menu to go to the function configuration function !img.png Click the Add button on the top right to add a new function, and the following window will pop up after clicking: !img1.png The following is a detailed parameter description: Name: The name used to mark the function prompt, the suggestion is English Plugin: The plugin this function applies to, multiple options can be selected Content: The specific content of the function, which will be entered into the editor Description: Description of the function Type: Type of function, can be: KeyWord, Operator, Function, default is KeyWord Example: For the use example of this function, it is convenient for users to understand how to use the function When the above content is written, click the Submit button at the bottom to save the operation, and you can use it in the editor later. --- The system provides a way to import functions in batches. Currently, it supports the import of content and URI addresses. Next, let's take a look at how to do it. We perform the batch import function by clicking the import button on the top right. !img2.png The content import method allows us to enter a list of functions, and they are divided according to each line. Adding the following keywords we need to import: sql SHOW USE In Plugin, we choose to use the ClickHouse plug-in, and in Type, we choose KeyWord. After the operation is completed, we click the Submit button at the bottom to use the import function of the current input function. The URI import method is relatively simple. We can import data in batches by specifying the remote server URI address, which can be your local server address or the address provided by the software. The URI address format provided by the software by default is sql (http|https)://datacap.edurt.io/resources/functions/plugin/keywords.txt (http|https)://datacap.edurt.io/resources/functions/plugin/operators.txt (http|https)://datacap.edurt.io/resources/functions/plugin/functions.txt We only need to replace the value of plugin in the address with the name of the plugin that needs to be imported. !!! warning It should be noted that due to local network problems, the URI import method may be slow. !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/system/functions/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Sql Template",
    "content": "!!! note The system supports the SQL template function, through which the realization of some monitoring and other functions can be supported. !!! --- The system supports some default templates, currently supports: getAllDatabaseAndTable getAllDatabase getAllTablesFromDatabase Of course, each template can support one or more plug-ins, and they will be used in subsequent operations of the system. After entering the system, click the corresponding Sql submenu under the top Settings menu to go to the function configuration function !img.png Click the Add button on the top right to add a new function, and the following window will pop up after clicking: !img1.png The following is a detailed parameter description: Name: The name used to mark the function prompt, the suggestion is English Plugin: The plugin this function applies to, multiple options can be selected Description: Description of the function Template: The SQL statement executed by the template When the above content is written, click the Submit button at the bottom to save the operation, and you can use it in the editor later. !!! warning The default template does not carry any parameters and we can execute it directly. !!! We can realize the template dynamic parameter passing function by defining variables. Let's take an example, we need to display all the data tables under the default database, the normal SQL is sql SHOW TABLES FROM default When we use the template, the SQL changes to sql SHOW TABLES FROM ${database:String} The system parses the parameter into database=String by collecting {database:String} expression, where database is the parameter name, and String is the type of parameter passing. When we use the expression time, we only need to pass the Map type parameter, where key=parameter name, value=data value passed according to the type.",
    "url": "https://datacap.devlive.org/zh-CN/reference/system/sql/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "菜单",
    "content": "鼠标移向顶部菜单的 Admin 标识下，会弹出下拉框，点击下拉框中的 Menu 子菜单。弹出类似如下窗口，默认列表为空，通过查询页面进行查询即可自动添加记录。 !img.png --- 点击列表展示区域的右侧添加按钮（它是一个 + 图标），点击后将弹出如下添加数据源窗口 !img1.png 填写完以上内容后，点击底部的 Submit 按钮保存代码片段。 !!! note 数据保存后，数据源列表会自动刷新。 !!! --- 点击列表中某个数据中 Action 中的第一个按钮，会弹出修改窗口，它和新建菜单一致。",
    "url": "https://datacap.devlive.org/zh-CN/reference/system/menu/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "InfluxDB",
    "content": "InfluxDB 是一个由 InfluxData 开发的开源时序型数据库。它由 Go 写成，着力于高性能地查询与存储时序型数据。InfluxDB 被广泛应用于存储系统的监控数据。 --- DataCap 整合 InfluxDB 模块，用于实现对 InfluxDB 数据源的数据操作。 在该模块中我们使用的是 influxdb-jdbc 依赖，版本为 0.2.6。 使用到的驱动器为 net.suteren.jdbc.influxdb.InfluxDbDriver。 驱动源码可参考：https://github.com/konikvranik/jdbc-influxdb/ --- | 操作 | 是否支持 | |:--------:|:----:| | SELECT | ✅ | > 所有驱动器支持的操作均支持。 --- !!! note 如果需要使用该数据源，需要将 DataCap 服务升级到 >= 2024.03.7 !!! 支持时间: 2024-06-26 --- !!! note 如果您的服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"基本配置\" | 属性 | 是否必要 | 默认值 | 备注 | |---|---|---|---| | 名称 | ✅ | - |-| | 主机地址 | ✅ | 127.0.0.1 | - | | 端口 | ✅ | 8086 | - | === \"授权配置\" | 属性 | 是否必要 | 默认值 | |---|---|---| | 用户名 | ❌ | - | | 密码 | ❌ | - | === \"自定义\" > 可以添加所有 InfluxDB 驱动支持的配置，方式为 key = value 默认: | 属性 | 默认值 | |---|---| | database | default | :::",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/influxdb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Solr",
    "content": "Apache Solr 是基于 Apache Lucene™ 构建的快速开源搜索平台，提供可扩展的索引和搜索，以及分面、命中突出显示和高级分析/标记化功能。Solr 由 Apache 软件基金会管理。 --- DataCap 整合 Apache Solr 模块，用于实现对 Apache Solr 数据源的数据操作。 在该模块中我们使用的是 solr-solrj 依赖，版本为 6.0.0。 使用到的驱动器为 org.apache.solr.client.solrj.io.sql.DriverImpl。 --- | 操作 | 是否支持 | |:--------:|:----:| | SELECT | ✅ | > 所有驱动器支持的操作均支持。 --- !!! note 如果需要使用该数据源，需要将 DataCap 服务升级到 >= 2024.03.7 !!! 支持时间: 2024-06-25 --- !!! note 如果您的服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"基本配置\" | 属性 | 是否必要 | 默认值 | 备注 | |---|---|---|---| | 名称 | ✅ | - |-| | 主机地址 | ✅ | 127.0.0.1 | 这里的地址需要填写 zookeeper 的相关地址 | | 端口 | ✅ | 8983 | - | === \"授权配置\" | 属性 | 是否必要 | 默认值 | |---|---|---| | 用户名 | ❌ | - | | 密码 | ❌ | - | === \"高级配置\" | 属性 | 是否必要 | 默认值 | 备注 | |---|---|---|---| | 数据库 | ✅ | local | 这里是指在 solr 服务器中创建的 collection | === \"自定义\" > 可以添加所有 Solr 驱动支持的配置，方式为 key = value 暂无默认配置 :::",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/solr/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Timescale",
    "content": "Timescale 是一个用于时间序列、事件和分析的 PostgreSQL 数据平台。 它为您提供 PostgreSQL 的可靠性、TimescaleDB 的时间序列超能力以及完全托管服务的安心。 它提供自动备份和恢复、复制的高可用性、无缝扩展和调整大小等功能。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.02.1 !!! 支持时间: 2024-02-05 --- ::: tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |------|---------------------------------|-------------| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5432 | === \"高级\" | 属性 | 是否必要 | 默认值 | |----------|--------------------------------|-------------| | Database | :check{20,#3CA34F}: | | ::: --- | 功能 | 支持 | |:-----:|:----------------:| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :x: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] all !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/timescale/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "ParadeDB",
    "content": "ParadeDB 是基于 Postgres 构建的 Elasticsearch 替代方案。我们正在对 Elasticsearch 的功能进行现代化改造， 从实时搜索和分析开始。 ParadeDB 不是 Postgres 的分支，而是安装了自定义扩展的常规 Postgres。ParadeDB 本身 随 Postgres 16 一起提供。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.02.1 !!! 支持时间: 2024-02-05 --- :::tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |------|---------------------------------|-------------| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5432 | === \"高级\" | 属性 | 是否必要 | 默认值 | |----------|--------------------------------|-------------| | Database | :check{20,#3CA34F}: | | ::: --- | 功能 | 支持 | |-----|----------------| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :x: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues !!! [x] all",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/paradedb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "ScyllaDB",
    "content": "ScyllaDB 是一个分布式 NoSQL 宽列数据库，适用于需要高性能和低延迟的数据密集型应用程序。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.01.1 !!! 支持时间: 2024-01-12 --- ::: tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9042 | === \"高级\" | 属性 | 是否必要 | 默认值 | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | | ::: --- | 功能 | 支持 | |:-----:|:----------------:| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :x: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] all !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/scylladb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "MatrixOne",
    "content": "MatrixOne 是一款面向未来的超融合云和边缘原生 DBMS，通过简化的分布式数据库引擎，跨多个数据中心、云、边缘和其他异构基础设施支持事务、分析和流式工作负载。 --- !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 2024.01.1 !!! 支持时间: 2024-01-12 --- :::tabs === \"配置\" | 属性 | 是否必要 | 默认值 | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | - | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 6001 | === \"授权\" | 属性 | 是否必要 | 默认值 | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | - | | Password | :check{20,#3CA34F}: | - | | SSL | :x{20,#ff0000}: | false | === \"高级\" | 属性 | 是否必要 | 默认值 | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | | === \"自定义\" 暂无默认配置，支持用户自定义添加 ::: --- | 功能 | 支持 | |:-----:|:----------------:| | 即席查询 | :check: | | 数据集 | :check: | | 流水线 | :check: | | 元数据管理 | :x: | --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] 1.1.0 !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/matrixone/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "ClickHouse",
    "content": "ClickHouse® 是一个面向列的数据库管理系统（DBMS），用于在线分析查询处理（OLAP）。ClickHouse 的性能超过了所有其他面向列的数据库管理系统。它每台服务器每秒处理数十亿行和数十 GB 的数据。 !!! note 如果需要使用此数据源，则需要将 DataCap 服务升级到 >= 1.0.x !!! 支持时间: 2022-09-22 --- !!! note 如果您的 ClickHouse 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9000 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | === \"自定义\" !!! note 您可以通过添加 Key Value 来添加已经支持的 ClickHouse 参数，参数可以是 参考文档 !!! ::: --- !!! warning 在线服务尚未测试，如果您有详细的测试结果，请提交 issues [x] 19.x [x] 20.x [x] 21.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/clickhouse/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "MySQL",
    "content": "MySQL is one of the most recognizable technologies in the modern big data ecosystem. Often called the most popular database and currently enjoying widespread, effective use regardless of industry, it’s clear that anyone involved with enterprise data or general IT should at least aim for a basic familiarity of MySQL. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.0.x !!! Support Time: 2022-09-19 --- DataCap uses configuration files by default mysql.json !!! note If your MySQL service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 3306 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :check{20,#3CA34F}: | \\- | | SSL | :x{20,#ff0000}: | false | === \"Advanced\" | Field | Required | Default Value | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | default | === \"Custom\" !!! note You can add the already supported MySQL parameters by adding Key Value, the parameters can be reference document Default: | Key | value | |:-----------------------------:|:------:| | useOldAliasMetadataBehavior | true | !!! ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 5.x [x] 6.x [x] 7.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/mysql/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Redis",
    "content": "The open source, in-memory data store used by millions of developers as a database, cache, streaming engine, and message broker. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.3.x !!! Support Time: 2022-12-01 --- DataCap uses configuration files by default redis.json !!! note If your Redis service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 6379 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Password | :x{20,#ff0000}: | \\- | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 6.x [x] 7.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/redis/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "H2 Database",
    "content": "H2 is an embedded database developed in Java that is itself just a class library. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.8.x !!! Support Time: 2023-04-05 --- !!! note If your h2 service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | === \"Authorization\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"Advanced\" | Field | Required | Default Value | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] all !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/h2/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Snowflake",
    "content": "Execute your most critical workloads on top of Snowflake's multi-cluster shared data architecture in a fully managed platform that capitalizes on the near-infinite resources of the cloud. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.4.x !!! Support Time: 2023-01-29 --- DataCap uses configuration files by default snowflake.json !!! note If your Snowflake service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 80 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :check{20,#3CA34F}: | \\- | === \"Advanced\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | === \"Custom\" !!! note You can add the already supported Snowflake parameters by adding Key Value, the parameters can be reference document !!! ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/snowflake/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Yandex Database",
    "content": "YDB is a fault-tolerant distributed SQL DBMS. YDB provides high availability, horizontal scalability, strict consistency, and ACID transaction support. Queries are made using an SQL dialect (YQL). !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.4.x !!! Support Time: 2023-01-30 --- DataCap uses configuration files by default ydb.json !!! note If your YDB service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 2136 | === \"Authorization\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"Advanced\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | local | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 2.1.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/ydb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Zookeeper",
    "content": "ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.5.x !!! Support Time: 2023-02-07 --- This chapter describes the SQL syntax used in Zookeeper on DataCap. !!! note \"Synopsis\" sql SELECT [ | <Columns> ] selectexpression [, ...] FROM fromitem [. ...] where fromitem is one of sql tablename [ a.b | a.b | a.b] !!! !!! danger When tablename is set to all the root directory is searched. !!! !!! note \"Select expressions\" Each selectexpression must be in one of the following forms: sql expression [ columnalias ] sql In the case of expression [ columnalias ], a single output column is defined. In the case of , all columns of the relation defined by the query are included in the result set. sql -------- data !!! !!! danger If it is a multi-level directory, such as /zookeeper/id/2, it will be written \\zookeeper\\.\\id\\.\\2\\, and use . to split between directories. !!! --- DataCap uses configuration files by default zookeeper.json !!! note If your Zookeeper service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1:2181 | | Port | :x{20,#ff0000}: | 1 | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 3.1.x - 3.7.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/zookeeper/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "DuckDB",
    "content": "DuckDB is an in-process SQL OLAP database management system. !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.6.x !!! Support Time: 2023-02-20 --- DataCap uses configuration files by default duckdb.json !!! note If your DuckDB service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | /root | | Port | :check{20,#3CA34F}: | 0 | === \"Advanced\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | local | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 0.7.0 !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/duckdb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "AliYun OSS",
    "content": "Fully managed object storage service to store and access any amount of data from anywhere --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.6.x !!! Support Time: 2023-02-23 --- This chapter describes the SQL syntax used in Aliyun OSS on DataCap. !!! note \"Synopsis\" sql SELECT [ | <Columns> ] selectexpression [, ...] FROM fromitem [. ...] where fromitem is one of sql tablename [ a.b | a.b | a.b] !!! !!! danger When tablename is set to all the root directory is searched. !!! !!! note \"Select expressions\" Each selectexpression must be in one of the following forms: sql expression [ columnalias ] sql In the case of expression [ columnalias ], a single output column is defined. In the case of , all columns of the relation defined by the query are included in the result set. sql -------- data !!! !!! danger If it is a multi-level directory, such as /oss/id/2, it will be written \\oss\\.\\id\\.\\2\\, and use . to split between directories. !!! --- DataCap uses configuration files by default alioss.json !!! note If your Aliyun OSS service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------------------------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | https://oss-cn-regison.aliyuncs.com | === \"Authorization\" | Field | Required | Description | Default Value | |:----------:|:---------------------------------:|:-------------:|:-------------:| | Username | :check{20,#3CA34F}: | access Id | \\- | | Password | :check{20,#3CA34F}: | access Secret | \\- | === \"Advanced\" | Field | Required | Description | Default Value | |:----------:|:--------------------------------:|:-----------:|:-------------:| | Database | :check{20,#3CA34F}: | bucket name | default | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] all version !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/alioss/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Kafka",
    "content": "Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.7.x !!! Support Time: 2023-03-06 --- This chapter describes the SQL syntax used in DataCap Kafka plugin. SHOW TOPICS SHOW CONSUMERS --- DataCap uses configuration files by default kafka.json !!! note If your Apache Kafka service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:----------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | localhost:9092 | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 1.0.0 [x] 1.1.0 [x] 1.2.0 !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/kafka/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "SHOW TOPICS",
    "content": "--- sql SHOW TOPICS --- Returns a list of all defined topics in the current cluster. Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/zh-CN/reference/sql-syntax/connectors/kafka/show_topics.html",
    "lang": "zh-CN"
  },
  {
    "title": "SHOW CONSUMERS",
    "content": "--- sql SHOW CONSUMERS SHOW CONSUMERS FROM topic --- Returns a list of all defined consumers in the current cluster (if a topic is specified, a list of consumers for the specified topic will be returned). Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/zh-CN/reference/sql-syntax/connectors/kafka/show_consumers.html",
    "lang": "zh-CN"
  },
  {
    "title": "SHOW DATABASES",
    "content": "--- sql SHOW DATABASES --- Returns a list of all defined topics in the current cluster. Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/zh-CN/reference/sql-syntax/connectors/kafka/show_databases.html",
    "lang": "zh-CN"
  },
  {
    "title": "SHOW TABLES",
    "content": "--- sql SHOW TABLES SHOW TABLES FROM database > database kafka topic name --- Returns a list of all defined consumers in the current cluster (if a topic is specified, a list of consumers for the specified topic will be returned). Returns NULL for any information that is not populated or unavailable on the data source. The list data is returned as a row of each column, and the default header is . | Column | Description | Notes | |--------|------------------------|---------------------------------| | | The name of the column | NULL in the table summary row |",
    "url": "https://datacap.devlive.org/zh-CN/reference/sql-syntax/connectors/kafka/show_tables.html",
    "lang": "zh-CN"
  },
  {
    "title": "CeresDB",
    "content": "CeresDB 是一款高性能、分布式的云原生时序数据库。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-12 --- !!! note 如果您的CeresDB服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5440 | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 1.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/ceresdb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "GreptimeDB",
    "content": "一个开源、云原生、分布式时间序列数据库，支持PromQL/SQL/Python。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-14 --- !!! note 如果您的 GreptimeDB 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 5440 | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 0.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/greptimedb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "QuestDB",
    "content": "QuestDB 是一个开源时间序列数据库，用于高吞吐量摄取和快速 SQL 查询，操作简单。它支持使用 InfluxDB 行协议、PostgreSQL 有线协议和用于批量导入和导出的 REST API 进行与模式无关的摄取。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-17 --- !!! note 如果您的 QuestDB 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9000 | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 7.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/questdb/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Doris",
    "content": "一个易于使用、高性能和统一的分析数据库 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-19 --- !!! note 如果您的 Doris 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9093 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/doris/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "StarRocks",
    "content": "开源、高性能的分析数据库 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-20 --- !!! note 如果您的 StarRocks 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9030 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 2.2.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/starrocks/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Hologres",
    "content": "Hologres是兼容PostgreSQL的一站式实时数据仓库引擎，支持PB级数据多维分析（OLAP）与即席分析（Ad Hoc），支持高并发低延迟的在线数据服务（Serving）。与MaxCompute、Flink、DataWorks深度融合，提供离在线一体化全栈数仓解决方案。 !!! note 如果你需要使用这个数据源, 您需要将 DataCap 服务升级到 >= 1.9.x !!! 支持时间: 2023-04-25 --- !!! note 如果您的 Hologres 服务版本需要其他特殊配置，请参考修改配置文件并重启 DataCap 服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | hologres-cn-regison.aliyuncs.com | | Port | :check{20,#3CA34F}: | 80 | === \"授权\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Username | :x{20,#ff0000}: | \\- | | Password | :x{20,#ff0000}: | \\- | === \"高级\" | Field | Required | Default Value | |:----------:|:-----------------------:|:-------------:| | Database | :x{20,#ff0000}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] all !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/hologres/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Hadoop HDFS",
    "content": "The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.9.x !!! Support Time: 2023-04-27 --- !!! note If your HDFS service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------------------------------:| | Name | :check{20,#3CA34F}: | \\- | === \"Advanced\" | Field | Required | Description | Default Value | |:----------:|:--------------------------------:|:-----------:|:-------------:| | file | :check{20,#3CA34F}: | core-site.xml <br /> hdfs-site.xml | [] | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 3.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/hdfs/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Pinot",
    "content": "Apache Pinot 是一个实时分布式 OLAP 数据存储，专为低延迟、高吞吐量分析而构建，非常适合面向用户的分析工作负载。 --- !!! note 如果需要使用该数据源，需要将DataCap服务升级到 >= 1.10.x !!! 支持时间: 2023-05-06 --- !!! note 如果您的服务版本需要其他特殊配置，请参考修改配置文件并重启DataCap服务。 !!! ::: tabs === \"配置\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9000 | === \"授权\" | Field | Required | Default Value | |:----------:|:---------------------------------:|:-------------:| | Username | :check{20,#3CA34F}: | \\- | | Password | :check{20,#3CA34F}: | \\- | ::: --- !!! warning 服务版本尚未测试，如果您有详细的测试并发现错误，请提交 issues [x] 0.8.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/pinot/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Apache Cassandra",
    "content": "Apache Cassandra® powers mission-critical deployments with improved performance and unparalleled levels of scale in the cloud. --- !!! note If you need to use this data source, you need to upgrade the DataCap service to >= 1.11.x !!! Support Time: 2023-06-07 --- !!! note If your plugin service version requires other special configurations, please refer to modifying the configuration file and restarting the DataCap service. !!! ::: tabs === \"Configure\" | Field | Required | Default Value | |:------:|:---------------------------------:|:-------------:| | Name | :check{20,#3CA34F}: | \\- | | Host | :check{20,#3CA34F}: | 127.0.0.1 | | Port | :check{20,#3CA34F}: | 9042 | === \"Advanced\" | Field | Required | Default Value | |:----------:|:--------------------------------:|:-------------:| | Database | :check{20,#3CA34F}: | datacenter | ::: --- !!! warning The online service has not been tested yet, if you have detailed test results, please submit issues to us [x] 0.4.x !!!",
    "url": "https://datacap.devlive.org/zh-CN/reference/connectors/cassandra/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Local (本地)",
    "content": "在 DataCap 中目前已经支持 Local，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=Local datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 Local datacap.fs.access: 不做任何设置 datacap.fs.secret: 不做任何设置 datacap.fs.endpoint: 不做任何设置 datacap.fs.bucket: 如果设置则为目录的名称，不设置则不做任何设置 --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-local</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： java package io.edurt.datacap.fs; import com.google.inject.Guice; import com.google.inject.Injector; import com.google.inject.Key; import com.google.inject.TypeLiteral; import lombok.extern.slf4j.Slf4j; import org.junit.Assert; import org.junit.Before; import org.junit.Test; import java.io.BufferedReader; import java.io.File; import java.io.IOException; import java.io.InputStreamReader; import java.nio.charset.StandardCharsets; import java.util.Optional; import java.util.Set; @Slf4j public class LocalFsTest { private Injector injector; private FsRequest request; @Before public void before() { injector = Guice.createInjector(new FsManager()); request = FsRequest.builder() .access(null) .secret(null) .endpoint(String.join(File.separator, System.getProperty(\"user.dir\"), \"data\")) .bucket(\"tmp\") .localPath(String.join(File.separator, System.getProperty(\"user.dir\"), \"src/main/java/io/edurt/datacap/fs/LocalFs.java\")) .fileName(\"LocalFs.java\") .build(); } @Test public void test() { Set<Fs> sets = injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})); Assert.assertEquals(\"Local\", sets.stream().findFirst().get().name()); } @Test public void writer() { Optional<Fs> optional = injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})).stream().findFirst(); if (optional.isPresent()) { Assert.assertEquals(\"Local\", optional.get().name()); } FsResponse response = optional.get().writer(request); Assert.assertEquals(true, response.isSuccessful()); } @Test public void reader() { Optional<Fs> optional = injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})).stream().findFirst(); if (optional.isPresent()) { Assert.assertEquals(\"Local\", optional.get().name()); } FsResponse response = optional.get().reader(request); Assert.assertEquals(true, response.isSuccessful()); log.info(\"====== [ {} ] ======\", response.getRemote()); try (BufferedReader reader = new BufferedReader(new InputStreamReader(response.getContext(), StandardCharsets.UTF8))) { String line; while ((line = reader.readLine()) != null) { log.info(line); } } catch (IOException e) { log.error(\"Reader error\", e); } } @Test public void testDelete() { injector.getInstance(Key.get(new TypeLiteral<Set<Fs>>() {})) .stream() .findFirst() .ifPresent(it -> { FsResponse response = it.delete(request); Assert.assertTrue(response.isSuccessful()); }); } }",
    "url": "https://datacap.devlive.org/zh-CN/reference/filesystem/local/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "阿里云 OSS",
    "content": "在 DataCap 中目前已经支持阿里云 OSS，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=AliOss datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 AliOss datacap.fs.access: 阿里云 OSS 的 AccessKey datacap.fs.secret: 阿里云 OSS 的 SecretKey datacap.fs.endpoint: 阿里云 OSS 的 Endpoint，如 oss-cn-hangzhou.aliyuncs.com datacap.fs.bucket: 阿里云 OSS 的 Bucket，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-alioss</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.alioss import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class AliOssFsTest { private val log = LoggerFactory.getLogger(AliOssFsTest::class.java) private val name = \"AliOss\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"IOUtilsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/alioss/IOUtilsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun testDelete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/zh-CN/reference/filesystem/aliyun/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "七牛云存储",
    "content": "在 DataCap 中目前已经支持七牛云，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=Qiniu datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 Qiniu datacap.fs.access: 七牛云的 AccessKey datacap.fs.secret: 七牛云的 SecretKey datacap.fs.endpoint: 七牛云的 Endpoint datacap.fs.bucket: 七牛云的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-qiniu</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.qiniu import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets import kotlin.test.assertTrue class QiniuFsTest { private val log = LoggerFactory.getLogger(QiniuFsTest::class.java) private val name = \"Qiniu\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"IOUtilsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/qiniu/IOUtilsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun testDelete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/zh-CN/reference/filesystem/qiniu/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "腾讯云 COS",
    "content": "在 DataCap 中目前已经支持腾讯云 COS，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=TencentCos datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 TencentCos datacap.fs.access: 腾讯云的 AccessKey datacap.fs.secret: 腾讯云的 SecretKey datacap.fs.endpoint: 腾讯云的 Endpoint datacap.fs.bucket: 腾讯云的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-tencent-cos</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.cos import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class TencentCosFsTest { private val log = LoggerFactory.getLogger(TencentCosFsTest::class.java) private val name = \"TencentCos\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"TencentCosFsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/cos/TencentCosFsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun delete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/zh-CN/reference/filesystem/cos/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "亚马逊云 S3",
    "content": "在 DataCap 中目前已经支持亚马逊云 S3，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=AmazonS3 datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 AmazonS3 datacap.fs.access: 亚马逊云的 AccessKey datacap.fs.secret: 亚马逊云的 SecretKey datacap.fs.endpoint: 亚马逊云的 Endpoint，如 cn-north-1，不需要输入整个的地址 datacap.fs.bucket: 亚马逊云的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-amazon-s3</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.s3 import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class AmazonS3FsTest { private val log = LoggerFactory.getLogger(AmazonS3FsTest::class.java) private val name = \"AmazonS3\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = \"TencentCosFsTest.kt\" request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/s3/AmazonS3FsTest.kt\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun delete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/zh-CN/reference/filesystem/s3/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "MinIO",
    "content": "在 DataCap 中目前已经支持 MinIO，支持功能如下： 上传文件 预览&下载文件 删除文件 --- 在 DataCap 中文件系统用于配置系统的缓冲以及临时文件的存储。修改 application.properties 文件如下内容： properties datacap.fs.type=MinIO datacap.fs.access= datacap.fs.secret= datacap.fs.endpoint= datacap.fs.bucket= datacap.fs.type: 文件系统的类型只能是 MinIO datacap.fs.access: MinIO 的 AccessKey datacap.fs.secret: MinIO 的 SecretKey datacap.fs.endpoint: MinIO 的 Endpoint，如 http://localhost:9000 datacap.fs.bucket: MinIO 的存储桶名称，如 datacap --- 该插件支持第三方依赖方式引入，引入依赖如下： xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-minio</artifactId> <version>${VERSION}</version> </dependency> 使用代码如下： kotlin package io.edurt.datacap.fs.minio import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.fs.FsService import io.edurt.datacap.fs.FsManager import io.edurt.datacap.fs.FsRequest import org.junit.Assert.assertTrue import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory.getLogger import java.io.BufferedReader import java.io.FileInputStream import java.io.IOException import java.io.InputStreamReader import java.nio.charset.StandardCharsets class MinIOFsTest { private val log = getLogger(this::class.java) private val name = \"MinIO\" private val fileName = \"MinIOUtilsTest.kt\" private var request = FsRequest() private var injector: Injector? = null @Before fun before() { request.access = System.getProperty(\"access\") request.secret = System.getProperty(\"secret\") request.bucket = System.getProperty(\"bucket\") request.fileName = fileName request.endpoint = System.getProperty(\"endpoint\") injector = Guice.createInjector(FsManager()) } @Test fun writer() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val stream = FileInputStream(\"src/test/kotlin/io/edurt/datacap/fs/minio/${fileName}\") request.stream = stream val response = plugin !!.writer(request) assertTrue(response.isSuccessful) } @Test fun reader() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.reader(request) assertTrue(response.isSuccessful) try { BufferedReader(InputStreamReader(response.context, StandardCharsets.UTF8)).use { reader -> var line: String? while ((reader.readLine().also { line = it }) != null) { log.info(line) } } } catch (e: IOException) { log.error(\"Reader error\", e) } } @Test fun delete() { val plugins: Set<Fs?>? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Fs?>?>() {})) val plugin: Fs? = plugins?.first { v -> v?.name().equals(name) } val response = plugin !!.delete(request) assertTrue(response.isSuccessful) } }",
    "url": "https://datacap.devlive.org/zh-CN/reference/filesystem/minio/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "latest",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:----------:|:------------:| | 2024.4.1 | 2024-12-31 | 尊敬的 DataCap 用户： DataCap 2024.4.1 版本现已正式发布。本次更新包含多项重要功能升级和性能优化，现将主要更新内容公布如下： 数据库功能增强 (实现功能) 新增数据库管理功能：支持创建、删除和切换数据库 完善表管理功能：支持创建表、删除表、插入数据等操作 MongoDB 驱动全面升级 优化查询性能：将 find 替换为 aggregate 新增元数据支持 支持 com.dbschema.MongoJdbcDriver 适配 完善版本控制和索引获取功能 工作流引擎优化 新增工作流任务提交功能 支持工作流重启操作 优化 SeatTunnel 执行器，支持自定义节点类型 SQL 解析器增强 优化 G4 表达式结构 新增 SHOW 语句支持 完善 SELECT 语句格式化功能 支持 CREATE DATABASE 语法 修复查询历史记录创建问题 修复 LocalDateTime 类型 JSON 转换问题 修复历史数据获取失败问题 优化达梦数据库插件版本获取 完善 Windows 平台支持 优化发布脚本 优化 CI/CD 流程 GitHub：https://github.com/devlive-community/datacap 官方网站：https://datacap.devlive.org/ Docker：已更新最新镜像 感谢社区用户一直以来的支持与反馈。如有问题或建议，欢迎通过 GitHub Issues 与我们交流。",
    "url": "https://datacap.devlive.org/zh-CN/release/latest.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.04.0",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:----------:|:------------:| | 2024.4.0 | 2024-12-02 | !!! note 本版本是一个全新的版本，完全使用了新的插件管理系统，新的 API 等各种新特性。本次更新为全新的版本，不兼容之前的版本。升级前要做好数据备份，以免数据丢失。数据库是兼容的，只要执行升级的 SQL 就可以了。<br /> 还需要注意的是升级版本后，要修改 datacapuser 和 datacaprole 表的 code 字段的值每条数据唯一即可，否则会导致无法登录。(如果是全新安装可忽略这个步骤)<br /> 执行以下 SQL 语句升级数据库:<br /> sql INSERT INTO datacapmenu VALUES (18,'全局 - 商店','STORE','','/store','',3,'VIEW',0,1,'common.store','Store',NULL,'2024-11-05 21:18:28',0,0,NULL); INSERT INTO datacaprolemenurelation VALUES ('1','18') --- 修复未登录导致国际化获取失败的问题 拆分插件系统为全新的模块，支持更好的插件管理 支持插件的在线安装，卸载等操作 增加通用测试模块 增加全新的插件商店 修复数据集保存时出现异常的问题 使用全新的 JsonView 注解，支持更好的数据安全及显示 --- 增加 Open API 文档",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.4.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.11",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.3.11 | 2024-11-17 | --- 修复部分 API 返回多余字段 添加清除 UI 依赖脚本 替换 yarn 为 pnpm 修复了仅分配 Submenu 时缺少 Parent Menu 的问题 --- 替换整体 UI 为 view-shadcn-ui",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.3.11.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.10",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.3.10 | 2024-10-09 | --- 移除示例地址 修复登录页面上的验证码问题 修复注册页面上的验证码问题 修复验证码验证规则 修复文档指定 Java 版本 --- 支持 MinIO --- Bump dep.antlr4.version from 4.13.1 to 4.13.2 Bump ag-grid-community and ag-grid-vue3 Bump commons-io:commons-io from 2.11.0 to 2.14.0 Bump com.clickhouse:clickhouse-jdbc from 0.4.6 to 0.6.5 Bump vite from 5.2.8 to 5.4.8 Bump rollup from 4.14.2 to 4.22.4 Bump micromatch from 4.0.5 to 4.0.8 Bump axios from 1.6.8 to 1.7.4 Bump org.yaml:snakeyaml from 2.2 to 2.3 Bump com.github.vertical-blank:sql-formatter from 2.0.3 to 2.0.5 Bump org.apache.maven.plugins:maven-javadoc-plugin from 3.8.0 to 3.10.0 Bump com.amazonaws:aws-java-sdk-s3 from 1.12.770 to 1.12.771 Bump org.testcontainers:oracle-xe from 1.18.1 to 1.20.1 Bump com.github.eirslett:frontend-maven-plugin from 1.13.4 to 1.15.0 Bump kotlin.version from 1.9.10 to 2.0.20 Bump org.duckdb:duckdbjdbc from 0.9.2 to 1.0.0 Bump com.taosdata.jdbc:taos-jdbcdriver from 3.2.7 to 3.3.0 Bump com.microsoft.sqlserver:mssql-jdbc from 12.6.0.jre8 to 12.8.1.jre8 Bump avatica.version from 1.22.0 to 1.25.0 Bump org.apache.iotdb:iotdb-jdbc from 1.1.0 to 1.3.2",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.3.10.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.9",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.9 | 2024-08-27 | --- 修复 maven 发布 CI 修复包缺少文件导致启动失败的问题 修复构建数据集不携带查询 SQL 的问题 修复数据集未携带 injector 导致构建失败的问题 --- 支持腾讯 COS 支持 Amazon S3 --- 升级 UI braces 3.0.2 到 3.0.3 升级 org.apache.kyuubi:kyuubi-hive-jdbc-shaded 1.7.1 到 1.9.2 升级 org.apache.maven.plugins:maven-compiler-plugin 3.12.1 到 3.13.0 升级 testcontainers.version 1.19.5 到 1.20.1 升级 org.apache.maven.plugins:maven-javadoc-plugin 3.6.3 到 3.8.0 升级 jackson.version 2.17.0 到 2.17.2 升级 org.elasticsearch.plugin:x-pack-sql-jdbc 8.12.0 到 8.15.0 升级 org.apache.commons:commons-csv 1.10.0 到 1.11.0 升级 monetdb:monetdb-jdbc 3.2 到 11.19.15",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.9.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.8",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.8 | 2024-07-29 | --- 优化核心打包方式 --- 优化打包方式为按需加载 --- 优化打包方式为按需加载 --- 优化打包方式为按需加载 --- 升级 net.snowflake:snowflake-jdbc 3.14.4 到 3.17.0 升级 com.dameng:DmJdbcDriver18 8.1.3.62 到 8.1.3.140 升级 hadoop.version 3.3.4 到 3.4.0 升级 org.apache.pinot:pinot-jdbc-client 0.8.0 到 1.1.0 升级 org.apache.phoenix:phoenix-core 4.16.0 到 5.2.0 升级 jackson.version 2.16.1 到 2.17.0",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.8.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.7",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.7 | 2024-07-02 | --- 重新布局 README.md 连接器列表 支持 Banner 显示应用信息 --- 支持 Apache Solr 支持 InfluxDB 重命名 ScyllaDB 包名 修复 SPI 异常连接未被拦截的问题 --- 重命名 file 为 convert 支持 xml 转换器 --- 升级 io.trino:trino-jdbc 414 至 450",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.7.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.6",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.6 | 2024-06-19 | --- 替换 findbugs 为 spotbugs 修复 JwtResponse 返回问题 适配数据源元数据 适配元数据转换器 --- 添加通知器文档 添加转换器文档 --- 支持 Json 支持 Txt 支持 Csv 支持 None",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.6.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.5",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.5 | 2024-06-05 | --- [Docker] 修复 JDK 版本 [User] 修复用户头像重复上传问题 [Role] 修复设置多个权限会导致菜单显示重复的问题 [Query] 修复并发访问导致无法记录查询历史记录的问题 [Dashboard] 支持上传缩略图 [Dashboard] 添加本地存储路径 [User] 修复用户登录注册加载状态 [User] 支持后台添加用户 [User] 优化用户操作函数 --- 替换 npm 为 yarn --- 添加 SPI 支持钉钉功能 移除 antlr4 代码",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.5.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.4",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.4 | 2024-05-19 | --- 移除 stale ci 修复聊天对话框文本过多导致样式混乱的问题 添加数据库、表、列补全符 修复了单词未设置为编辑器的问题 更新 openai-java-sdk 版本 支持显示查询错误信息 支持即席查询可视化 --- 修复查看SQL时使用ESC导致页面异常的问题 --- 修复虚拟列无法构建 修复临时查询不传递列类型的问题 支持根据表达式生成别名 更换头像 添加玫瑰图 --- 支持多数据源 --- 支持 aliyun oss 将 fs 存储实验功能标记为正式功能",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.4.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.3",
    "content": "DataCap 发布！ | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.3 | 2024-04-19 | --- [用户] 支持上传头像 [数据源] 删除 v1 saveAndUpdate、getInfo fc [数据源] 替换 getSource 函数 [数据源] 添加代码和名称栏 [数据源] 重构元数据布局 [数据源] 将数据库信息添加到元数据中 [函数] 修复info中未定义属性转换时的错误 [查询] 删除旧的 api [查询] 修复历史数据为空的问题 [片段] 适配器基础模块 [流水线] 适配器基础模块 [仪表板] 优化仪表板获取方式 [仪表板] 添加描述 [仪表板] 添加默认图像 删除配置目录下的 schema 启用统一的 BaseEntity、BaseRepository、BaseController、BaseService 支持 saveOrUpdate 方法根据属性设置相关值 添加代码权限信息 --- [布局] 优化菜单选择和高亮显示 [布局] 支持父菜单选择高亮显示 [布局] 添加404页面回调 [布局] 添加403页面重定向 [布局] 添加重定向到网络页面 [用户] 修复加载状态 添加页面加载进度 删除 echarts 合并 i18n --- 修复数据集构建中缺失列的问题 固定信息路径 优化查询列传输数据过大的问题 添加获取列的权限 突出显示指标维度容器 将 id 替换为代码 [折线图] 添加标题 删除未使用的组件 将配置合并到编辑器中 修复配置不回显的问题 删除未使用的配置组件 禁用未查询可视化功能 添加散点图 添加雷达图 添加漏斗图 添加仪表图表 --- 替换 schema 路径 修复 docker compose 镜像并更改示例地址",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.3.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.2",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.2 | 2024-04-12 | > 紧急发布版本：由于发布前测试时，调用 API 失误，导致构建数据集功能无法使用，该版本主要用于修改构建数据集功能 --- 修复 logo 路径引用错误问题 --- 修复数据集无法构建问题 --- 修复初始化脚本丢失字段问题",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.2.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.03.1",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.03.1 | 2024-04-11 | > 本次版本更新，我们主要针对 UI 进行了重构 --- 修复依赖包导入错误 [报表] 修复发布报表后跳转到报表列表的问题 [仪表盘] 支持显示报表名称 [仪表盘] 修复报表右下边框缺失的问题 [仪表盘] 修复修改跳转时报错的问题 --- 重构 UI 为 shadcn ui --- 移除旧版构建功能 支持指定启动脚本 --- 修复由于未配置生命周期列导致建表失败的问题 构建失败禁止即席查询 将即席查询分组合并到维度中 修复计算总行数和总大小 --- 支持删除功能 --- [Seatunnel] 支持 flink 引擎 [Seatunnel] 支持 seatunnel 引擎 --- 更新 testcontainers.version 1.17.6 到 1.19.5 更新 com.datastax.oss:java-driver-core 4.16.0 到 4.17.0",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.03.1.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.02.1",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:-----------:|:------------:| | 2024.02.1 | 2024-03-01 | --- 支持 ParadeDB (https://github.com/devlive-community/datacap/issues/589) 支持 TimescaleDB (https://github.com/devlive-community/datacap/issues/593) --- [图表] 添加面积图 (https://github.com/devlive-community/datacap/issues/611) [图表] 添加多面积图 (https://github.com/devlive-community/datacap/issues/610) [图表] 添加饼图 (https://github.com/devlive-community/datacap/issues/613) [图表] 支持折线图断点处理 [图表] 支持分组直方图 (https://github.com/devlive-community/datacap/issues/612) [图表] 支持词云图 (https://github.com/devlive-community/datacap/issues/614) 支持添加主键 (https://github.com/devlive-community/datacap/issues/624) 添加总行数和总大小 (https://github.com/devlive-community/datacap/pull/676) 支持自定义别名的即席查询 (https://github.com/devlive-community/datacap/issues/615) 支持维度排序 支持指标排序 支持数据采样列 (https://github.com/devlive-community/datacap/issues/605) 添加有符号号码类型 修复了小屏幕上列表中的显示问题 支持即席查询自定义函数 (https://github.com/devlive-community/datacap/issues/603) 添加筛选器分组 合并查询列 修复复制数据导致重复请求的问题 修复编辑报表分组异常问题 修复无限循环查询问题 支持过滤器 IS NULL， IS NOT NULL 修复了多列查询导致 expres 混淆的问题 支持过滤器 LIKE， NOT LIKE 支持过滤器 =， <> 支持过滤器 >， >= 支持过滤器 <， <= 支持历史错误信息同步 支持清空数据 (https://github.com/devlive-community/datacap/issues/622) 支持动态添加列 (https://github.com/devlive-community/datacap/issues/623) 重构数据集详细信息页面 添加日期时间类型 支持自定义生命周期 (https://github.com/devlive-community/datacap/issues/616) 禁用 BOOLEAN、DATETIME 类型分配 支持虚拟列 (https://github.com/devlive-community/datacap/issues/602) 支持docker compose (https://github.com/devlive-community/datacap/issues/648) 支持七牛云存储 (https://github.com/devlive-community/datacap/issues/618) --- 更新 io.crate:crate-jdbc 2.6.0 到 2.7.0 更新 Impala:ImpalaJDBC42 2.6.29.1035 到 2.6.32.1041 更新 org.yaml:snakeyaml 2.0 到 2.2 更新 org.apache.hive:hive-jdbc 3.1.2 到 3.1.3 更新 org.elasticsearch.plugin:x-pack-sql-jdbc 8.11.3 到 8.12.0 更新 jackson.version 2.16.0 到 2.16.1 更新 org.apache.maven.plugins:maven-javadoc-plugin 3.6.0 到 3.6.3 更新 com.microsoft.sqlserver:mssql-jdbc 11.2.1.jre8 到 12.6.0.jre8 更新 postgresql.version 42.6.0 到 42.7.2",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.02.1.html",
    "lang": "zh-CN"
  },
  {
    "title": "2024.01.1",
    "content": "DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 2024.01.1 | 2024-01-30 | --- [仪表盘] 支持移除报表 [仪表盘] 支持预览 [仪表盘] 支持修改 [元数据] 支持 PostgreSQL 数据库 [元数据] 支持同步错误消息 [元数据] 支持构建后元数据同步 (https://github.com/devlive-community/datacap/issues/585) [元数据] 支持数据源手动同步元数据 (https://github.com/devlive-community/datacap/issues/586) [元数据] 支持数据源同步历史记录 添加查询模式 修复了函数创建/更新时间问题 优化问题模板 支持 GitHub CI 本地化 添加系统架构图 (https://github.com/devlive-community/datacap/issues/491) 支持缓冲查询结果 (https://github.com/devlive-community/datacap/issues/490) --- 修复数据源并发异常问题 (https://github.com/devlive-community/datacap/issues/513) --- 添加 SQL 解析器 SPI 添加 Trino SQL 解析器 (https://github.com/devlive-community/datacap/issues/569) 添加 MySQL SQL 解析器 (https://github.com/devlive-community/datacap/issues/568) --- 添加调度 SPI 支持默认调度器 --- 修复流水线分组问题 支持 SqlServer 重构 Seatunnel 执行器 重构 SPI 使用 Kotlin 重写 重构流水线配置方法 --- 支持 MatrixOne (https://github.com/devlive-community/datacap/issues/574) 支持 ScyllaDB (https://github.com/devlive-community/datacap/issues/543) --- 添加列模型（指标｜维度） 添加即席查询可视化编辑器 添加折线图 添加数据表格 添加柱状图 支持展示查询 SQL 支持指标自定义表达式 支持维度分组 添加指标表达式提示 按类型指定表达式 支持指标别名 支持发布|编辑数据集报表 支持分区键 支持列别名 支持同步历史记录 支持即席查询 (https://github.com/devlive-community/datacap/issues/581) 支持定时数据同步 (https://github.com/devlive-community/datacap/issues/590) --- 新增文件系统集成开发文档 添加合作伙伴演示文档 (https://github.com/devlive-community/datacap/issues/582) --- 更新 org.apache.maven.plugins:maven-compiler-plugin 3.3 到 3.12.1 更新 com.dameng:DmJdbcDriver18 8.1.2.192 到 8.1.3.62 更新 mysql 到 8.0.30 更新 snowflake 到 3.14.4 更新 com.taosdata.jdbc:taos-jdbcdriver 3.0.0 到 3.2.7 更新 org.apache.kylin:kylin-jdbc 2.6.3 到 4.0.3 更新 slf4j.version 1.7.36 到 2.0.10 更新 org.apache.maven.plugins:maven-assembly-plugin 3.5.0 到 3.6.0 更新 org.duckdb:duckdbjdbc 0.8.1 到 0.9.2 更新 org.apache.ignite:ignite-core 2.8.1 到 2.16.0 更新 org.projectlombok:lombok 1.18.28 到 1.18.30",
    "url": "https://datacap.devlive.org/zh-CN/release/2024.01.1.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.18.0",
    "content": "!!! note 当前版本涉及几项重大更新。 !!! DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.18.0 | 2023-12-22 | --- [元数据] 支持克隆行 [元数据] 支持截断表 [元数据] 支持刷新数据 [元数据] 支持删除表 [元数据] 支持展示表结构 [元数据] 支持展示表 ER 图 [元数据] 支持导出表数据 [元数据] 支持新建表 [元数据] 支持新建列 [元数据] 支持删除列 [查询] 添加自动添加 LIMIT [用户] 支持用户头像 [数据源] 优化数据源删除提示信息 [报表] 支持数据报表功能 [仪表盘] 支持仪表盘功能 [数据集] 支持数据集数据 --- 支持 FileSystem 支持 Local FileSystem --- 添加 SDK 使用文档 --- 支持多版本 修复连接具柄导致连接失败 --- 支持 MySQL JDBC 方式 支持拖拽构建 --- 更新 org.mongodb:mongodb-jdbc 2.0.2 到 2.0.3 更新 ch.qos.logback:logback-classic 1.2.11 到 1.2.13 更新 org.apache.kafka:kafka-clients 2.8.1 到 3.6.1 更新 ch.qos.logback:logback-core 1.2.11 到 1.2.13 更新 com.oracle.database.jdbc:ojdbc8 21.9.0.0 到 23.3.0.23.09 更新 org.elasticsearch.plugin:x-pack-sql-jdbc 7.10.0 到 8.11.3 更新 jackson.version 2.14.2 到 2.16.0",
    "url": "https://datacap.devlive.org/zh-CN/release/1.18.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.17.0",
    "content": "!!! note 当前版本涉及几项重大更新。 !!! DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.17.0 | 2023-11-20 | --- 删除无效的元数据管理器 优化问题模板 将版本添加到应用程序 !img.png 支持在启动时检查java版本是否兼容 修复了在不选择表的情况下直接选择列的错误 支持自定义列显示 !Column Display 支持重置列位置 !Reset Column Position 查询表视图会导致无法转换的值 添加 jvm 配置 当数据为空时，不返回任何列信息 修复切换表查询数据导致查询列为空的问题 支持复位自动递增 !Reset Auto Increment 统一每个插件返回的数据类型 支持查看构建语句语句语句 !Build Statement 为表添加筛选器 !Filter 支持添加新的行数据 !Add Row --- 删除未使用的组件 --- 更新 com.h2database:h2 2.1.214 到 2.2.224 更新 org.jetbrains.dokka:dokka-maven-plugin 1.8.10 到 1.9.10",
    "url": "https://datacap.devlive.org/zh-CN/release/1.17.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.16.0",
    "content": "!!! note 当前版本涉及几项重大更新。 !!! DataCap 发布! | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.16.0 | 2023-11-01 | --- 支持列顺序 !Column Order 支持删除行 支持删除多行 !Delete Rows 支持无主键数据更新 !Update Columns 支持根据主键更新 !Update Without Primary Key 支持预览待处理的更改 --- 支持选择查询 !Selection Query 支持自定义配置 !Custom Configure --- 添加用户配置文件文档 --- 更新 org.apache.maven.plugins:maven-javadoc-plugin 3.5.0 到 3.6.0 更新 com.oceanbase:oceanbase-client 2.4.2 到 2.4.5 更新 org.apache.maven.plugins:maven-javadoc-plugin 3.5.0 到 3.6.0",
    "url": "https://datacap.devlive.org/zh-CN/release/1.16.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.15.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.15.0 | 2023-10-18 | --- 支持数据库同步 支持表同步 支持列同步 添加调度程序历史记录 重构元数据模块 改进 SQL 文件 --- 替换 monaco 为 ace --- 添加中文部署文档 优化片段和数据来源中文文档",
    "url": "https://datacap.devlive.org/zh-CN/release/1.15.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.14.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.14.0 | 2023-09-14 | --- 修复数据源检查任务返回空的问题 添加验证码 支持登录验证码 支持验证码失败自动刷新 支持注册验证码 支持注册启用 移动 etc 到 configure --- 修复空编辑器破坏异常 公共页面增加布局 修复了个人资料页面错误 修复登录页面样式异常的问题 --- 支持 Kafka 输入｜输出 支持 ClickHouse 输入｜输出 支持删除 构建管道页面 支持提交 支持 SWITCH 字段类型 添加执行者标志 支持限流排队 支持停止 服务重启时重置流水线 添加日志界面并优化UI 支持字段描述 支持字段 SELECT 类型 支持字段检查 支持字段数组 支持 Redis 输出 支持指定运行时机制 --- 将 com.google.guava:guava 从 31.1-jre 更改为 32.1.2-jre 将 org.devlive.sdk:openai-java-sdk 从 1.5.0 升级到 1.9.0 将 com.h2database:h2 从 2.1.214 提升到 2.2.220 将 org.projectlombok:lombok 从 1.18.24 更改为 1.18.28 将 org.apache.kafka:kafka-clients 从 2.8.0 升级到 2.8.1 将 org.duckdb:duckdbjdbc 从 0.7.0 升级到 0.8.1 将 com.github.eirslett:frontend-maven-plugin 从 1.12.1 升级到 1.13.4 将 kotlin.version 从 1.8.20 升级到 1.9.10 将 org.sonatype.plugins:nexus-staging-maven-plugin 从 1.6 升级到 1.6.13",
    "url": "https://datacap.devlive.org/zh-CN/release/1.14.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.13.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.13.0 | 2023-08-09 | --- 修复了 openai-java-sdk 版本 添加数据源版本 添加数据源扫描任务 优化状态图标 删除对内置 H2 数据库的支持 --- 支持文字 修复 div 空白 添加了日历热图中文 修复了数据贡献图错误 当数据源不可用时列表禁用选择 --- 修复无法执行set语法sql的问题 修复连接未关闭的问题 --- 支持获取DBName和TableName --- 修复了 default 数据库中没有查询错误的问题",
    "url": "https://datacap.devlive.org/zh-CN/release/1.13.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.12.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.12.0 | 2023-07-11 | --- 删除日志默认调试级别 拆分各模块 修复错误依赖 支持菜单重定向 修复了 SQL 架构 支持是否功能菜单 将 openai sdk 替换为 openai-java-sdk 重构 chatgpt --- 修复缓存不清除，需要重新点击的问题 修复了无效令牌的错误 添加缓冲区标签名称 修复使用h2数据库的个人资料页面异常的问题 修复服务重启导致404",
    "url": "https://datacap.devlive.org/zh-CN/release/1.12.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.11.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.11.0 | 2023-06-13 | --- 添加了查询历史返回的行数 修复了主页上的 404 错误 将插件剥离到单独的文件夹中 --- 重构文件夹 添加菜单面包屑 同步服务器路由 添加非登录页面 添加源管理器路由 修复路由构建失败回调异常 修复数据源类型标记异常 --- 支持 apache cassandra",
    "url": "https://datacap.devlive.org/zh-CN/release/1.11.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.10.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:--------:|:------------:| | 1.10.0 | 2023-05-30 | --- 修复服务启动默认连接 mongo 修复了 sql 模板的 h2 db updatetime 和 createtime 改进 H2 元数据管理获取类型 改进 mysql 元数据管理获取类型 固定元数据管理数据页默认为 1 重构数据渲染表 支持栏目类型 添加耗时和查看执行 SQL 支持可选择的每页总计 支持标题提示数据类型 支持复制选定的数据结果 支持选择指定列查询 支持过滤器 修复默认用户创建时间为空 支持权限 固定用户 createTime 为空 --- 修复不清除网络授权信息 优化数据管理获取数据 禁用警告输出到控制台 增加编辑器缓冲提示限制 删除默认排序规则 重命名用户仪表板路径 添加仪表板聊天风格 修复导航样式 添加数据源加载状态 --- 支持 apache pinot 支持 mongo 社区版 --- 升级 clickhouse-jdbc 0.3.2-patch9 到 0.4.6 升级 oracle-xe 1.17.6 到 1.18.1 升级 kyuubi-hive-jdbc-shaded 1.6.0-incubating 到 1.7.1",
    "url": "https://datacap.devlive.org/zh-CN/release/1.10.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.9.0",
    "content": "!!! note 当前版本涉及几个主要更新。 !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap 已发布 :tada: :tada: :tada: :tada: :tada: :tada: | 发布版本 | 发布时间 | |:-------:|:------------:| | 1.9.0 | 2023-05-04 | --- 支持 github packages 优化 docker 镜像发布流程 支持格式化日期 添加数据库连接指定时区 修复了默认的 h2 数据库未初始化的定时任务 将 admin 用户添加到 README.md 添加 docker 镜像标签 在 README.md 中添加微信二维码 添加 docker 徽章 修复数据源创建时间为空 --- 添加中文文档 添加 Rainbond 部署文档 添加插件文档 支持顶部滚动通知 --- 修复数据表无效分页 修复了无法正确渲染的问题 修复包含国际化数据的渲染缺失的翻译结果 支持复制多选行 修复数据源测试状态问题 支持关闭消息 添加定时任务链接 --- 支持 ceresdb 支持 greptimedb 支持 questdb 支持 apache doris 支持 starrocks 支持 hologres 支持 apache hadoop hdfs --- 移除 http 重试逻辑 --- 修复了 ydb 依赖冲突 --- 添加配置文件 --- 升级 trino-jdbc 397 到 414 (#331) 升级 iotdb-jdbc 0.13.0 到 1.1.0 (#309)",
    "url": "https://datacap.devlive.org/zh-CN/release/1.9.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.8.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.8.0 | 2023-04-10 | --- Rename executor directory Optimize document release timing Fixed format connect url close #304 Support proxy for chatgpt close #299 ChatGPT is currently unable to associate context close #298 Support returning parsing error results Add schedule lib Support the code editor supports automatic prompts for data source library tables and columns close #301 Fix initialization sql script Support h2 database Remove some invalid jars Add docker publish ci --- Refactor install docs --- The code editor supports code fragments close #300 --- Support h2 for native (memory) --- Perfect test case Support SHOW DATABASES and SHOW TABLES ... --- Fixed validation sql content --- Bump jackson.version from 2.13.4 to 2.14.2 Bump postgresql from 42.5.0 to 42.6.0 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:-----------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.8.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.7.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.7.0 | 2023-03-20 | --- Add other issues template Add role Upgrade JDK 8 to 11 Support chatgpt Add submit pipeline api --- Add seatunnel executor --- Support execute sql on source Fixed code bugs --- Add icon to connectors --- Add executor spi --- Fixed duplicate tree menu data Optimized type display icon Optimize data source testing｜save interaction Support query history display plug-in type Add system announcement display Fixed the 'keyword' is repeated with tab page addition bug #208 Replace markdown preview component --- Support kafka --- Upgrade redis version from 3.6.3 to 4.3.1 Bump maven-assembly-plugin from 3.1.1 to 3.5.0 #272 Bump antlr4.version from 4.9.3 to 4.12.0 #262 Bump jedis from 3.6.3 to 4.3.1 #254 Bump DmJdbcDriver18 from 8.1.2.141 to 8.1.2.192 #234 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:-----------:| | @why198852 | | @Stacey1018 | | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.7.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.6.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.6.0 | 2023-03-02 | --- Add logo Support SHOW PATHS xxx Fixed function time field Refactor all module Add http lib Add logger lib --- JDBC: Repair Connection failure Do not close the connection --- Add default watermark Remove about page Add routing permission control Optimize lazy loading of the tree menu of the query page --- Support duckdb for jdbc close #249 Support alioss for native #250 --- Support SHOW PATHS --- Bump maven-javadoc-plugin from 3.4.1 to 3.5.1 Bump oceanbas-client from 2.4.0 to 2.4.2 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:----------:| | @why198852 | | @mlboy | | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.6.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.5.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.5.0 | 2023-02-16 | --- Support dsl query Remove incubator Add sql parser Refactor the module directories Set port default value is 0 --- Fixed jdbc no password exception is configured --- Support multi column sort --- Support zookeeper for native --- Add powered by page --- Fixed mget,hget value is displayed as null #219 --- Bump maven-javadoc-plugin from 2.10.4 to 3.4.1 Bump ojdbc8 from 21.1.0.0 to 21.9.0.0 Bump mongodb-jdbc from 2.0.0 to 2.0.2 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:--------------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.5.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.4.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.4.0 | 2023-01-31 | --- Fixed restart script Supports monitor process Do not modify the default system SQL template Fixed plugin template by name Support user login log Refactoring plug-in configuration extraction mode --- Support data source manager Add client cli --- Plug-in ICONS are displayed based on the plug-in type Optimize editor auto prompt Support watermark Templates are not supported for adding data sources Fixed footer link --- Support snowflake for jdbc Support ydb for jdbc --- Refactor some docs --- Fixed command multiple parameters --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:--------------:| | @qianmoQ | | @hometownglory | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.4.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.3.0",
    "content": "!!! note The current release involves several major updates. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.3.0 | 2022-12-16 | --- Support change username Support custom sql template Support plugin function Add restart script --- Optimize the presentation of the data source list Add data source description and prompt Support query history id order Support quote query history --- Support oceanbase for jdbc Support redis for native Support neo4j for jdbc Support iotdb for jdbc Support auth for native --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.3.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.2.0",
    "content": "!!! note The current release involves several major updates. The following link is Roadmap !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:---------------:|:------------:| | 1.2.0 | 2022-11-30 | --- Support http protocol --- Support for data result column header hiding (#139) Support data result filtering (#132 #140) Replace @antv/g2 to echarts Replace ant-design-vue to iview Replace @antv/s2 to ag-grid Optimize about page Optimize not found page Add not authorized page Add version badge Add not network page Support result visual line chart --- Support cratedb Support cratedb for http Support dameng Support clickhouse for http Support tdengine for jdbc Support impala for jdbc --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.2.0.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.1.0.20221115",
    "content": "!!! note The current release involves several major updates. The following link is Roadmap !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:----------------:|:------------:| | 1.1.0.20221115 | 2022-11-15 | --- Replace plugin name to id Support for internationalization issues-82 Reduce size of docker image Switch bash docker image to eclipse-temurin:8-jdk-focal Support ssl issues-75 Extract the plug-in to get the global tool Support database write operation issues-70 Supports user rights management Support code snippet issues-74 Support editor auto completion Support to provide data source schema tree bar issues-106 Support multiple editor issues-110 Add profile for user Support change user password Add data source radar map within 7 days Add about page Add feedback issues-126 --- Add custom validator --- Support MongoDB Support Dremio Support HBase jdbc for Phoenix issues-103 Support H2 Support SqlServer Support Oracle --- Fix cannot init RedisConnection issues-71 --- Update version to 7.10.0 --- Bump Kyuubi 1.6.0-incubating --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| |@pan3793| |@javalover123| |@shuangzishuai| |@GtoCm| |@why198852| | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.1.0.20221115.html",
    "lang": "zh-CN"
  },
  {
    "title": "1.0.0.20221015",
    "content": "!!! note This is the first new version we've released. !!! :tada: :tada: :tada: :tada: :tada: :tada: DataCap is released :tada: :tada: :tada: :tada: :tada: :tada: | Release Version | Release Time | |:----------------:|:------------:| | 1.0.0.20221015 | 2022-10-15 | --- Building SPI supports multiple data sources Supports web visualization based on Vue architecture Support Data source usage history Data statistics for data sources and history --- [x] Support ClickHouse [x] Support MySQL [x] Support Presto [x] Support Redis [x] Support PostgreSQL [x] Support Trino [x] Support ElasticSearch [x] Support Apache Druid [x] Support Apache Kyuubi [x] Support Apache Hive [x] Support Apache Kylin [x] Support Apache Ignite [x] Support IBM DB2 --- !!! danger Many thanks to the following contributors for contributing to the source code of this release In no particular order | GitHub ID | |:---------:| | @mlboy | | @qianmoQ | !!!",
    "url": "https://datacap.devlive.org/zh-CN/release/1.0.0.20221015.html",
    "lang": "zh-CN"
  },
  {
    "title": "开发规范",
    "content": "本文档主要用来介绍 DataCap 服务端和 UI 端的开发规范。 !!! danger 请仔细阅读该开发规范，并且遵守该规范，否则可能导致服务端和 UI 端的代码提交无法审核通过。 !!! 我们使用严格的提交信息格式： <类型>(<范围>): <描述> [可选 正文] [可选 脚注] 类型必须是以下之一： feat: 新功能 fix: Bug 修复 docs: 仅文档更改 style: 不影响代码含义的更改（空格、格式化等） refactor: 既不修复错误也不添加功能的代码更改 perf: 提高性能的代码更改 test: 添加或修正测试 chore: 构建过程或辅助工具的变动 revert: 撤销之前的提交 范围应该是受影响的模块名称。 示例： feat(auth): 实现基于 JWT 的身份验证 添加 JWT 中间件 实现令牌生成和验证 添加用户认证路由 集成 Redis 存储刷新令牌 close (#123) !!! warning 所有的 组件 | 字段名 必须都以驼峰命名法命名 !!! vue title=\"正确示例\" export default defineComponent({ name: 'RoleHome' }) const showName = ref(false) vue title=\"错误示例\" export default defineComponent({ name: 'rolehome' }) const showname = ref(false) !!! warning 组件命名必须以 Shadcn 开头，如 ShadcnButton、ShadcnInput、ShadcnSpace !!! vue title=\"正确示例\" <ShadcnButton size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <ShadcnIcon icon=\"Pencil\" size=\"15\"/> </ShadcnButton> vue title=\"错误示例\" <shadcn-button size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <shadcn-icon icon=\"pencil\" size=\"15\"/> </shadcn-button> !!! warning 属性命名首字母必须小写，如 size，多个单词必须已驼峰命名法命名，如 showName !!! vue title=\"正确示例\" const size = ref('small') const showName = ref(false) vue title=\"错误示例\" const Size = ref('small') const showname = ref(false) const show-name = ref(false) !!! warning 函数名必须符合以下规范 !!! 处理函数必须以 handle 开头，如 handleChangeInfo 回调函数必须以 on 开头，如 onChange 涉及到弹出层的函数必须以 visible 开头，如 visibleInfo vue title=\"正确示例\" const handleChangeInfo = () => {} const onChange = () => {} const visibleInfo = () => {} vue title=\"错误示例\" const changeinfo = () => {} const change = () => {} const aaaInfo = () => {} !!! note 多个组件内必须以新行分割 !!! vue title=\"正确示例\" <ShadcnSpace> <ShadcnTooltip :content=\"$t('common.editData')\"> <ShadcnButton size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <ShadcnIcon icon=\"Pencil\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> <ShadcnTooltip :content=\"$t('role.common.assignMenu').replace('$NAME', row?.name)\"> <ShadcnButton size=\"small\" circle @click=\"handlerAssignMenu(true, row)\"> <ShadcnIcon icon=\"Menu\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> </ShadcnSpace> vue title=\"错误示例\" <ShadcnSpace> <ShadcnTooltip :content=\"$t('common.editData')\"> <ShadcnButton size=\"small\" circle @click=\"handlerChangeInfo(true, row)\"> <ShadcnIcon icon=\"Pencil\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> <ShadcnTooltip :content=\"$t('role.common.assignMenu').replace('$NAME', row?.name)\"> <ShadcnButton size=\"small\" circle @click=\"handlerAssignMenu(true, row)\"> <ShadcnIcon icon=\"Menu\" size=\"15\"/> </ShadcnButton> </ShadcnTooltip> </ShadcnSpace> !!! warning 请保证属性的顺序，以便于后续的维护 !!! 多属性的正常顺序是 系统属性 (必须都在第一位，如 v-model) 固定属性 (如 show-total) 动态变量 (如 :page-size=\"pageSize\") 动态事件 (如 @on-change=\"onPageChange\") vue title=\"正确示例\" <ShadcnPagination v-model=\"pageIndex\" show-total :page-size=\"pageSize\" @on-change=\"onPageChange\"/> vue title=\"错误示例\" <ShadcnPagination v-model=\"pageIndex\" @on-change=\"onPageChange\" show-total :page-size=\"pageSize\"/> !!! warning 多个属性且超过 3 个必须以新行分割 !!! vue title=\"正确示例\" <ShadcnPagination v-model=\"pageIndex\" show-total @on-change=\"onPageChange\"/> <ShadcnPagination v-model=\"pageIndex\" show-total :page-size=\"pageSize\" @on-change=\"onPageChange\"/> vue title=\"错误示例\" <ShadcnPagination v-model=\"pageIndex\" show-total @on-change=\"onPageChange\"/> <ShadcnPagination v-model=\"pageIndex\" show-total :page-size=\"pageSize\" @on-change=\"onPageChange\"/>",
    "url": "https://datacap.devlive.org/zh-CN/developer/specification/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "服务端",
    "content": "本文章主要用来介绍我们如何来贡献 datacap 服务端源码。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------|:--------------|:---| | JDK | 1.8 \\| 11 | 必须 | | Maven | >= 3.5 | 可选 | | IDEA \\| Eclipse \\| 其他 | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 进入项目根目录执行以下代码 bash ./mvnw -T 1C clean install package -Dspotbugs.skip -Dgpg.skip -Dcheckstyle.skip -DskipTests=true --- 打开 IDEA 编辑器，弹出类似如下窗口 !img.png !!! note 不同的 IDEA 版本，各个按钮的所在位置可能不相同。 !!! 点击红框指出的菜单，选择 datacap 源码的根目录，点击打开即可，此时会进入源码的页面，加载完成后窗口如下 !img1.png 左侧展示的是 datacap 源码的各个目录。以下是每个目录的功能： .github: github 需要的一些 issues 模版，pull request 模版，还有一些 CI 自动化配置脚本等 .mvn: maven wapper 的配置 client: 客户端的相关源码 datacap-cli: datacap 命令行源码 configure: datacap 相关配置 assembly datacap 打包用到的各种配置 etc datacap 服务端配置文件 bin datacap 服务启动脚本 conf datacap 服务配置文件 plugins datacap 插件配置文件 git-forks datacap fork 主仓库脚本 git-hook datacap git 本地提交相关 hook proxy datacap 服务部署代理配置 publish datacap 用于发布的各脚本 core datacap 服务端核心源码 datacap-captcha datacap 验证码源码 datacap-common datacap 常用工具源码 datacap-parser datacap SQL 解析器源码 datacap-security datacap 安全相关源码 datacap-server datacap 服务端源码 datacap-service datacap 服务端服务源码 datacap-spi datacap 服务端 SPI 源码 datacap-web datacap Web 前端源码 docs: datacap 文档源码 driver: datacap 数据源驱动源码 executor: datacap 执行器源码 file: datacap 文件格式化转换器源码 fs: datacap 文件系统源码 lib: datacap 依赖包源码 notify: datacap 通知器源码 parser: datacap sql 解析器源码 plugin: datacap 插件源码 scheduler: datacap 调度器源码 shaded: datacap 重制依赖源码 --- 找到 DataCap 主文件，路径为 src/main/java/io/edurt/datacap/server/DataCap.java 在文件上右键，类似下图 !img2.png 弹出类似如下窗口 !img3.png 点击 Modify options, 找到 Program arguments，并点击它 !img4.png 此时窗口将会变成 !img5.png 在输入框中添加 --spring.config.location= 里面的值填写 configure/etc/conf 的绝对路径，比如项目在 /root/datacap 目录下，那么该值为 /root/datacap/configure/etc/conf/ !!! danger 注意 conf 后必须增加 / 否则配置会不生效 !!! 完成后点击 OK 按钮，即可启动服务，访问 http://localhost:9096/ 成功后，表示服务启动成功。",
    "url": "https://datacap.devlive.org/zh-CN/developer/server/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "UI 端",
    "content": "本文章主要用来介绍我们如何来贡献 datacap UI 端源码。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------------|:---------|:---| | Node | v18.x+ | 必须 | | yarn | 1.22+ | 必须 | | IDEA \\| Eclipse \\| WebStrom | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 可参考 服务端 文档中的 加载源码到 IDEA 部分。 --- 进入源码目录 sql cd core/datacap-ui 安装依赖 (推荐使用 yarn 或 pnpm) bash pnpm install 启动服务 bash pnpm run dev 返回类似如下信息 !img.png 通过浏览器访问 http://localhost:8080 调试源代码，不要使用 Network 返回的地址，看到返回如下页面表示启动成功。 !img1.png !!! danger 如果服务端也启动后，系统将会跳转到登录页面。 !!!",
    "url": "https://datacap.devlive.org/zh-CN/developer/web/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "文档端",
    "content": "本文章主要用来介绍我们如何来贡献 datacap UI 端源码。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------|:-----|:---| | mkdocs | 1.5+ | 必须 | | Python | 3+ | 必须 | | IDEA \\| Eclipse \\| 其他 | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 可参考 服务端 文档中的 加载源码到 IDEA 部分。 --- 进入源码目录 sql cd docs 安装依赖 bash pip install -r requirements.txt 启动服务 bash mkdocs serve --dev-addr=0.0.0.0:8001 返回类似如下信息 !img.png 通过浏览器访问 http://localhost:8001 调试源代码，看到返回如下页面表示启动成功。 !img1.png",
    "url": "https://datacap.devlive.org/zh-CN/developer/doc/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "SDK 集成",
    "content": "本文章主要用来介绍如何集成 datacap 提供的 plugin 插件快速访问对应数据源。在本文中我们使用 datacap-jdbc-mysql SDK 来访问 MySQL 数据源。 --- 引入插件依赖 Apache Maven xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-jdbc-mysql</artifactId> <version>VERSION</version> </dependency> Gradle xml implementation group: 'io.edurt.datacap', name: 'datacap-jdbc-mysql', version: 'VERSION' !!! danger 请将 VERSION 替换为对应的版本号 !!! --- java String sql = \"SHOW TABLES\"; // 构建连接配置 Configure configure = new Configure(); configure.setHost(\"localhost\"); configure.setPort(3306); configure.setUsername(Optional.of(\"root\")); configure.setPassword(Optional.of(\"12345678\")); // 初始化 MySQL 插件 MySQLPlugin plugin = new MySQLPlugin(); plugin.connect(configure); // 获取执行结果 Response response = plugin.execute(sql); response.columns.forEach(item -> { log.info(item); }); !!! note 请注意 datacap-jdbc-mysql 插件仅支持 MySQL 数据源，如果需要其他 SDK 请使用其他的插件。 !!!",
    "url": "https://datacap.devlive.org/zh-CN/developer/sdk/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "自定义功能",
    "content": "本文章主要用来介绍如何集成在 datacap 中开发新的功能。我们这里以 github 中的 issues-481 为例来讲解。 --- 重要 克隆源码到本地（如果您需要提交代码到主仓库中需要先将源码 fork 到您的 github 账户中） bash git clone https://github.com/devlive-community/datacap.git !!! note 如果您已经 fork 源码到您的账户中，请将 devlive-community 替换为您的 github 账户的 ID !!! 重要 以下是基本的环境配置 | 环境 | 版本 | 必需 | |:----------------------------|:--------------|:---| | JDK | 1.8 \\| 11 | 必须 | | Maven | >= 3.5 | 可选 | | IDEA \\| Eclipse \\| 其他 | 任意版本 | 必须 | !!! note 在本文中我们使用的是 IDEA 编辑器环境，用户可以根据自己喜好更换相应编辑器。 !!! 可参考 服务端 文档中的 加载源码到 IDEA 部分。 --- 针对于源码开发分为以下部分： 服务端 [可选] UI 端 [可选] 文档 (如果增加了新功能，涉及到 UI 建议添加文档方便用户快速了解使用方式) --- 服务端的源码大致可以分为以下部分： entity ： 数据模型 repository： 数据库操作 service： 业务逻辑 controller： 接口 sql： SQL 语句 以上三个部分在 datacap 中已经提供相应的基础支持，如果没有特殊的需求可以直接使用提供的基础功能，该示例中我们不做特殊的需求开发，只用到 datacap 提供的基础功能。 --- 在 datacap 中提供了 BaseEntity 类，可以作为基础的实体类使用。它提供了以下属性： id：主键 name：名称 active：是否激活 createtime：创建时间 updatetime：更新时间 我们新建 ReportEntity 类，代码如下： java import edu.umd.cs.findbugs.annotations.SuppressFBWarnings; import io.edurt.datacap.service.enums.ReportType; import lombok.AllArgsConstructor; import lombok.Data; import lombok.EqualsAndHashCode; import lombok.NoArgsConstructor; import lombok.ToString; import lombok.experimental.SuperBuilder; import org.springframework.data.jpa.domain.support.AuditingEntityListener; import javax.persistence.Column; import javax.persistence.Entity; import javax.persistence.EntityListeners; import javax.persistence.EnumType; import javax.persistence.Enumerated; import javax.persistence.JoinColumn; import javax.persistence.JoinTable; import javax.persistence.ManyToOne; import javax.persistence.Table; @Data @SuperBuilder @ToString @NoArgsConstructor @AllArgsConstructor @EqualsAndHashCode(callSuper = true) @Entity @Table(name = \"datacapreport\") @EntityListeners(AuditingEntityListener.class) @SuppressFBWarnings(value = {\"EIEXPOSEREP\", \"EQOVERRIDINGEQUALSNOTSYMMETRIC\"}) public class ReportEntity extends BaseEntity { @Column(name = \"configure\") private String configure; @Column(name = \"type\") @Enumerated(EnumType.STRING) private ReportType type; @ManyToOne @JoinTable(name = \"datacapreportuserrelation\", joinColumns = @JoinColumn(name = \"reportid\"), inverseJoinColumns = @JoinColumn(name = \"userid\")) private UserEntity user; } !!! warning 代码中的 @SuperBuilder 一定要添加，不然我们无法使用父类提供的 builder 方法。 !!! @Table(name = \"datacapreport\") 这里我们需要修改表名，格式为 datacap${表名}。 在以上示例中我们增加了三个属性 (根据功能的情况而定需要的属性)： configure：配置 type：类型 user：用户关联信息 --- 我们新建 ReportRepository 接口，代码如下： java package io.edurt.datacap.service.repository; import io.edurt.datacap.service.entity.ReportEntity; import org.springframework.data.repository.PagingAndSortingRepository; public interface ReportRepository extends PagingAndSortingRepository<ReportEntity, Long> { } 如果没有特殊需求，这里我们使用 JPA 提供的默认方法，无需自己实现。 --- 在 datacap 中提供了 BaseService 类，可以作为基础的实体类使用。它提供了以下属性： getAll 根据指定的过滤器获取所有数据 getById 根据指定的 ID 获取数据 saveOrUpdate 保存或更新数据 deleteById 删除指定的 ID 数据 我们新建 ReportService 类，代码如下： java package io.edurt.datacap.service.service; import io.edurt.datacap.service.entity.ReportEntity; public interface ReportService extends BaseService<ReportEntity> { } 添加实现类 ReportServiceImpl，代码如下： java package io.edurt.datacap.service.service.impl; import io.edurt.datacap.service.service.ReportService; import org.springframework.stereotype.Service; @Service public class ReportServiceImpl implements ReportService { } !!! note 如果没有特殊的需求，实现类可以不添加。 !!! --- 在 datacap 中提供了 BaseController 类，可以作为基础的实体类使用。它提供了以下属性： list 根据指定的过滤器获取所有数据 saveOrUpdate 保存或更新数据 delete 删除指定的 ID 数据 deleteForPath 根据指定的路径删除数据 getInfoForPath 根据指定的路径获取数据 我们新建 ReportController 类，代码如下： java package io.edurt.datacap.server.controller; import io.edurt.datacap.service.entity.ReportEntity; import io.edurt.datacap.service.repository.ReportRepository; import io.edurt.datacap.service.service.ReportService; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController() @RequestMapping(value = \"/api/v1/report\") public class ReportController extends BaseController<ReportEntity> { private final ReportRepository repository; private final ReportService service; protected ReportController(ReportRepository repository, ReportService service) { super(repository, service); this.repository = repository; this.service = service; } } @RequestMapping(value = \"/api/v1/report\") 这里我们需要修改表名，格式为 /api/v1/${路由}. !!! danger 一定要将自定义 controller 的构造函数中的参数修改为相关的具体类。 !!! --- !!! note 如果没有新增 SQL 相关可以忽略此步骤。 !!! 通过以上步骤已经提供了 API，底层处理相关类，剩下的需要我们添加对于功能实现的相关 SQL 语句。 将 SQL 语句添加到 datacap.sql 文件中，格式如下： sql CREATE TABLE datacapreport ( id BIGINT AUTOINCREMENT PRIMARY KEY, name VARCHAR(255), active BOOLEAN DEFAULT TRUE, createtime DATETIME, updatetime DATETIME, configure LONGTEXT, type VARCHAR(255) ); CREATE TABLE datacapreportuserrelation ( reportid BIGINT, userid BIGINT ); !!! note 如果是新版本开发的话，需要在 core/datacap-server/src/main/schema 目录下新建 版本号 文件夹，在该文件夹下创建 schema.sql 文件。并将内容添加到 schema.sql 文件中。 !!! --- UI 端的源码大致可以分为以下部分： 构建菜单 构建 Vue 页面 --- !!! note 如果构建的功能不支持菜单，可以忽略此步骤。此操作需要登录管理员权限。 !!! 构建菜单可以参考 管理菜单。 该功能的配置如下 !img.png 跳转到权限模块进行新菜单权限的分配。 !img1.png --- 在修改国际化配置需要修改两个配置文件(如果增加了其他的国际化文件修改的会更多) core/datacap-web/src/i18n/langs/en/common.ts core/datacap-web/src/i18n/langs/zhCn/common.ts 在文件中增加以下代码 java report: 'Report' report: '报表' --- 在 core/datacap-web/src/services/admin 目录下新建 ReportService.ts 文件，代码如下： ts import {ResponseModel} from '@/model/ResponseModel'; import {BaseService} from '@/services/BaseService'; const baseUrl = '/api/v1/report'; class ReportService extends BaseService<any> { constructor() { super(baseUrl); } deleteById(id: number): Promise<ResponseModel> { throw new Error('Method not implemented.'); } getByName<T>(name: string): Promise<ResponseModel> { return Promise.resolve(undefined); } } export default new ReportService(); --- 在 core/datacap-web/src/views/admin 目录下新建 report 目录，并在该文件夹下创建 ReportUtils.ts 文件，代码如下： ts const createHeaders = (i18n: any) => { return [ { title: i18n.t('common.no'), key: 'id', sortable: 'custom' }, { title: i18n.t('common.name'), key: 'name' }, { title: i18n.t('common.type'), key: 'type' }, { title: i18n.t('common.createTime'), key: 'createTime', ellipsis: true, tooltip: true }, { title: i18n.t('common.endTime'), key: 'endTime', ellipsis: true, tooltip: true }, { title: i18n.t('common.action'), slot: 'action', key: 'action' } ]; } export { createHeaders }; 在 core/datacap-web/src/views/admin 目录下新建 report 目录，并在该文件夹下创建 AdminReport.vue 文件，代码如下： vue <template> <div> <Card style=\"width:100%\" :title=\"$t('common.report')\" dis-hover> <Table :loading=\"loading\" :columns=\"headers\" :data=\"data.content\"> <template #action=\"{ row }\"> <Space> </Space> </template> </Table> <p v-if=\"!loading\" style=\"margin-top: 10px;\"> <Page v-model=\"pagination.current\" show-sizer show-elevator show-total :total=\"pagination.total\" :page-size=\"pagination.pageSize\" @on-page-size-change=\"handlerSizeChange\" @on-change=\"handlerIndexChange\"> </Page> </p> </Card> </div> </template> <script lang=\"ts\"> import {defineComponent} from \"vue\"; import {useI18n} from 'vue-i18n'; import Common from \"@/common/Common\"; import {ResponsePage} from \"@/model/ResponsePage\"; import {createHeaders} from \"@/views/admin/report/ReportUtils\"; import ReportService from \"@/services/admin/ReportService\"; import {Filter} from \"@/model/Filter\"; import {Pagination, PaginationBuilder} from \"@/model/Pagination\"; const filter: Filter = new Filter(); const pagination: Pagination = PaginationBuilder.newInstance(); export default defineComponent({ name: \"ReportAdmin\", setup() { const i18n = useI18n(); const headers = createHeaders(i18n); const currentUserId = Common.getCurrentUserId(); return { headers, filter, currentUserId } }, data() { return { data: ResponsePage, loading: false, pagination: { total: 0, current: 1, pageSize: 10 } } }, created() { this.handlerInitialize(this.filter); }, methods: { handlerInitialize(filter: Filter) { this.loading = true; ReportService.getAll(filter) .then((response) => { if (response.status) { this.data = response.data; this.pagination.total = response.data.total; } this.loading = false; }) }, handlerSizeChange(size: number) { this.pagination.pageSize = size; this.handlerTableChange(this.pagination); }, handlerIndexChange(index: number) { this.pagination.current = index; this.handlerTableChange(this.pagination); }, handlerTableChange(pagination: any) { this.pagination.current = pagination.current; this.pagination.pageSize = pagination.pageSize; this.handlerInitialize(this.filter) } } }); </script>",
    "url": "https://datacap.devlive.org/zh-CN/developer/feature/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "Java 实现",
    "content": "DataCap 支持自定义插件，使用者可以编写自己的插件集成到系统中。该文档主要讲解如何快速集成一个插件到 DataCap 系统中。 !!! note 本文使用集成基于 HTTP 协议的 QuestDB 数据存储系统来演示。 !!! xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-spi</artifactId> <scope>provided</scope> </dependency> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-common</artifactId> </dependency> <dependency> <groupId>commons-beanutils</groupId> <artifactId>commons-beanutils</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>org.testcontainers</groupId> <artifactId>testcontainers</artifactId> <scope>test</scope> </dependency> 以上配置添加了 datacap-spi 和 datacap-common 模块，其他的是一些辅助依赖。 !!! warning 需要注意的是如果您是单独开启的项目需要指定各个依赖的版本号。 !!! java public class QuestDBPluginModule extends AbstractPluginModule implements PluginModule { @Override public String getName() { return \"QuestDB\"; } @Override public PluginType getType() { return PluginType.HTTP; } @Override public AbstractPluginModule get() { return this; } protected void configure() { Multibinder<Plugin> plugin = Multibinder.newSetBinder(this.binder(), Plugin.class); plugin.addBinding().to(QuestDBPlugin.class); } } 加载器需要继承 AbstractPluginModule 类，并实现 PluginModule 接口，这样系统会在启动时自动将插件加载到系统中。 !!! note 需要注意的是，需要覆盖父类中的 configure() 方法，并将插件绑定到系统中。 !!! java @Slf4j public class QuestDBPlugin implements Plugin { private HttpConfigure configure; private HttpConnection connection; private Response response; @Override public String name() { return \"QuestDB\"; } @Override public String description() { return \"Integrate QuestDB data sources\"; } @Override public PluginType type() { return PluginType.HTTP; } @Override public void connect(Configure configure) { try { this.response = new Response(); this.configure = new HttpConfigure(); BeanUtils.copyProperties(this.configure, configure); this.connection = new HttpConnection(this.configure, this.response); } catch (Exception ex) { this.response.setIsConnected(Boolean.FALSE); this.response.setMessage(ex.getMessage()); } } @Override public Response execute(String content) { if (ObjectUtils.isNotEmpty(this.connection)) { log.info(\"Execute questdb plugin logic started\"); this.response = this.connection.getResponse(); QuestDBAdapter processor = new QuestDBAdapter(this.connection); this.response = processor.handlerExecute(content); log.info(\"Execute questdb plugin logic end\"); } this.destroy(); return this.response; } @Override public void destroy() { if (ObjectUtils.isNotEmpty(this.connection)) { this.connection.destroy(); } } } 执行器需要实现 Plugin 接口，该接口中提供了以下方法 name(): 插件有唯一名称，同名插件只会在第一次加载时生效 description(): 对于该插件的描述 type(): 插件类型 connect(Configure configure): 插件需要提前连接信息，比如当前插件插件，就是插件的连接阶段（系统预设 HTTP 连接方式直接使用）。 execute(String content): 具体执行操作逻辑 destroy(): 插件最后的销毁，注意销毁需要包含连接中的信息 java @Slf4j public class QuestDBAdapter extends HttpAdapter { public QuestDBAdapter(HttpConnection connection) { super(connection); } @Override public Response handlerExecute(String content) { Time processorTime = new Time(); processorTime.setStart(new Date().getTime()); Response response = this.httpConnection.getResponse(); HttpConfigure configure = new HttpConfigure(); if (response.getIsConnected()) { List<String> headers = new ArrayList<>(); List<String> types = new ArrayList<>(); List<Object> columns = new ArrayList<>(); try { BeanUtils.copyProperties(configure, this.httpConnection.getConfigure()); configure.setAutoConnected(Boolean.FALSE); configure.setRetry(0); configure.setMethod(HttpMethod.GET); configure.setPath(\"exec\"); Map<String, String> parameters = Maps.newHashMap(); parameters.put(\"query\", content); configure.setParams(parameters); configure.setDecoded(true); HttpConnection httpConnection = new HttpConnection(configure, new Response()); HttpClient httpClient = HttpClient.getInstance(configure, httpConnection); String body = httpClient.execute(); QuestDBResponse requestResponse = JSON.objectmapper.readValue(body, QuestDBResponse.class); if (ObjectUtils.isNotEmpty(requestResponse.getQuery())) { response.setIsSuccessful(true); if (ObjectUtils.isNotEmpty(requestResponse.getColumns())) { requestResponse.getColumns() .forEach(schema -> { headers.add(schema.getName()); types.add(schema.getType()); }); } requestResponse.getDataset() .forEach(record -> columns.add(handlerFormatter(configure.getFormat(), headers, record))); } else { response.setIsSuccessful(Boolean.FALSE); response.setMessage(requestResponse.getError()); } } catch (Exception ex) { log.error(\"Execute content failed content {} exception \", content, ex); response.setIsSuccessful(Boolean.FALSE); response.setMessage(ex.getMessage()); } finally { response.setHeaders(headers); response.setTypes(types); response.setColumns(columns); } } processorTime.setEnd(new Date().getTime()); response.setProcessor(processorTime); return response; } } 插件转换器用于对当前插件执行后的结果的转化，将其转换为 DataCap 中可以使用的逻辑。主要是用于封装 Response 返回结果。 本文是基于 JDBC 的插件所以直接继承 HttpAdapter 父类即可实现部分功能。 在 resources 源目录下添加 META-INF 和 services 目录 !!! warning services 在 resources 目录中需要 !!! 创建 io.edurt.datacap.spi.PluginModule 文件，内容如下 java io.edurt.datacap.plugin.http.questdb.QuestDBPluginModule 该文件的内容是我们定义好的插件加载模块。 !!! warning 插件的单元测试可以参考已经发布的插件进行测试 !!!",
    "url": "https://datacap.devlive.org/zh-CN/developer/plugin/java/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "自定义 Plugin 连接器",
    "content": "DataCap 支持自定义插件，使用者可以编写自己的插件集成到系统中。该文档主要讲解如何快速集成一个插件到 DataCap 系统中。 !!! note 本文使用集成基于 JDBC 协议的 StarRocks 数据存储系统来演示。 !!! xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-spi</artifactId> <scope>provided</scope> </dependency> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-common</artifactId> </dependency> <dependency> <groupId>commons-beanutils</groupId> <artifactId>commons-beanutils</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </dependency> <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-simple</artifactId> <scope>test</scope> </dependency> <dependency> <groupId>org.testcontainers</groupId> <artifactId>testcontainers</artifactId> <scope>test</scope> </dependency> 以上配置添加了 datacap-spi 和 datacap-common 模块，其他的是一些辅助依赖。 !!! warning 需要注意的是如果您是单独开启的项目需要指定各个依赖的版本号。 !!! kotlin class StarRocksPluginModule : AbstractPluginModule(), PluginModule { override fun getName(): String { return \"StarRocks\" } override fun getType(): PluginType { return PluginType.JDBC } override fun get(): AbstractPluginModule { return this } override fun configure() { val module = Multibinder.newSetBinder(binder(), String::class.java) module.addBinding().toInstance(this.javaClass.simpleName) val plugin: Multibinder<Plugin> = Multibinder.newSetBinder(binder(), Plugin::class.java) plugin.addBinding().to(StarRocksPlugin::class.java) } } 加载器需要继承 AbstractPluginModule 类，并实现 PluginModule 接口，这样系统会在启动时自动将插件加载到系统中。 !!! note 需要注意的是，需要覆盖父类中的 configure() 方法，并将插件绑定到系统中。 !!! kotlin class StarRocksPlugin : Plugin { private val log = getLogger(StarRocksPlugin::class.java) private var jdbcConfigure: JdbcConfigure? = null private var jdbcConnection: JdbcConnection? = null private var jdbcResponse: Response? = null override fun name(): String { return \"StarRocks\" } override fun description(): String { return \"Integrate StarRocks data sources\" } override fun type(): PluginType { return PluginType.JDBC } override fun connect(configure: Configure?) { try { log.info(\"Connecting to StarRocks\") jdbcResponse = Response() jdbcConfigure = JdbcConfigure() BeanUtils.copyProperties(jdbcConfigure, configure) jdbcConfigure!!.jdbcDriver = \"com.mysql.cj.jdbc.Driver\" jdbcConfigure!!.jdbcType = \"mysql\" jdbcConnection = object : JdbcConnection(jdbcConfigure, jdbcResponse) {} } catch (ex: Exception) { jdbcResponse!!.isConnected = false jdbcResponse!!.message = ex.message } } override fun execute(content: String?): Response { if (ObjectUtils.isNotEmpty(jdbcConnection)) { log.info(\"Execute starrocks plugin logic started\") jdbcResponse = jdbcConnection?.response val processor = JdbcAdapter(jdbcConnection) jdbcResponse = processor.handlerExecute(content) log.info(\"Execute starrocks plugin logic end\") } destroy() return jdbcResponse!! } override fun destroy() { if (ObjectUtils.isNotEmpty(jdbcConnection)) { jdbcConnection?.destroy() jdbcConnection = null } } } 执行器需要实现 Plugin 接口，该接口中提供了以下方法 name(): 插件有唯一名称，同名插件只会在第一次加载时生效 description(): 对于该插件的描述 type(): 插件类型 connect(Configure configure): 插件需要提前连接信息，比如当前插件插件，就是插件的连接阶段（系统预设 HTTP 连接方式直接使用）。 execute(String content): 具体执行操作逻辑 destroy(): 插件最后的销毁，注意销毁需要包含连接中的信息 插件转换器用于对当前插件执行后的结果的转化，将其转换为 DataCap 中可以使用的逻辑。主要是用于封装 Response 返回结果。 本文是基于 JDBC 的插件所以直接继承 JdbcAdapter 父类即可实现部分功能。 在 resources 源目录下添加 META-INF 和 services 目录 !!! warning services 在 resources 目录中需要 !!! 创建 io.edurt.datacap.spi.PluginModule 文件，内容如下 java io.edurt.datacap.plugin.jdbc.starrocks.StarRocksPluginModule 该文件的内容是我们定义好的插件加载模块。 !!! warning 插件的单元测试可以参考已经发布的插件进行测试 !!!",
    "url": "https://datacap.devlive.org/zh-CN/developer/plugin/kotlin/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "自定义 Fs 文件系统",
    "content": "DataCap 支持自定义文件系统，使用者可以编写自己的文件存储系统集成到 DataCap 中。该文档主要讲解如何快速集成一个文件存储系统到 DataCap 系统中。 该模块我们主要使用到的是 fs 模块内的代码，我们本文使用本地存储来做示例。 --- 新建项目后在 pom.xml 文件中增加以下内容： xml <dependencies> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-fs-spi</artifactId> <version>${project.version}</version> </dependency> </dependencies> 我们添加 datacap-fs-spi 依赖，这样我们就可以实现集成文件系统。 --- kotlin package io.edurt.datacap.fs import com.google.inject.multibindings.Multibinder class LocalModule : FsModule() { override fun configure() { Multibinder.newSetBinder(binder(), Fs::class.java) .addBinding() .to(LocalFs::class.java) } } --- kotlin package io.edurt.datacap.fs import org.slf4j.LoggerFactory import java.io.File class LocalFs : Fs { private val log = LoggerFactory.getLogger(this.javaClass) override fun writer(request: FsRequest): FsResponse { log.info(\"LocalFs writer origin path [ {} ]\", request.localPath) val targetPath = listOf(request.endpoint, request.bucket, request.fileName).joinToString(File.separator) val response = FsResponse.builder() .origin(request.localPath) .remote(targetPath) .successful(true) .build() log.info(\"LocalFs writer target path [ {} ]\", targetPath) try { if (request.localPath == null || request.localPath.isEmpty()) { IOUtils.copy(request.stream, targetPath, true) } else { IOUtils.copy(request.localPath, targetPath, true) } log.info(\"LocalFs writer [ {} ] successfully\", targetPath) } catch (e: Exception) { log.error(\"LocalFs writer error\", e) response.successful = false response.message = e.message } return response } override fun reader(request: FsRequest): FsResponse { val targetPath = listOf(request.endpoint, request.bucket, request.fileName).joinToString(File.separator) log.info(\"LocalFs reader origin path [ {} ]\", targetPath) val response = FsResponse.builder() .remote(targetPath) .successful(true) .build() try { response.context = IOUtils.reader(targetPath) log.info(\"LocalFs reader [ {} ] successfully\", targetPath) } catch (e: Exception) { log.error(\"LocalFs reader error\", e) response.successful = false response.message = e.message } return response } } 在转换器中我们只需要实现以下两个方法： public FsResponse writer(FsRequest request) 用于写数据到文件系统 public FsResponse reader(FsRequest request) 用于读取文件系统中的数据 --- kotlin package io.edurt.datacap.fs import java.io.InputStream import java.nio.file.Files import java.nio.file.Path import java.nio.file.Paths import java.nio.file.StandardCopyOption import java.nio.file.StandardOpenOption object IOUtils { / Copies a file from the source path to the target path. @param source the path of the file to be copied @param target the path where the file should be copied to @param createdDir a flag indicating whether the parent directories of the target path should be created if they do not exist @return true if the file was successfully copied, false otherwise / fun copy(source: String, target: String, createdDir: Boolean): Boolean { try { val targetPath: Path = Paths.get(target) if (createdDir) { Files.createDirectories(targetPath.parent) } Files.copy(Paths.get(source), targetPath, StandardCopyOption.REPLACEEXISTING) return true } catch (e: Exception) { throw RuntimeException(e) } } / Copies the contents of the input stream to the specified target file path. @param stream the input stream containing the contents to be copied @param target the path of the target file where the contents will be copied to @param createdDir indicates whether the parent directory of the target file has been created @return true if the contents were successfully copied, false otherwise / fun copy(stream: InputStream, target: String, createdDir: Boolean): Boolean { try { val targetPath: Path = Paths.get(target) if (createdDir) { Files.createDirectories(targetPath.parent) } Files.copy(stream, targetPath, StandardCopyOption.REPLACEEXISTING) return true } catch (e: Exception) { throw RuntimeException(e) } } / This function takes a source file path and returns an InputStream object that can be used to read the contents of the file. @param source the path of the source file to read @return an InputStream object representing the source file / fun reader(source: String): InputStream { try { return Files.newInputStream(Paths.get(source), StandardOpenOption.READ) } catch (e: Exception) { throw RuntimeException(e) } } } 在转换器中我们主要实现本地数据的读写操作。 --- 在 resources 源目录下添加 META-INF 和 services 目录，格式为 resources/META-INF/services，创建 io.edurt.datacap.fs.FsModule 文件，内容如下 kotlin io.edurt.datacap.fs.LocalModule !!! note 通过以上内容我们实现了本地数据存储的模块支持。我们只需要在要使用存储的地方引用该模块即可。 !!! !!! warning 我们建议添加对模块的测试用例，可参考已经提交的代码进行模拟测试。 !!!",
    "url": "https://datacap.devlive.org/zh-CN/developer/filesystem/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "自定义 File 转换器",
    "content": "DataCap 支持自定义 File 转换器，使用者可以编写自己的文件转换器集成到 DataCap 中。该文档主要讲解如何快速集成一个文件转换器到 DataCap 系统中。 该模块我们主要使用到的是 file 模块内的代码，我们本文使用 json 来做示例。 --- 新建项目后在 pom.xml 文件中增加以下内容： xml <dependencies> <dependency> <groupId>org.jetbrains.kotlin</groupId> <artifactId>kotlin-reflect</artifactId> </dependency> <dependency> <groupId>com.google.inject</groupId> <artifactId>guice</artifactId> </dependency> <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> </dependency> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-file-spi</artifactId> </dependency> <dependency> <groupId>com.fasterxml.jackson.core</groupId> <artifactId>jackson-core</artifactId> </dependency> </dependencies> <build> <plugins> <plugin> <groupId>org.jetbrains.dokka</groupId> <artifactId>dokka-maven-plugin</artifactId> </plugin> </plugins> </build> 我们添加 datacap-file-spi 依赖，这样我们就可以实现集成文件转换器。 --- kotlin package io.edurt.datacap.convert.json import com.google.inject.multibindings.Multibinder import io.edurt.datacap.convert.File import io.edurt.datacap.convert.FileModule class JsonModule : FileModule() { override fun configure() { Multibinder.newSetBinder(this.binder(), File::class.java) .addBinding() .to(JsonFile::class.java) } } --- kotlin package io.edurt.datacap.convert.json import com.fasterxml.jackson.core.JsonEncoding import com.fasterxml.jackson.core.JsonFactory import com.fasterxml.jackson.core.JsonGenerationException import com.fasterxml.jackson.databind.JsonNode import com.fasterxml.jackson.databind.ObjectMapper import com.fasterxml.jackson.databind.node.ObjectNode import io.edurt.datacap.common.utils.DateUtils import io.edurt.datacap.convert.File import io.edurt.datacap.convert.FileConvert.formatFile import io.edurt.datacap.convert.model.FileRequest import io.edurt.datacap.convert.model.FileResponse import org.slf4j.LoggerFactory.getLogger import java.io.BufferedReader import java.io.IOException import java.io.InputStreamReader import java.util.Objects.requireNonNull class JsonFile : File { private val log = getLogger(this::class.java) override fun format(request: FileRequest): FileResponse { val response = FileResponse() try { log.info(\"${name()} format start time [ ${DateUtils.now()} ]\") log.info(\"${name()} format headers start\") response.headers = request.headers log.info(\"${name()} format headers end\") log.info(\"${name()} format columns start\") val mapper = ObjectMapper() val columns = mutableListOf<Any>() request.columns .forEach { column -> val jsonNode = mapper.createObjectNode() for (headerIndex in request.headers.indices) { val header = request.headers[headerIndex] as String when (column) { is List<> -> jsonNode.putPOJO(header, column[headerIndex]) else -> jsonNode.putPOJO(header, column) } } columns.add(jsonNode) } response.columns = columns log.info(\"${name()} format columns end\") log.info(\"${name()} format end time [ ${DateUtils.now()} ]\") response.successful = true } catch (e: IOException) { response.successful = false response.message = e.message } return response } override fun formatStream(request: FileRequest): FileResponse { val response = FileResponse() try { requireNonNull(\"Stream must not be null\") log.info(\"${name()} format stream start time [ ${DateUtils.now()} ]\") val mapper = ObjectMapper() request.stream ?.let { BufferedReader(InputStreamReader(it, Charsets.UTF8)).use { reader -> val jsonNode: JsonNode = mapper.readTree(reader) log.info(\"${name()} format stream json node count [ ${jsonNode.size()} ]\") val headers = mutableListOf<Any>() if (jsonNode.isArray && jsonNode.size() > 0) { jsonNode[0].fieldNames() .forEachRemaining { headers.add(it) } } response.headers = headers val columns = mutableListOf<Any>() if (jsonNode.isArray) { jsonNode.elements() .forEachRemaining { node -> val column = mutableMapOf<String, Any?>() node.fields() .forEachRemaining { field -> column[field.key] = field.value } columns.add(column) } } response.columns = columns it.close() } } log.info(\"${name()} format stream end time [ ${DateUtils.now()} ]\") response.successful = true } catch (e: IOException) { response.successful = false response.message = e.message } return response } override fun writer(request: FileRequest): FileResponse { val response = FileResponse() try { log.info(\"${name()} writer origin path [ ${request.path} ]\") log.info(\"${name()} writer start time [ ${DateUtils.now()} ]\") val file = formatFile(request, name()) log.info(\"${name()} writer file absolute path [ ${file.absolutePath} ]\") val factory = JsonFactory() factory.createGenerator(file, JsonEncoding.UTF8) .use { generator -> generator.writeStartArray() request.columns .forEach { column -> generator.writeStartObject() for (headerIndex in request.headers.indices) { when (column) { is List<> -> generator.writeObjectField(request.headers[headerIndex] as String, column[headerIndex]) is ObjectNode -> { generator.codec = ObjectMapper() val header = request.headers[headerIndex] as String generator.writeObjectField(header, column.get(header)) } else -> generator.writeObjectField(request.headers[headerIndex] as String, column) } } generator.writeEndObject() } generator.writeEndArray() } log.info(\"${name()} writer end time [ ${DateUtils.now()} ]\") response.path = file.absolutePath response.successful = true } catch (e: IOException) { response.successful = false response.message = e.message } catch (e: JsonGenerationException) { response.successful = false response.message = e.message } return response } override fun reader(request: FileRequest): FileResponse { val response = FileResponse() try { log.info(\"${name()} reader origin path [ ${request.path} ]\") log.info(\"${name()} reader start time [ ${DateUtils.now()} ]\") val file = formatFile(request, name()) log.info(\"${name()} reader file absolute path [ ${file.absolutePath} ]\") val mapper = ObjectMapper() val jsonNode: JsonNode = mapper.readTree(file) log.info(\"${name()} reader file json node count [ ${jsonNode.size()} ]\") log.info(\"${name()} reader file headers start\") val headers = mutableListOf<Any>() if (jsonNode.isArray && jsonNode.size() > 0) { jsonNode[0].fieldNames() .forEachRemaining { headers.add(it) } } response.headers = headers log.info(\"${name()} reader file headers end\") log.info(\"${name()} reader file columns start\") val columns = mutableListOf<Any>() if (jsonNode.isArray) { jsonNode.elements() .forEachRemaining { node -> val column = mutableMapOf<String, Any?>() node.fields() .forEachRemaining { field -> column[field.key] = field.value } columns.add(column) } } response.columns = columns log.info(\"${name()} reader file columns end\") response.successful = true } catch (e: Exception) { response.successful = false response.message = e.message } return response } } --- 在 resources 源目录下添加 META-INF 和 services 目录，格式为 resources/META-INF/services，创建 io.edurt.datacap.convert.FileModule 文件，内容如下 kotlin io.edurt.datacap.convert.json.JsonModule > 通过以上内容我们实现了 Json 文件转换器的支持。我们只需要在要使用 Json 文件转换器的地方引用该模块即可。比如我们在 server 模块中使用到该模块，则在 server/pom.xml 文件中增加以下内容 xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-file-json</artifactId> <version>${project.version}</version> </dependency> --- kotlin package io.edurt.datacap.convert.json import com.google.inject.Guice.createInjector import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.convert.File import io.edurt.datacap.convert.FileManager import org.junit.Assert.assertEquals import org.junit.Test class JsonModuleTest { private val injector: Injector = createInjector(FileManager()) @Test fun test() { injector.getInstance(Key.get(object : TypeLiteral<Set<File>>() {})) .stream() .findFirst() .ifPresent { assertEquals(\"Json\", it.name()) } } } --- kotlin package io.edurt.datacap.convert.json import com.google.inject.Guice.createInjector import com.google.inject.Injector import io.edurt.datacap.convert.FileFilter import io.edurt.datacap.convert.FileManager import io.edurt.datacap.convert.model.FileRequest import org.junit.Before import org.junit.Test import org.slf4j.LoggerFactory.getLogger import java.io.File import java.io.FileInputStream import kotlin.test.assertTrue class JsonFileTest { private val log = getLogger(this::class.java) private val name = \"Json\" private var injector: Injector? = null private val request: FileRequest = FileRequest() @Before fun before() { injector = createInjector(FileManager()) request.name = \"test\" request.path = System.getProperty(\"user.dir\") request.headers = listOf(\"name\", \"age\") val l1 = listOf(\"Test\", 12) val l2 = listOf(\"Test1\", 121) request.columns = listOf(l1, l2) } @Test fun testFormat() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> val response = file.format(request) log.info(\"headers: [ ${response.headers} ]\") response.columns .let { columns -> columns.forEachIndexed { index, line -> log.info(\"index: [ $index ], line: [ $line ]\") } } assertTrue { response.successful == true } } } } @Test fun testFormatStream() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> request.stream = FileInputStream(File(\"${System.getProperty(\"user.dir\")}/${request.name}.json\")) val response = file.formatStream(request) log.info(\"headers: [ ${response.headers} ]\") response.columns .let { columns -> columns.forEachIndexed { index, line -> log.info(\"index: [ $index ], line: [ $line ]\") } } assertTrue { response.successful == true } } } } @Test fun testWriter() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> assertTrue { file.writer(request) .successful == true } } } } @Test fun testReader() { injector?.let { injector -> FileFilter.filter(injector, name) .ifPresent { file -> val response = file.reader(request) log.info(\"headers: ${response.headers}\") response.columns .forEach { log.info(\"columns: $it\") } assertTrue { response.successful == true } } } } }",
    "url": "https://datacap.devlive.org/zh-CN/developer/file/kotlin/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "自定义 Notify 通知器",
    "content": "DataCap 支持自定义通知器，使用者可以编写自己的通知器集成到 DataCap 中。该文档主要讲解如何快速集成一个通知器到 DataCap 系统中。 该模块我们主要使用到的是 notify 模块内的代码，我们本文使用钉钉通知器来做示例。 --- 新建项目后在 pom.xml 文件中增加以下内容： xml <dependencies> <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-notify-spi</artifactId> <version>${project.version}</version> </dependency> </dependencies> 我们添加 datacap-notify-spi 依赖，这样我们就可以实现集成通知器。 --- kotlin package io.edurt.datacap.notify.dingtalk import com.google.inject.multibindings.Multibinder import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.NotifyModule class DingTalkModule : NotifyModule() { override fun configure() { Multibinder.newSetBinder(this.binder(), Notify::class.java) .addBinding() .to(DingTalkNotify::class.java) } } --- kotlin package io.edurt.datacap.notify.dingtalk import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.model.NotifyRequest import io.edurt.datacap.notify.model.NotifyResponse class DingTalkNotify : Notify { override fun send(request: NotifyRequest): NotifyResponse { return DingTalkUtils.send(request) } } 在转换器中我们只需要实现以下两个方法： fun send(request: NotifyRequest): NotifyResponse 用于实现发送器执行逻辑 --- kotlin package io.edurt.datacap.notify.dingtalk import io.edurt.datacap.common.utils.JsonUtils import io.edurt.datacap.common.utils.SignUtils import io.edurt.datacap.lib.http.HttpClient import io.edurt.datacap.lib.http.HttpConfigure import io.edurt.datacap.lib.http.HttpMethod import io.edurt.datacap.notify.NotifyType import io.edurt.datacap.notify.dingtalk.model.ReturnModel import io.edurt.datacap.notify.dingtalk.model.TextModel import io.edurt.datacap.notify.model.NotifyRequest import io.edurt.datacap.notify.model.NotifyResponse import org.apache.commons.lang3.StringUtils.isNotEmpty import org.slf4j.Logger import org.slf4j.LoggerFactory.getLogger object DingTalkUtils { private val log: Logger = getLogger(DingTalkUtils::class.java) @JvmStatic fun send(request: NotifyRequest): NotifyResponse { val configure = HttpConfigure() configure.autoConnected = false configure.retry = 0 configure.protocol = \"https\" configure.host = \"oapi.dingtalk.com\" configure.port = 443 configure.path = \"robot/send\" configure.method = HttpMethod.POST val params = mutableMapOf(\"accesstoken\" to request.access) if (isNotEmpty(request.secret)) { val signResponse = SignUtils.sign(request.secret) log.info(\"Sign response: ${JsonUtils.toJSON(signResponse)}\") params[\"sign\"] = signResponse.sign params[\"timestamp\"] = signResponse.timestamp.toString() } configure.params = params log.info(\"Notify request params: ${JsonUtils.toJSON(params)}\") val text = TextModel() text.content = request.content configure.body = JsonUtils.toJSON(mapOf(\"text\" to text, \"msgtype\" to formatMessageType(request))) log.info(\"Notify request body: ${configure.body}\") val client = HttpClient(configure) val returnModel = JsonUtils.toObject(client.execute(), ReturnModel::class.java) val response = NotifyResponse() if (returnModel.code == 0) { response.successful = true response.message = null } else { response.successful = false response.message = returnModel.message } return response } private fun formatMessageType(request: NotifyRequest): String { if (request.type == NotifyType.TEXT) { return \"text\" } else { return \"markdown\" } } } 在工具类中我们主要实现发送钉钉消息操作。 --- 在 resources 源目录下添加 META-INF 和 services 目录，格式为 resources/META-INF/services，创建 io.edurt.datacap.fs.FsModule 文件，内容如下 kotlin io.edurt.datacap.notify.dingtalk.DingTalkModule > 通过以上内容我们实现了 DingTalk 通知器的支持。我们只需要在要使用 DingTalk 通知器器的地方引用该模块即可。比如我们在 server 模块中使用到该模块，则在 server/pom.xml 文件中增加以下内容 xml <dependency> <groupId>io.edurt.datacap</groupId> <artifactId>datacap-notify-dingtalk</artifactId> <version>${project.version}</version> </dependency> kotlin package io.edurt.datacap.notify.dingtalk import io.edurt.datacap.notify.model.NotifyRequest import org.junit.Assert import org.junit.Before import org.junit.Test class DingTalkUtilsTest { private val request: NotifyRequest = NotifyRequest() @Before fun before() { request.access = \"ACCESS\" request.content = \"Test Message\" request.secret = \"SECRET\" } @Test fun testSend() { Assert.assertFalse( DingTalkUtils.send(request) .successful ) } } kotlin package io.edurt.datacap.notify.dingtalk import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.NotifyManager import org.junit.Assert.assertNotNull import org.junit.Before import org.junit.Test class DingTalkModuleTest { private val name = \"DingTalk\" private var injector: Injector? = null @Before fun before() { injector = Guice.createInjector(NotifyManager()) } @Test fun test() { val notify: Notify? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Notify>>() {})) ?.first { it.name() == name } assertNotNull(notify) } } kotlin package io.edurt.datacap.notify.dingtalk import com.google.inject.Guice import com.google.inject.Injector import com.google.inject.Key import com.google.inject.TypeLiteral import io.edurt.datacap.notify.Notify import io.edurt.datacap.notify.NotifyManager import io.edurt.datacap.notify.model.NotifyRequest import org.junit.Assert.assertNotNull import org.junit.Before import org.junit.Test class DingTalkNotifyTest { private val name = \"DingTalk\" private var injector: Injector? = null private val request: NotifyRequest = NotifyRequest() @Before fun before() { injector = Guice.createInjector(NotifyManager()) request.access = \"ACCESS\" request.content = \"Test Message\" request.secret = \"SECRET\" } @Test fun test() { val notify: Notify? = injector?.getInstance(Key.get(object : TypeLiteral<Set<Notify>>() {})) ?.first { it.name() == name } assertNotNull(notify?.send(request)) } }",
    "url": "https://datacap.devlive.org/zh-CN/developer/notify/kotlin/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "流水线",
    "content": "在 DataCap 系统中，流水线功能用户可以随意配置，根据低层执行器的版本可以任意调整，系统会根据配置自动识别并编译最终的配置并发送到执行器中。 我们可提供的配置有: | 字段 | 类型 | 描述 | |:---------------:|:-----------:|:------------------------------------------------------:| | field | String | 字段名 | | origin | String | 默认等于字段值，自定义列名使用, 如果是 host|port 格式，系统将会将字段通过 : 进行拼接 | | required | Boolean | 当值为 true 时，表示该字段为必填项 | | override | Boolean | 如果该标志为 true，则表示通过用户配置提取该字段，默认数据将被丢弃 | | input | Boolean | 是否为输入参数 | | width | Integer | 组件宽度, 默认 300 | | type | FieldType | 字段类型, 默认 INPUT | | tooltip | String | 提示信息 | | description | String | 描述信息 | | value | Object | 当前配置输入的结果 | | hidden | Boolean | 如果该配置项为 true，则前端不会显示，启用后才会显示。 | | defaultValues | Array | 如果类型为 SELECT ，则需要传入默认数据 | !!! danger 我们不建议用户自行修改以上配置，如有出现配置异常将会导致插件无法使用该功能 !!!",
    "url": "https://datacap.devlive.org/zh-CN/developer/pipeline/home.html",
    "lang": "zh-CN"
  },
  {
    "title": "使用案例",
    "content": "",
    "url": "https://datacap.devlive.org/zh-CN/useCases.html",
    "lang": "zh-CN"
  },
  {
    "title": "合作伙伴",
    "content": "",
    "url": "https://datacap.devlive.org/zh-CN/partners.html",
    "lang": "zh-CN"
  }
]